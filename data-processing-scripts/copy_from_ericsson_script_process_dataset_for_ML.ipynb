{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e474c47-aaef-4c74-8103-6cc324364e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook has the script to read the log files, preprocess them, \n",
    "# make visulaization plots and then create datasets to be used \n",
    "# by the ML algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af782c9-afcc-4a2a-a4d8-8cd5d8eacbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c367492d-1453-43c2-991f-5f705d6b7f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import matplotlib.gridspec as gridspec\n",
    "import time\n",
    "from decimal import *\n",
    "import numpy as np\n",
    "import csv\n",
    "import time\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "plt.rcParams.update({'font.size': 22, 'figure.autolayout' : True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb7f7b8-3882-4177-89c0-8b3e4a185ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_cols_to_step_size(cols, steps):\n",
    "    if steps == 1:\n",
    "        return cols\n",
    "    #else\n",
    "    expanded_cols = []\n",
    "    for col in cols:\n",
    "        expanded_cols.extend([col+'_'+str(i) for i in range(steps)])\n",
    "    return expanded_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae512a0b-52fc-4042-81eb-963a0f87a1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========================================\n",
    "# Initial settings  \n",
    "#=========================================\n",
    "\n",
    "n_step_history = True\n",
    "num_steps = 5\n",
    "feat_step_size = '10ms'\n",
    "\n",
    "wind_parse_dataset=True\n",
    "create_dataset=True\n",
    "plot_single_file_metrics=False\n",
    "time_step_size = '50ms'\n",
    "data_dir = '/proj/ngan/data-measurements/dataset_january2023/50ms_delay/'\n",
    "\n",
    "# All values above this will be discarded to make the histograms look more readable\n",
    "# It is NOT being used to filter out samples that go into the dataset  \n",
    "ul_delay_outlier_threshold = 50 # ms\n",
    "\n",
    "# files in ran_logs that we are interested in \n",
    "files = ['ulSchedGrantEvent.tsv', 'dlSchedGrantEvent.tsv', 'ABF_BeamMgmtWbm.tsv', \n",
    "         'ABF_BeamMgmtNbm.tsv', \n",
    "         'ABF_NrLegAdditionSuccessEvent.tsv',\n",
    "         'ABF_NrLegAdditionMessagesStats.tsv',\n",
    "         'dlLegSwitchEvent.tsv',\n",
    "         'dlLegSwitchStatus.tsv'] # This is used to know when leg switching happens \n",
    "         #'ABF_BeamMgmtWbmNbm.tsv'] # Does not exist for position 2. Skipping for now \n",
    "\n",
    "# These features are being dropped so that time related features are not picked up to correlate with performance  \n",
    "drop_features = ['absTime', 'absTime0', 'timeStamp', 'ver', 'bbUeRef', 'esfn', 'pucchSfn'] \n",
    "# These features are in some scenarios but not all and hence need to be removed\n",
    "drop_features_after_processing = ['wbm_loadUe_ueTraceIdMsw', 'nbm_loadUe_ueTraceIdMsw', 'dl_loadUe_ueTraceIdMsw']\n",
    "# cRnti and ueTraceIdMsw will be dropped as well after separating measurement and loader UEs \n",
    "# Several other features are dropped in the learning algorithm script before using as input to training \n",
    "\n",
    "# List of all ran_logs files in case we want to look at any other files \n",
    "#files = ['ABF_NrLegAdditionMessagesStats.tsv', 'ABF_BeamMgmtWbChanged.tsv', \n",
    "#         'dlSchedGrantEvent.tsv', 'ABF_BeamMgmtWbmNbmRes.tsv', \n",
    "#         'ABF_BeamMgmtWbm.tsv', 'ABF_NrConnectedUEStats.tsv', \n",
    "#         'NrNodeConfig.tsv', 'ABF_BeamMgmtNbm.tsv', \n",
    "#         'ulSchedGrantEvent.tsv', 'ABF_BeamMgmtWbmNbm.tsv']\n",
    "\n",
    "drop_cols = ['ul_beamIndex', 'ul_loadUe_beamIndex', # non-gen cols\n",
    "             # constant cols\n",
    "             'ul_startPrb', 'ul_isTransformPrecoding', 'ul_BSValueLcg0', 'ul_BSValueLcg1',\n",
    "             'ul_BSValueLcg2', 'ul_BSValueLcg3', 'ul_BSValueLcg4', 'ul_BSValueLcg6',\n",
    "             'ul_BSValueLcg7', 'ul_BSRestimateLcg0', 'ul_BSRestimateLcg1',\n",
    "             'ul_BSRestimateLcg2', 'ul_BSRestimateLcg3', 'ul_BSRestimateLcg4',\n",
    "             'ul_BSRestimateLcg6', 'ul_BSRestimateLcg7', 'ul_VBit',\n",
    "             'ul_wbUsedNbOverridden', 'ul_carrierAggregationUsed',\n",
    "             'ul_clpcCarrierDemand', 'ul_isClpcSaturated', 'ul_totalCellsReqScheduling',\n",
    "             'ul_linkAdaptationUeMode', 'ul_isDrxEnabled', \n",
    "             #--------\n",
    "             'ul_loadUe_startPrb',\n",
    "             'ul_loadUe_isTransformPrecoding', 'ul_loadUe_BSValueLcg0',\n",
    "             'ul_loadUe_BSValueLcg1', 'ul_loadUe_BSValueLcg2', 'ul_loadUe_BSValueLcg3',\n",
    "             'ul_loadUe_BSValueLcg4', 'ul_loadUe_BSValueLcg6', 'ul_loadUe_BSValueLcg7',\n",
    "             'ul_loadUe_BSRestimateLcg0', 'ul_loadUe_BSRestimateLcg1',\n",
    "             'ul_loadUe_BSRestimateLcg2', 'ul_loadUe_BSRestimateLcg3',\n",
    "             'ul_loadUe_BSRestimateLcg4', 'ul_loadUe_BSRestimateLcg6',\n",
    "             'ul_loadUe_BSRestimateLcg7', 'ul_loadUe_VBit',\n",
    "             'ul_loadUe_wbUsedNbOverridden', 'ul_loadUe_carrierAggregationUsed',\n",
    "             'ul_loadUe_clpcCarrierDemand', 'ul_loadUe_isClpcSaturated',\n",
    "             'ul_loadUe_totalCellsReqScheduling', 'ul_loadUe_linkAdaptationUeMode',\n",
    "             'ul_loadUe_isDrxEnabled']\n",
    "drop_cols = expand_cols_to_step_size(drop_cols, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a499c24a-6efa-46a1-988f-af1b2aa902a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#===============================================================================\n",
    "# Lists to aggregate raw metrics as an array over every file iterated through \n",
    "# This is done to create tables and plots after iterating through the log files\n",
    "#===============================================================================\n",
    "\n",
    "# BE CAREFUL some of these names are also being used for windowed metrics that are being plot after resampling\n",
    "# CHECK and change these names if I want to revive using raw samples for the delay distribution \n",
    "# All windowed delays \n",
    "aggr_uldelay = []\n",
    "aggr_dldelay = []\n",
    "\n",
    "# All raw values of select input features  \n",
    "aggr_ul_loadUe_measNumOfPrb = []\n",
    "aggr_ul_loadUe_startPrb = []\n",
    "aggr_ul_loadUe_numOfPrbs = []\n",
    "\n",
    "# Windowed delay over Different UE devices\n",
    "aggr_uldelay_sony = []\n",
    "aggr_uldelay_wnc = []\n",
    "\n",
    "# Windowed delays Different mobility cases \n",
    "aggr_uldelay_static = []\n",
    "aggr_uldelay_moving = []\n",
    "\n",
    "# Windowed delays Different pkt sizes \n",
    "# this is done per device so make sure that you are iterating over only one when you take the mean  \n",
    "aggr_uldelay_1400 = []\n",
    "aggr_uldelay_100 = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Resampled/windowed LTE ul delays for different UE devices \n",
    "# This needs to be done here because LTE samples will be removed from the dataset \n",
    "wind_aggr_uldelay_lte_wnc = np.array([])\n",
    "wind_aggr_uldelay_lte_sony = np.array([])\n",
    "\n",
    "# To aggregate raw MCS, CQI and RSRP metrics to make an overlay plot of these metrics over different loads\n",
    "# For the proportion of bytes and PRBS per MCS graph \n",
    "aggr_mcs_stat_load10 = np.array([])\n",
    "aggr_mcs_stat_load60 = np.array([])\n",
    "aggr_mcs_stat_load60_1400B_m = np.array([])\n",
    "aggr_mcs_stat_load60_100B_m = np.array([])\n",
    "aggr_sinr0_stat_load10 = np.array([])\n",
    "aggr_sinr1_stat_load10 = np.array([])\n",
    "aggr_sinr0_stat_load60 = np.array([])\n",
    "aggr_sinr1_stat_load60 = np.array([])\n",
    "aggr_bytes_stat_load10 = np.array([])\n",
    "aggr_bytes_stat_load60 = np.array([])\n",
    "aggr_prbs_stat_load10 = np.array([])\n",
    "aggr_prbs_stat_load60 = np.array([])\n",
    "\n",
    "# Aggregate raw valaues of some input features\n",
    "# For the histograms of SINR, BSR and network load over the entire dataset per device  \n",
    "aggr_ul_m_WBeamRsrp_sony = np.array([])\n",
    "aggr_ul_m_NBeamRsrp_sony = np.array([])\n",
    "aggr_ul_m_sinrAchievable_sony = np.array([])\n",
    "aggr_ul_m_BSRestimate_sony = np.array([])\n",
    "aggr_ul_ld_macSduInBytes_sony = np.array([])\n",
    "aggr_dl_m_sinr_sony = np.array([])\n",
    "\n",
    "aggr_ul_m_WBeamRsrp_wnc = np.array([])\n",
    "aggr_ul_m_NBeamRsrp_wnc = np.array([])\n",
    "aggr_ul_m_sinrAchievable_wnc = np.array([])\n",
    "aggr_ul_m_BSRestimate_wnc = np.array([])\n",
    "aggr_ul_ld_macSduInBytes_wnc = np.array([])\n",
    "aggr_dl_m_sinr_wnc = np.array([])\n",
    "\n",
    "f = open('delay_percentiles_per_file.csv', 'w', newline='')\n",
    "delay_percentiles_writer = csv.writer(f)\n",
    "delay_percentiles_writer.writerow(['measUeDev', 'delayPktSize', 'position', 'load', '10_perc', '50_perc', '90_perc', '97_perc'])\n",
    "#======================================================\n",
    "start_time = time.time()\n",
    "# separate datasets created for each ue_dev and delay_pkt_size \n",
    "for delayPktSize in delayPktSize_list: \n",
    "    for dev_id,measUeDev in enumerate(measUeDev_list):\n",
    "        \n",
    "        dataset = pd.DataFrame()\n",
    "        # to gather per load stats for each chipset \n",
    "        # It is done over only one delay pkt size and one dev \n",
    "        per_load_ul_delay = [0,0,0,0,0,0,0]\n",
    "        per_load_ul_txAtt = [0,0,0,0,0,0,0]\n",
    "        per_load_ul_ack = [0,0,0,0,0,0,0]\n",
    "        per_load_ul_buffer_meas = [0,0,0,0,0,0,0]\n",
    "        per_load_ul_buffer_loader = [0,0,0,0,0,0,0]\n",
    "        \n",
    "        for pos_id,position in enumerate(position_list):\n",
    "            # aggregate over load so that it can be plot on a single subplot\n",
    "            for ld_id,load in enumerate(load_list):\n",
    "                iteration_start_time = time.time()\n",
    "                print('---------------------------------------------------------------------------------------------------------------------')\n",
    "                print('delayPktSize: ', delayPktSize)\n",
    "                print('measUeDev: ', measUeDev)\n",
    "                print('position: ', position)\n",
    "                print('load: ', load)\n",
    "                print('---------------------------------------------------------------------------------------------------------------------')\n",
    "                # read the delay and ran logs file\n",
    "                delay_log_str = (data_dir+'delay_logs/load'+load+\n",
    "                                 'Mbps_'+\n",
    "                                 'delayPktSize'+delayPktSize+\n",
    "                                 'B_pos'+position+\n",
    "                                 '_'+measUeDev+'.csv')                    \n",
    "                delay = pd.read_csv(delay_log_str, sep=',')\n",
    "                print('Done Reading delay logs')\n",
    "                # Read ran log CSV files. \n",
    "                ran_logs_dir = (data_dir+'ran_logs/load'+load+\n",
    "                                'Mbps_delay_50_PktSize'+delayPktSize+\n",
    "                                'B_pos'+position+'_'+\n",
    "                                measUeDev+\n",
    "                                '/logs/')\n",
    "                # An extra tab at the end of each row is resulting in a NaN column. Remove that\n",
    "                # Check if there are any rows with NaNs\n",
    "                print('Reading from:'+\n",
    "                      'load'+load+'Mbps_delayPktSize'+delayPktSize+\n",
    "                      'B_pos'+position+'_'+measUeDev)\n",
    "                ulSchedGrantEvent = pd.read_csv(ran_logs_dir+files[0], sep='\\t')\n",
    "                ulSchedGrantEvent.drop(ulSchedGrantEvent.columns[-1], axis=1, inplace=True)\n",
    "                print('ulSchedGrantEvent Num rows: ', ulSchedGrantEvent.shape[0])\n",
    "                #print(ulSchedGrantEvent.columns)\n",
    "                #print(\"Number of rows with NaNs: \" + str(np.count_nonzero(ulSchedGrantEvent.isnull())))  \n",
    "\n",
    "                dlSchedGrantEvent = pd.read_csv(ran_logs_dir+files[1], sep='\\t')\n",
    "                dlSchedGrantEvent.drop(dlSchedGrantEvent.columns[-1], axis=1, inplace=True)\n",
    "                print('dlSchedGrantEvent Num rows: ', dlSchedGrantEvent.shape[0])\n",
    "                #print(\"Number of rows with NaNs: \" + str(np.count_nonzero(dlSchedGrantEvent.isnull())))  \n",
    "                \n",
    "                ABF_BeamMgmtWbm = pd.read_csv(ran_logs_dir+files[2], sep='\\t')\n",
    "                ABF_BeamMgmtWbm.drop(ABF_BeamMgmtWbm.columns[-1], axis=1, inplace=True)\n",
    "                ABF_BeamMgmtWbm = ABF_BeamMgmtWbm.rename(columns={'CRnti': 'cRnti'})\n",
    "                print('ABF_BeamMgmtWbm Num rows: ', ABF_BeamMgmtWbm.shape[0])\n",
    "                #print(\"Number of rows with NaNs: \" + str(np.count_nonzero(ABF_BeamMgmtWbm.isnull())))  \n",
    "\n",
    "                ABF_BeamMgmtNbm = pd.read_csv(ran_logs_dir+files[3], sep='\\t')\n",
    "                ABF_BeamMgmtNbm.drop(ABF_BeamMgmtNbm.columns[-1], axis=1, inplace=True)\n",
    "                ABF_BeamMgmtNbm = ABF_BeamMgmtNbm.rename(columns={'CRnti': 'cRnti'})\n",
    "                print('ABF_BeamMgmtNbm Num rows: ', ABF_BeamMgmtNbm.shape[0])\n",
    "                #print(\"Number of rows with NaNs: \" + str(np.count_nonzero(ABF_BeamMgmtNbm.isnull())))  \n",
    "\n",
    "                #ABF_BeamMgmtWbmNbm = pd.read_csv(ran_logs_dir+files[4], sep='\\t')\n",
    "                #ABF_BeamMgmtWbmNbm.drop(ABF_BeamMgmtWbmNbm.columns[-1], axis=1, inplace=True)\n",
    "                #ABF_BeamMgmtWbmNbm = ABF_BeamMgmtWbmNbm.rename(columns={'CRnti': 'cRnti'})\n",
    "                #print('ABF_BeamMgmtWbmNbm Num rows: ', ABF_BeamMgmtWbmNbm.shape[0])\n",
    "                #print(\"Number of rows with NaNs: \" + str(np.count_nonzero(ABF_BeamMgmtWbmNbm.isnull()))) \n",
    "                \n",
    "                print('Done Reading ran logs')\n",
    "                print('======================================================')\n",
    "                \n",
    "                # DEBUG\n",
    "                # Save the columns to file so we know what all we have \n",
    "                #ulSchedGrantEvent.columns.to_series().to_csv('ulSchedGrantEvent_cols.csv', index=False)\n",
    "                #dlSchedGrantEvent.columns.to_series().to_csv('dlSchedGrantEvent_cols.csv', index=False)\n",
    "                #ABF_BeamMgmtWbm.columns.to_series().to_csv('ABF_BeamMgmtWbm_cols.csv', index=False)\n",
    "                #ABF_BeamMgmtNbm.columns.to_series().to_csv('ABF_BeamMgmtNbm_cols.csv', index=False)\n",
    "        \n",
    "                # convert epoch timestamp to datatime format \n",
    "                ulSchedGrantEvent['timeStamp'] = pd.to_datetime(ulSchedGrantEvent['timeStamp'])\n",
    "                dlSchedGrantEvent['timeStamp'] = pd.to_datetime(dlSchedGrantEvent['timeStamp'])\n",
    "                ABF_BeamMgmtWbm['timeStamp'] = pd.to_datetime(ABF_BeamMgmtWbm['timeStamp'])\n",
    "                ABF_BeamMgmtNbm['timeStamp'] = pd.to_datetime(ABF_BeamMgmtNbm['timeStamp'])\n",
    "                #ABF_BeamMgmtWbmNbm['timeStamp'] = pd.to_datetime(ABF_BeamMgmtWbmNbm['timeStamp'])\n",
    "                ul_delay = delay[['t2', 'owd_ul']] # used to be t1 which I think is the transmit time stamp \n",
    "                dl_delay = delay[['t4', 'owd_dl']] # used to be t3 which I think is the transmit time stamp \n",
    "                ul_delay['t2'] = pd.to_datetime(ul_delay['t2'], unit='ms', origin='unix')\n",
    "                ul_delay.rename(columns = {'t2':'timeStamp'}, inplace = True)\n",
    "                dl_delay['t4'] = pd.to_datetime(dl_delay['t4'], unit='ms', origin='unix')\n",
    "                dl_delay.rename(columns = {'t4':'timeStamp'}, inplace = True)\n",
    "    \n",
    "                # set the datetime format timestamp as the index so this is now a series indexed by time\n",
    "                ulSchedGrantEvent.set_index('timeStamp', inplace=True)\n",
    "                dlSchedGrantEvent.set_index('timeStamp', inplace=True)\n",
    "                ABF_BeamMgmtWbm.set_index('timeStamp', inplace=True)\n",
    "                ABF_BeamMgmtNbm.set_index('timeStamp', inplace=True)\n",
    "                #ABF_BeamMgmtWbmNbm.set_index('timeStamp', inplace=True)\n",
    "                ul_delay.set_index('timeStamp', inplace=True)  \n",
    "                dl_delay.set_index('timeStamp', inplace=True)  \n",
    "                \n",
    "                print('Done setting time as index')\n",
    "                print('======================================================')\n",
    "                \n",
    "                # DEBUG\n",
    "                #print(ulSchedGrantEvent['ueTraceIdMsw'].value_counts())\n",
    "                #print(dlSchedGrantEvent['ueTraceIdMsw'].value_counts())\n",
    "                #print(ABF_BeamMgmtWbm['ueTraceIdMsw'].value_counts())\n",
    "                #print(ABF_BeamMgmtNbm['ueTraceIdMsw'].value_counts())\n",
    "                \n",
    "                # Remove entries for user ID 0 and drop some time indicating features \n",
    "                ulSchedGrantEvent = ulSchedGrantEvent.loc[ulSchedGrantEvent['ueTraceIdMsw'] != '0x00000000',:].drop(drop_features, axis=1, errors='ignore')\n",
    "                dlSchedGrantEvent = dlSchedGrantEvent.loc[dlSchedGrantEvent['ueTraceIdMsw'] != '0x00000000',:].drop(drop_features, axis=1, errors='ignore')\n",
    "                ABF_BeamMgmtWbm = ABF_BeamMgmtWbm.loc[ABF_BeamMgmtWbm['ueTraceIdMsw'] != '0x00000000',: ].drop(drop_features, axis=1, errors='ignore')\n",
    "                ABF_BeamMgmtNbm = ABF_BeamMgmtNbm.loc[ABF_BeamMgmtNbm['ueTraceIdMsw'] != '0x00000000',:].drop(drop_features, axis=1, errors='ignore')\n",
    "                #ABF_BeamMgmtWbmNbm = ABF_BeamMgmtWbmNbm.loc[ABF_BeamMgmtWbmNbm['cRnti'] != 0,:].drop(drop_features, axis=1, errors='ignore')\n",
    "                \n",
    "                print('Done dropping features from drop_features')\n",
    "                print('======================================================')\n",
    "                \n",
    "                # Remove delay values that are 0. Do not know why these happen, but very infrequently they do \n",
    "                print('Number of samples where ul_delay was 0: ', ul_delay[ul_delay['owd_ul'] == 0].shape[0])\n",
    "                print('Number of samples where dl_delay was 0: ', dl_delay[dl_delay['owd_dl'] == 0].shape[0])\n",
    "                ul_delay = ul_delay[ul_delay['owd_ul'] > 0]\n",
    "                dl_delay = dl_delay[dl_delay['owd_dl'] > 0]\n",
    "                \n",
    "                print('Done removing 0 valued delay samples')\n",
    "                print('======================================================')\n",
    "                \n",
    "                # ueTraceIdMsw is a consistent identifier for UEs even as they switch to LTE and back (unlike RNTI).   \n",
    "                if load != '0':\n",
    "                    load_ue_id=ulSchedGrantEvent.groupby(by=['ueTraceIdMsw'])['tbSizeInBits'].sum().sort_values(ascending=False).index[0]\n",
    "                    meas_ue_id=ulSchedGrantEvent.groupby(by=['ueTraceIdMsw'])['tbSizeInBits'].sum().sort_values(ascending=False).index[1]\n",
    "                else:\n",
    "                    load_ue_id=ulSchedGrantEvent.groupby(by=['ueTraceIdMsw'])['tbSizeInBits'].sum().sort_values(ascending=False).index[1]\n",
    "                    meas_ue_id=ulSchedGrantEvent.groupby(by=['ueTraceIdMsw'])['tbSizeInBits'].sum().sort_values(ascending=False).index[0]\n",
    "                    \n",
    "                print(ulSchedGrantEvent.groupby(by=['ueTraceIdMsw'])['tbSizeInBits'].sum().sort_values(ascending=False))\n",
    "                print('load_ue_id: ', load_ue_id)\n",
    "                print('meas_ue_id: ', meas_ue_id)\n",
    "                print('Done setting meas and load UE IDs')\n",
    "                print('======================================================')\n",
    "                \n",
    "                # Filter out samples for each UE and then drop cRnti as a feature \n",
    "                # measurement UE features\n",
    "                m_ulSchedGrantEvent = ulSchedGrantEvent.loc[ulSchedGrantEvent['ueTraceIdMsw'] == meas_ue_id,:].drop('ueTraceIdMsw', axis=1, errors='ignore')\n",
    "                m_dlSchedGrantEvent = dlSchedGrantEvent.loc[dlSchedGrantEvent['ueTraceIdMsw'] == meas_ue_id,:].drop('ueTraceIdMsw', axis=1, errors='ignore')\n",
    "                m_ABF_BeamMgmtWbm = ABF_BeamMgmtWbm.loc[ABF_BeamMgmtWbm['ueTraceIdMsw'] == meas_ue_id,:].drop('ueTraceIdMsw', axis=1, errors='ignore')\n",
    "                m_ABF_BeamMgmtNbm = ABF_BeamMgmtNbm.loc[ABF_BeamMgmtNbm['ueTraceIdMsw'] == meas_ue_id,:].drop('ueTraceIdMsw', axis=1, errors='ignore')\n",
    "                #m_ABF_BeamMgmtWbmNbm = ABF_BeamMgmtWbmNbm.loc[ABF_BeamMgmtWbmNbm['ueTraceIdMsw'] == meas_ue_id,:].drop('ueTraceIdMsw', axis=1, errors='ignore')\n",
    "                # loader UE features \n",
    "                ld_ulSchedGrantEvent = ulSchedGrantEvent.loc[ulSchedGrantEvent['ueTraceIdMsw'] == load_ue_id,:].drop('ueTraceIdMsw', axis=1, errors='ignore')\n",
    "                ld_dlSchedGrantEvent = dlSchedGrantEvent.loc[dlSchedGrantEvent['ueTraceIdMsw'] == load_ue_id,:].drop('ueTraceIdMsw', axis=1, errors='ignore')\n",
    "                ld_ABF_BeamMgmtWbm = ABF_BeamMgmtWbm.loc[ABF_BeamMgmtWbm['ueTraceIdMsw'] == load_ue_id,:].drop('ueTraceIdMsw', axis=1, errors='ignore')\n",
    "                ld_ABF_BeamMgmtNbm = ABF_BeamMgmtNbm.loc[ABF_BeamMgmtNbm['ueTraceIdMsw'] == load_ue_id,:].drop('ueTraceIdMsw', axis=1, errors='ignore')\n",
    "                #ld_ABF_BeamMgmtWbmNbm = ABF_BeamMgmtWbmNbm.loc[ABF_BeamMgmtWbmNbm['ueTraceIdMsw'] == load_ue_id,:].drop('ueTraceIdMsw', axis=1, errors='ignore')\n",
    "                \n",
    "                # Create a series with the shifts between NR and LTE by looking when the RNTI has changed \n",
    "                m_rnti_change = m_ulSchedGrantEvent['cRnti'] - m_ulSchedGrantEvent['cRnti'].shift(1)\n",
    "                m_rnti_change_index = m_ulSchedGrantEvent.index\n",
    "                m_rnti_change[m_rnti_change != 0] = 1\n",
    "                \n",
    "                # Then drop the cRnti \n",
    "                m_ulSchedGrantEvent = m_ulSchedGrantEvent.drop('cRnti', axis=1, errors='ignore')\n",
    "                m_dlSchedGrantEvent = m_dlSchedGrantEvent.drop('cRnti', axis=1, errors='ignore')\n",
    "                m_ABF_BeamMgmtWbm = m_ABF_BeamMgmtWbm.drop('cRnti', axis=1, errors='ignore')\n",
    "                m_ABF_BeamMgmtNbm = m_ABF_BeamMgmtNbm.drop('cRnti', axis=1, errors='ignore')\n",
    "                #m_ABF_BeamMgmtWbmNbm = m_ABF_BeamMgmtWbmNbm.drop('cRnti', axis=1, errors='ignore')\n",
    "                ld_ulSchedGrantEvent = ld_ulSchedGrantEvent.drop('cRnti', axis=1, errors='ignore')\n",
    "                ld_dlSchedGrantEvent = ld_dlSchedGrantEvent.drop('cRnti', axis=1, errors='ignore')\n",
    "                ld_ABF_BeamMgmtWbm = ld_ABF_BeamMgmtWbm.drop('cRnti', axis=1, errors='ignore')\n",
    "                ld_ABF_BeamMgmtNbm = ld_ABF_BeamMgmtNbm.drop('cRnti', axis=1, errors='ignore')\n",
    "                #ld_ABF_BeamMgmtWbmNbm = ld_ABF_BeamMgmtWbmNbm.drop('cRnti', axis=1, errors='ignore')\n",
    "                \n",
    "                print('Done separating samples of meas and load UEs')\n",
    "                print('======================================================')\n",
    "                \n",
    "                # DEBUG ############\n",
    "                # Tried to understand what are the different values of csiRequest and if I can identify them as \n",
    "                # macSdu logs entries, BSR log entries or something like that. \n",
    "                # I could not tell what the code 0, 2, 5 are for the csiRequest column  \n",
    "                #print(\"m_ulSchedGrantEvent['csiRequest'].value_counts()\")\n",
    "                #print(m_ulSchedGrantEvent['csiRequest'].value_counts())\n",
    "                #m_ulSchedGrantEvent.loc[ m_ulSchedGrantEvent['csiRequest'] == 0].to_csv('temp_0.csv', index=True)\n",
    "                #m_ulSchedGrantEvent.loc[ m_ulSchedGrantEvent['csiRequest'] == 2].to_csv('temp_2.csv', index=True)\n",
    "                #m_ulSchedGrantEvent.loc[ m_ulSchedGrantEvent['csiRequest'] == 5].to_csv('temp_5.csv', index=True)\n",
    "                #print(\"ld_ulSchedGrantEvent['csiRequest'].value_counts()\")\n",
    "                #print(ld_ulSchedGrantEvent['csiRequest'].value_counts())\n",
    "                \n",
    "                # DEBUG ###############\n",
    "                # Understanding how many samples are in the ulsched log file as the load changes\n",
    "                # The number of samples are not uniform and hence taking mean of values especially \n",
    "                # for the ones that have a lot of zeros will reduce the accuracy of the value    \n",
    "                #diff = m_ulSchedGrantEvent.index.to_series().diff()\n",
    "                #diff = diff.drop(index=diff.index[0]).astype(np.int64) / int(1e6)\n",
    "                #print('m_ulSchedGrantEvent time_diff')\n",
    "                #print(diff.describe())\n",
    "                \n",
    "                #diff = ld_ulSchedGrantEvent.index.to_series().diff()\n",
    "                #diff = diff.drop(index=diff.index[0]).astype(np.int64) / int(1e6)\n",
    "                #print('ld_ulSchedGrantEvent time_diff')\n",
    "                #print(diff.describe())\n",
    "                \n",
    "                #diff = m_dlSchedGrantEvent.index.to_series().diff()\n",
    "                #diff = diff.drop(index=diff.index[0]).astype(np.int64) / int(1e6)\n",
    "                #print('m_dlSchedGrantEvent time_diff')\n",
    "                #print(diff.describe())\n",
    "                \n",
    "                #diff = ld_dlSchedGrantEvent.index.to_series().diff()\n",
    "                #diff = diff.drop(index=diff.index[0]).astype(np.int64) / int(1e6)\n",
    "                #print('ld_dlSchedGrantEvent time_diff')\n",
    "                #print(diff.describe())\n",
    "                \n",
    "                #diff = m_ABF_BeamMgmtWbm.index.to_series().diff()\n",
    "                #diff = diff.drop(index=diff.index[0]).astype(np.int64) / int(1e6)\n",
    "                #print('m_ABF_BeamMgmtWbm time_diff')\n",
    "                #print(diff.describe())\n",
    "                \n",
    "                #diff = ld_ABF_BeamMgmtWbm.index.to_series().diff()\n",
    "                #diff = diff.drop(index=diff.index[0]).astype(np.int64) / int(1e6)\n",
    "                #print('ld_ABF_BeamMgmtWbm time_diff')\n",
    "                #print(diff.describe())\n",
    "                \n",
    "                #diff = m_ABF_BeamMgmtNbm.index.to_series().diff()\n",
    "                #diff = diff.drop(index=diff.index[0]).astype(np.int64) / int(1e6)\n",
    "                #print('m_ABF_BeamMgmtNbm time_diff')\n",
    "                #print(diff.describe())\n",
    "                \n",
    "                #diff = ld_ABF_BeamMgmtNbm.index.to_series().diff()\n",
    "                #diff = diff.drop(index=diff.index[0]).astype(np.int64) / int(1e6)\n",
    "                #print('ld_ABF_BeamMgmtNbm time_diff')\n",
    "                #print(diff.describe())\n",
    " \n",
    "                # checking if taking mean or count makes a big difference \n",
    "                #print('resampled ld_ulSchedGrantEvent macSduInBytes as mean ')\n",
    "                #print(ld_ulSchedGrantEvent['macSduInBytes'].resample(time_step_size).mean().describe())\n",
    "                #print('resampled ld_ulSchedGrantEvent macSduInBytes as count ')\n",
    "                #print((ld_ulSchedGrantEvent['macSduInBytes'].resample(time_step_size).sum()*8/50000).describe())\n",
    "                \n",
    "                # DEBUG ############### \n",
    "                # How many rows in ul and dl sched info files exist even when mcs/tbs/prbs is 0 for the entry \n",
    "                #print('Num of rows in ulSchedGrantEvent: ', ulSchedGrantEvent.shape[0])\n",
    "                #print('Num of rows with mcsIndex=0: ', ulSchedGrantEvent.loc[ulSchedGrantEvent['mcsIndex'] == 0, 'csiRequest'])\n",
    "                #print('Num of rows with mcsIndex>0: ', ulSchedGrantEvent.loc[ulSchedGrantEvent['mcsIndex'] > 0, 'csiRequest'])\n",
    "                #print('Num of rows with numOfPrbs=0: ', ulSchedGrantEvent.loc[ulSchedGrantEvent['numOfPrbs'] == 0,:].shape[0])\n",
    "                #print('Num of rows with tbSizeInBits=0: ', ulSchedGrantEvent.loc[ulSchedGrantEvent['tbSizeInBits'] == 0,:].shape[0])\n",
    "                #print('Num of rows with macSduInBytes>0: ', ulSchedGrantEvent.loc[ulSchedGrantEvent['macSduInBytes'] > 0,:].shape[0])\n",
    "                #print('Num of rows in meas UE with macSduInBytes>0: ', m_ulSchedGrantEvent.loc[m_ulSchedGrantEvent['macSduInBytes'] > 0,:].shape[0])\n",
    "                #print('Num of rows in loader UE with macSduInBytes>0: ', ld_ulSchedGrantEvent.loc[ld_ulSchedGrantEvent['macSduInBytes'] > 0,:].shape[0])\n",
    "                #print('Num of rows with postEqSinr0=0: ', ulSchedGrantEvent.loc[ulSchedGrantEvent['postEqSinr0'] == 0,:].shape[0])\n",
    "                #print('Num of rows with postEqSinr1=0: ', ulSchedGrantEvent.loc[ulSchedGrantEvent['postEqSinr1'] == 0,:].shape[0])\n",
    "                #print('Num of rows with WBeamRsrpCurrent=0: ', ulSchedGrantEvent.loc[ulSchedGrantEvent['WBeamRsrpCurrent'] == 0,:].shape[0])\n",
    "                #print('Num of rows with NBeamRsrpCurrent=0: ', ulSchedGrantEvent.loc[ulSchedGrantEvent['NBeamRsrpCurrent'] == 0,:].shape[0])\n",
    "                #print('Num of rows in dlSchedGrantEvent: ', dlSchedGrantEvent.shape[0])\n",
    "                #print('Num of rows with DL sinr=0: ', dlSchedGrantEvent.loc[dlSchedGrantEvent['sinr'] == 0,:].shape[0])\n",
    "                #print('Num of rows with DL cqi=0: ', dlSchedGrantEvent.loc[dlSchedGrantEvent['cqi'] == 0,:].shape[0])\n",
    "                #print(ulSchedGrantEvent.loc[ulSchedGrantEvent['mcsIndex'] == 0,['mcsIndex','numOfPrbs','tbSizeInBits', 'ACK'] ]) \n",
    "                #print(ulSchedGrantEvent.loc[ulSchedGrantEvent['ACK'] == 0,['mcsIndex','numOfPrbs','tbSizeInBits', 'ACK'] ])\n",
    "                #print('Num of rows in m_dlSchedGrantEvent: ', m_dlSchedGrantEvent.shape[0])\n",
    "                #print('mcsIndex=0: ', m_dlSchedGrantEvent.loc[m_dlSchedGrantEvent['mcsIndex'] == 0,:].shape[0])\n",
    "                #print('numOfPrbs=0: ', m_dlSchedGrantEvent.loc[m_dlSchedGrantEvent['numOfPrbs'] == 0,:].shape[0])\n",
    "                #print('tbSizeInBits=0: ', m_dlSchedGrantEvent.loc[m_dlSchedGrantEvent['tbSizeInBits'] == 0,:].shape[0])                \n",
    "                \n",
    "                # Aggregate into arrays the metrics that we would like to aggregate over multiple files \n",
    "                aggr_uldelay.extend(ul_delay['owd_ul'])\n",
    "                aggr_dldelay.extend(dl_delay['owd_dl'])\n",
    "                aggr_ul_loadUe_measNumOfPrb.extend(ld_ulSchedGrantEvent['measNumOfPrb'])\n",
    "                aggr_ul_loadUe_numOfPrbs.extend(ld_ulSchedGrantEvent['numOfPrbs'])\n",
    "\n",
    "                # to aggregate metrics over positions (stationary vs moving) for each load for a view of impact of load on these metrics  \n",
    "                per_load_ul_delay[ld_id] = per_load_ul_delay[ld_id] + np.mean(ul_delay['owd_ul'])\n",
    "                per_load_ul_txAtt[ld_id] = per_load_ul_txAtt[ld_id] + np.mean(m_ulSchedGrantEvent['transmissionAttempt'])\n",
    "                per_load_ul_ack[ld_id] = per_load_ul_ack[ld_id] + np.mean(m_ulSchedGrantEvent['ACK'])\n",
    "                per_load_ul_buffer_meas[ld_id] = per_load_ul_buffer_meas[ld_id] + np.mean(m_ulSchedGrantEvent['BSRestimate'])\n",
    "                per_load_ul_buffer_loader[ld_id] = per_load_ul_buffer_loader[ld_id] + np.mean(ld_ulSchedGrantEvent['BSRestimate'])\n",
    "                \n",
    "                # Print some stuff I want to see printed\n",
    "                print('ul delay percentiles: ', np.nanquantile(ul_delay['owd_ul'], 0.10) ,np.nanquantile(ul_delay['owd_ul'], 0.50), \n",
    "                      np.nanquantile(ul_delay['owd_ul'], 0.90), np.nanquantile(ul_delay['owd_ul'], 0.97))\n",
    "                # and save it to a file too like a table\n",
    "                delay_percentiles_writer.writerow([measUeDev, delayPktSize, position, load, np.nanquantile(ul_delay['owd_ul'], 0.10), np.nanquantile(ul_delay['owd_ul'], 0.50), \n",
    "                      np.nanquantile(ul_delay['owd_ul'], 0.90), np.nanquantile(ul_delay['owd_ul'], 0.97)])\n",
    "                \n",
    "                # per dev type size aggregation of delay \n",
    "                if measUeDev == 'wnc':\n",
    "                    aggr_uldelay_wnc = np.append(aggr_uldelay_wnc, ul_delay[ul_delay['owd_ul'] < ul_delay_outlier_threshold]['owd_ul'])\n",
    "                    \n",
    "                    aggr_ul_m_WBeamRsrp_wnc = np.append(aggr_ul_m_WBeamRsrp_wnc, m_ulSchedGrantEvent['WBeamRsrpCurrent'])\n",
    "                    aggr_ul_m_NBeamRsrp_wnc = np.append(aggr_ul_m_NBeamRsrp_wnc, m_ulSchedGrantEvent['NBeamRsrpCurrent'])\n",
    "                    aggr_ul_m_sinrAchievable_wnc = np.append(aggr_ul_m_sinrAchievable_wnc, m_ulSchedGrantEvent['sinrAchievable'])\n",
    "                    aggr_ul_m_BSRestimate_wnc = np.append(aggr_ul_m_BSRestimate_wnc, m_ulSchedGrantEvent['BSRestimate'])\n",
    "                    aggr_ul_ld_macSduInBytes_wnc = np.append(aggr_ul_ld_macSduInBytes_wnc, ld_ulSchedGrantEvent['macSduInBytes'])\n",
    "                    aggr_dl_m_sinr_wnc = np.append(aggr_dl_m_sinr_wnc, m_dlSchedGrantEvent['sinr'])\n",
    "                    \n",
    "                elif measUeDev == 'sony':\n",
    "                    aggr_uldelay_sony = np.append(aggr_uldelay_sony, ul_delay[ul_delay['owd_ul'] < ul_delay_outlier_threshold]['owd_ul'])\n",
    "                    \n",
    "                    aggr_ul_m_WBeamRsrp_sony = np.append(aggr_ul_m_WBeamRsrp_sony, m_ulSchedGrantEvent['WBeamRsrpCurrent'])\n",
    "                    aggr_ul_m_NBeamRsrp_sony = np.append(aggr_ul_m_NBeamRsrp_sony, m_ulSchedGrantEvent['NBeamRsrpCurrent'])\n",
    "                    aggr_ul_m_sinrAchievable_sony = np.append(aggr_ul_m_sinrAchievable_sony, m_ulSchedGrantEvent['sinrAchievable'])\n",
    "                    aggr_ul_m_BSRestimate_sony = np.append(aggr_ul_m_BSRestimate_sony, m_ulSchedGrantEvent['BSRestimate'])\n",
    "                    aggr_ul_ld_macSduInBytes_sony = np.append(aggr_ul_ld_macSduInBytes_sony, ld_ulSchedGrantEvent['macSduInBytes'])\n",
    "                    aggr_dl_m_sinr_sony = np.append(aggr_dl_m_sinr_sony, m_dlSchedGrantEvent['sinr'])\n",
    "                \n",
    "                # per packet size aggregation of delay \n",
    "                if delayPktSize == '1400':\n",
    "                    aggr_uldelay_1400.extend(ul_delay[ul_delay['owd_ul'] < ul_delay_outlier_threshold]['owd_ul'])\n",
    "                elif delayPktSize == '100':\n",
    "                    aggr_uldelay_100.extend(ul_delay[ul_delay['owd_ul'] < ul_delay_outlier_threshold]['owd_ul'])\n",
    "                \n",
    "                # per stationary/moving combined over all stationary positions aggregation of delay\n",
    "                if position == '0' :\n",
    "                    aggr_uldelay_moving.extend(ul_delay[ul_delay['owd_ul'] < ul_delay_outlier_threshold]['owd_ul'])\n",
    "                else:\n",
    "                    aggr_uldelay_static.extend(ul_delay[ul_delay['owd_ul'] < ul_delay_outlier_threshold]['owd_ul'])\n",
    "                \n",
    "                # over all samples \n",
    "                aggr_uldelay.extend(ul_delay[ul_delay['owd_ul'] < ul_delay_outlier_threshold]['owd_ul'])\n",
    "                \n",
    "                # Aggregating for the prescheduling demoonstation plot \n",
    "                if position != '0' and load == '10':\n",
    "                    aggr_mcs_stat_load10 = np.append(aggr_mcs_stat_load10, ld_ulSchedGrantEvent['mcsIndex'])\n",
    "                    aggr_sinr0_stat_load10 = np.append(aggr_sinr0_stat_load10, ld_ulSchedGrantEvent['postEqSinr0'])\n",
    "                    aggr_sinr1_stat_load10 = np.append(aggr_sinr1_stat_load10, ld_ulSchedGrantEvent['postEqSinr1'])\n",
    "                    aggr_bytes_stat_load10 = np.append(aggr_bytes_stat_load10, ld_ulSchedGrantEvent['tbSizeInBits']/8)\n",
    "                    aggr_prbs_stat_load10 = np.append(aggr_prbs_stat_load10, ld_ulSchedGrantEvent['numOfPrbs'])\n",
    "                if position != '0' and load == '60':\n",
    "                    if delayPktSize == '1400':\n",
    "                        aggr_mcs_stat_load60_1400B_m = np.append(aggr_mcs_stat_load60_1400B_m, m_ulSchedGrantEvent['mcsIndex'])\n",
    "                    else: #delayPktSize == '100'\n",
    "                        aggr_mcs_stat_load60_100B_m = np.append(aggr_mcs_stat_load60_100B_m, m_ulSchedGrantEvent['mcsIndex'])  \n",
    "                    aggr_mcs_stat_load60 = np.append(aggr_mcs_stat_load60, ld_ulSchedGrantEvent['mcsIndex'])\n",
    "                    aggr_sinr0_stat_load60 = np.append(aggr_sinr0_stat_load60, ld_ulSchedGrantEvent['postEqSinr0'])\n",
    "                    aggr_sinr1_stat_load60 = np.append(aggr_sinr1_stat_load60, ld_ulSchedGrantEvent['postEqSinr1'])\n",
    "                    aggr_bytes_stat_load60 = np.append(aggr_bytes_stat_load60, ld_ulSchedGrantEvent['tbSizeInBits']/8)\n",
    "                    aggr_prbs_stat_load60 = np.append(aggr_prbs_stat_load60, ld_ulSchedGrantEvent['numOfPrbs'])\n",
    "                \n",
    "                # Aggregate standard deviation over resample windows to compare how variant select features are \n",
    "                # between stat and moving and between device types\n",
    "                if (position == '0' and measUeDev == 'wnc' and delayPktSize == '1400' and load == '30'):\n",
    "                    wind_q10to90_ulDelay_mov_wnc = ul_delay.resample('1000ms').quantile(0.90) - ul_delay.resample('1000ms').quantile(0.10)\n",
    "                    wind_q50_ulDelay_mov_wnc = ul_delay.resample('1000ms').quantile(0.50)\n",
    "                    #wind_mean_dlsinr_mov_wnc = m_dlSchedGrantEvent.resample('1000ms').mean()\n",
    "                    #wind_sd_dlsinr_mov_wnc = m_dlSchedGrantEvent.resample('1000ms').std()    \n",
    "                elif (position == '1' and measUeDev == 'wnc' and delayPktSize == '1400' and load == '30'):\n",
    "                    wind_q10to90_ulDelay_stat_wnc = ul_delay.resample('1000ms').quantile(0.90)- ul_delay.resample('1000ms').quantile(0.10)\n",
    "                    wind_q50_ulDelay_stat_wnc = ul_delay.resample('1000ms').quantile(0.50)\n",
    "                    #wind_mean_dlsinr_stat_wnc = m_dlSchedGrantEvent.resample('1000ms').mean()\n",
    "                    #wind_sd_dlsinr_stat_wnc = m_dlSchedGrantEvent.resample('1000ms').std()\n",
    "                elif (position == '0' and measUeDev == 'sony' and delayPktSize == '1400' and load == '30'):\n",
    "                    wind_q10to90_ulDelay_mov_sony = ul_delay.resample('1000ms').quantile(0.90) - ul_delay.resample('1000ms').quantile(0.10)\n",
    "                    wind_q50_ulDelay_mov_sony = ul_delay.resample('1000ms').quantile(0.50)\n",
    "                    #wind_mean_dlsinr_mov_sony = m_dlSchedGrantEvent.resample('1000ms').mean()\n",
    "                    #wind_sd_dlsinr_mov_sony = m_dlSchedGrantEvent.resample('1000ms').std()\n",
    "                elif (position == '1' and measUeDev == 'sony' and delayPktSize == '1400' and load == '30'):\n",
    "                    wind_q10to90_ulDelay_stat_sony = ul_delay.resample('1000ms').quantile(0.90) - ul_delay.resample('1000ms').quantile(0.10)\n",
    "                    wind_q50_ulDelay_stat_sony = ul_delay.resample('1000ms').quantile(0.50)\n",
    "                    #wind_mean_dlsinr_stat_sony = m_dlSchedGrantEvent.resample('1000ms').mean()\n",
    "                    #wind_sd_dlsinr_stat_sony = m_dlSchedGrantEvent.resample('1000ms').std()\n",
    "                 \n",
    "                print('Done aggregating raw metrics over files for plots/analysis') \n",
    "                print('======================================================')\n",
    "                \n",
    "                #=================================================  Resample  ==================================================\n",
    "                if wind_parse_dataset:\n",
    "                    # testing chipset type \n",
    "                    print('meas UE: ', m_ulSchedGrantEvent['chipsetType'].value_counts())\n",
    "                    print('loader UE: ', ld_ulSchedGrantEvent['chipsetType'].value_counts())\n",
    "                        \n",
    "                    # I am opnly going to consider the ul sched log file when using n step history because I am using 10 ms windows \n",
    "                    # and the other logs do not have samples that frequent\n",
    "                    if n_step_history:                        \n",
    "                        # create past steps and aggregate them as features \n",
    "                        # Before resampling using mean.. some metrics need to be summed instead, so do that separately first \n",
    "                        m_sum_mac_bytes = m_ulSchedGrantEvent['macSduInBytes'].resample(feat_step_size).sum().resample(time_step_size).agg(list)\n",
    "                        m_sum_tb_bits = m_ulSchedGrantEvent['tbSizeInBits'].resample(feat_step_size).sum().resample(time_step_size).agg(list)\n",
    "                        ld_sum_mac_bytes = ld_ulSchedGrantEvent['macSduInBytes'].resample(feat_step_size).sum().resample(time_step_size).agg(list)\n",
    "                        ld_sum_tb_bits = ld_ulSchedGrantEvent['tbSizeInBits'].resample(feat_step_size).sum().resample(time_step_size).agg(list)\n",
    "                        \n",
    "                        # Resample into windows of step size and mean over the window\n",
    "                        reSamp_m_ulDelay = ul_delay.resample(time_step_size).mean()\n",
    "                        reSamp_m_dlDelay = dl_delay.resample(time_step_size).mean()\n",
    "                        \n",
    "                        reSamp_m_ulSched = m_ulSchedGrantEvent.resample(feat_step_size).mean().resample(time_step_size).agg(list)\n",
    "                        reSamp_ld_ulSched = ld_ulSchedGrantEvent.resample(feat_step_size).mean().resample(time_step_size).agg(list)\n",
    "                        \n",
    "                        # Find the time range over which to crop the logs\n",
    "                        sampled_log_start_time = max(reSamp_m_ulDelay.index[0], \n",
    "                                                     reSamp_m_dlDelay.index[0],\n",
    "                                                     reSamp_m_ulSched.index[0],\n",
    "                                                     reSamp_ld_ulSched.index[0])\n",
    "                        sampled_log_end_time = min(reSamp_m_ulDelay.index[-1], \n",
    "                                                   reSamp_m_dlDelay.index[-1],\n",
    "                                                   reSamp_m_ulSched.index[-1],\n",
    "                                                   reSamp_ld_ulSched.index[-1])\n",
    "                        \n",
    "                        # crop into this time period to make everything uniform so we can combine them together\n",
    "                        reSamp_m_ulSched = reSamp_m_ulSched[sampled_log_start_time : sampled_log_end_time].iloc[1:]\n",
    "                        reSamp_ld_ulSched = reSamp_ld_ulSched[sampled_log_start_time : sampled_log_end_time].iloc[1:]\n",
    "                        reSamp_m_ulDelay = reSamp_m_ulDelay[sampled_log_start_time : sampled_log_end_time].iloc[1:]\n",
    "                        reSamp_m_dlDelay = reSamp_m_dlDelay[sampled_log_start_time : sampled_log_end_time].iloc[1:]\n",
    "                        \n",
    "                        # open up the columns \n",
    "                        print('before adding the step cols: ', reSamp_m_ulSched.shape)\n",
    "                        \n",
    "                        for col in reSamp_m_ulSched.columns:\n",
    "                            # add the list as multiple colums \n",
    "                            step_cols = [col+'_'+str(i) for i in range(num_steps)]\n",
    "                            reSamp_m_ulSched[step_cols] = reSamp_m_ulSched[col].str[0:num_steps][0]\n",
    "                            reSamp_ld_ulSched[step_cols] = reSamp_ld_ulSched[col].str[0:num_steps][0]\n",
    "                            #print('After adding one set of step cols: ', reSamp_m_ulSched.shape)\n",
    "                            # then delete the col with the list\n",
    "                            reSamp_m_ulSched = reSamp_m_ulSched.drop([col], axis=1)\n",
    "                            reSamp_ld_ulSched = reSamp_ld_ulSched.drop([col], axis=1)\n",
    "                            #print('After dropping the OG col: ', reSamp_m_ulSched.shape)\n",
    "                        \n",
    "                        # replace using the sum metrics that we computed before\n",
    "                        reSamp_ld_ulSched['macSduInBytes'] = ld_sum_mac_bytes\n",
    "                        reSamp_ld_ulSched['tbSizeInBits'] = ld_sum_tb_bits\n",
    "\n",
    "                        reSamp_m_ulSched['macSduInBytes'] = m_sum_mac_bytes\n",
    "                        reSamp_m_ulSched['tbSizeInBits'] = m_sum_tb_bits\n",
    "                        # replace using the sum metrics that we computed before\n",
    "                        print(reSamp_m_ulSched.columns)\n",
    "                        print(reSamp_ld_ulSched.columns)\n",
    "                        \n",
    "                        # concatenate column wise the measurement UE and loader UE featutres for all log files     \n",
    "                        # WARNING: if the columns are mismatched then those will just be moved to the end and concatenated\n",
    "                        this_run_data = pd.concat([reSamp_m_ulSched.add_prefix('ul_'), \n",
    "                                                   reSamp_ld_ulSched.add_prefix('loadUe_').add_prefix('ul_'), \n",
    "                                                   reSamp_m_ulDelay, reSamp_m_dlDelay], axis=1)\n",
    "                        print('Number of NAs in this run wind combined log: ', this_run_data.shape[0] - this_run_data.dropna().shape[0])\n",
    "                                        \n",
    "                        # Drop the samples where a leg switch has happened for the meas UE only since that is when delay values are over NR  \n",
    "                        # Add 'ul_loadUe_NBeamRsrpCurrent' to the drop subset if you want to drop even if loader UE disconnects \n",
    "                        print('Number of NAs dropped because of meas UE leg switch: ', \n",
    "                              this_run_data.shape[0] - this_run_data.dropna(subset=['ul_NBeamRsrpCurrent_1']).shape[0])\n",
    "                        # Drop the values that correspond to it going over LTE. We identify this by seeing if there are any NR ulSched RAN metrics being logged\n",
    "                        this_run_data = this_run_data.dropna(subset=['ul_NBeamRsrpCurrent_1'])\n",
    "                        \n",
    "                    #=================\n",
    "                    else:\n",
    "                        # Before resampling using mean.. some metrics need to be summed instead, so do that separately first \n",
    "                        m_sum_mac_bytes = m_ulSchedGrantEvent['macSduInBytes'].resample(time_step_size).sum()\n",
    "                        m_sum_tb_bits = m_ulSchedGrantEvent['tbSizeInBits'].resample(time_step_size).sum()\n",
    "                        # Resample into windows of step size and mean over the window\n",
    "                        reSamp_m_ulSched = m_ulSchedGrantEvent.resample(time_step_size).mean()\n",
    "                        reSamp_m_dlSched = m_dlSchedGrantEvent.resample(time_step_size).mean()\n",
    "                        reSamp_m_BeamMgmtWbm = m_ABF_BeamMgmtWbm.resample(time_step_size).mean()\n",
    "                        reSamp_m_BeamMgmtNbm = m_ABF_BeamMgmtNbm.resample(time_step_size).mean()\n",
    "                        #reSamp_m_BeamMgmtWbmNbm = m_ABF_BeamMgmtWbmNbm.resample(time_step_size).mean()\n",
    "                        # replace using the sum metrics that we computed before\n",
    "                        reSamp_m_ulSched['macSduInBytes'] = m_sum_mac_bytes\n",
    "                        reSamp_m_ulSched['tbSizeInBits'] = m_sum_tb_bits\n",
    "\n",
    "                        # Before resampling using mean.. some metrics need to be summed instead, so do that separately first \n",
    "                        ld_sum_mac_bytes = ld_ulSchedGrantEvent['macSduInBytes'].resample(time_step_size).sum()\n",
    "                        ld_sum_tb_bits = ld_ulSchedGrantEvent['tbSizeInBits'].resample(time_step_size).sum()\n",
    "                        reSamp_ld_ulSched = ld_ulSchedGrantEvent.resample(time_step_size).mean()\n",
    "                        reSamp_ld_dlSched = ld_dlSchedGrantEvent.resample(time_step_size).mean()\n",
    "                        reSamp_ld_BeamMgmtWbm = ld_ABF_BeamMgmtWbm.resample(time_step_size).mean()\n",
    "                        reSamp_ld_BeamMgmtNbm = ld_ABF_BeamMgmtNbm.resample(time_step_size).mean()\n",
    "                        #reSamp_ld_BeamMgmtWbmNbm = ld_ABF_BeamMgmtWbmNbm.resample(time_step_size).mean()\n",
    "                        # replace using the sum metrics that we computed before\n",
    "                        reSamp_ld_ulSched['macSduInBytes'] = ld_sum_mac_bytes\n",
    "                        reSamp_ld_ulSched['tbSizeInBits'] = ld_sum_tb_bits\n",
    "\n",
    "                        reSamp_m_ulSched['macSduInBytes'] = m_sum_mac_bytes\n",
    "                        reSamp_m_ulSched['tbSizeInBits'] = m_sum_tb_bits\n",
    "                        # replace using the sum metrics that we computed before\n",
    "\n",
    "                        reSamp_m_ulDelay = ul_delay.resample(time_step_size).mean()\n",
    "                        reSamp_m_dlDelay = dl_delay.resample(time_step_size).mean()\n",
    "\n",
    "                        # Find the time range over which to crop the logs\n",
    "                        sampled_log_start_time = max(reSamp_m_dlDelay.index[0], \n",
    "                                                     reSamp_m_ulSched.index[0], \n",
    "                                                     reSamp_m_dlSched.index[0])\n",
    "                        sampled_log_end_time = min(reSamp_m_dlDelay.index[-1], \n",
    "                                                   reSamp_m_ulSched.index[-1], \n",
    "                                                   reSamp_m_dlSched.index[-1])\n",
    "\n",
    "                        # Crop all frames into this time range\n",
    "                        reSamp_m_ulSched = reSamp_m_ulSched[sampled_log_start_time : sampled_log_end_time]\n",
    "                        reSamp_m_dlSched = reSamp_m_dlSched[sampled_log_start_time : sampled_log_end_time]\n",
    "                        reSamp_m_BeamMgmtWbm = reSamp_m_BeamMgmtWbm[sampled_log_start_time : sampled_log_end_time]\n",
    "                        reSamp_m_BeamMgmtNbm = reSamp_m_BeamMgmtNbm[sampled_log_start_time : sampled_log_end_time]\n",
    "                        #reSamp_m_BeamMgmtWbmNbm = reSamp_m_BeamMgmtWbmNbm[sampled_log_start_time : sampled_log_end_time]\n",
    "\n",
    "                        reSamp_ld_ulSched = reSamp_ld_ulSched[sampled_log_start_time : sampled_log_end_time]\n",
    "                        reSamp_ld_dlSched = reSamp_ld_dlSched[sampled_log_start_time : sampled_log_end_time]\n",
    "                        reSamp_ld_BeamMgmtWbm = reSamp_ld_BeamMgmtWbm[sampled_log_start_time : sampled_log_end_time]\n",
    "                        reSamp_ld_BeamMgmtNbm = reSamp_ld_BeamMgmtNbm[sampled_log_start_time : sampled_log_end_time]\n",
    "                        #reSamp_ld_BeamMgmtWbmNbm = reSamp_ld_BeamMgmtWbmNbm[sampled_log_start_time : sampled_log_end_time]           \n",
    "\n",
    "                        reSamp_m_ulDelay = reSamp_m_ulDelay[sampled_log_start_time : sampled_log_end_time]      \n",
    "                        reSamp_m_dlDelay = reSamp_m_dlDelay[sampled_log_start_time : sampled_log_end_time]\n",
    "                    \n",
    "                        print('Before: num samples in wind ul delay log: ', reSamp_m_ulDelay.shape[0])\n",
    "                        print('start, stop time: ', reSamp_m_ulDelay.index[0], reSamp_m_ulDelay.index[-1])\n",
    "                        print('Number of NAs in wind ul delay log: ', reSamp_m_ulDelay.shape[0] - reSamp_m_ulDelay.dropna().shape[0])\n",
    "\n",
    "                        # Print to see how many NAs are in each type of log for selected window size\n",
    "                        print('Number of NAs in this run wind reSamp_m_ulSched log: ', reSamp_m_ulSched.shape[0] - reSamp_m_ulSched.dropna().shape[0])\n",
    "                        #print('Number of NAs in this run wind reSamp_m_dlSched log: ', reSamp_m_dlSched.shape[0] - reSamp_m_dlSched.dropna().shape[0])\n",
    "                        print('Number of NAs in this run wind reSamp_m_BeamMgmtWbm log: ', reSamp_m_BeamMgmtWbm.shape[0] - reSamp_m_BeamMgmtWbm.dropna().shape[0])\n",
    "                        print('Number of NAs in this run wind reSamp_m_BeamMgmtNbm log: ', reSamp_m_BeamMgmtNbm.shape[0] - reSamp_m_BeamMgmtNbm.dropna().shape[0])\n",
    "                        print('Number of NAs in this run wind reSamp_ld_ulSched log: ', reSamp_ld_ulSched.shape[0] - reSamp_ld_ulSched.dropna().shape[0])\n",
    "                        #print('Number of NAs in this run wind reSamp_ld_dlSched log: ', reSamp_ld_dlSched.shape[0] - reSamp_ld_dlSched.dropna().shape[0])\n",
    "                        print('Number of NAs in this run wind reSamp_ld_BeamMgmtWbm log: ', reSamp_ld_BeamMgmtWbm.shape[0] - reSamp_ld_BeamMgmtWbm.dropna().shape[0])\n",
    "                        print('Number of NAs in this run wind reSamp_ld_BeamMgmtNbm log: ', reSamp_ld_BeamMgmtNbm.shape[0] - reSamp_ld_BeamMgmtNbm.dropna().shape[0])\n",
    "                        print('Number of NAs in this run wind reSamp_m_ulDelay log: ', reSamp_m_ulDelay.shape[0] - reSamp_m_ulDelay.dropna().shape[0])\n",
    "                        print('Number of NAs in this run wind reSamp_m_dlDelay log: ', reSamp_m_dlDelay.shape[0] - reSamp_m_dlDelay.dropna().shape[0])\n",
    "                    \n",
    "                        # concatenate column wise the measurement UE and loader UE featutres for all log files     \n",
    "                        # WARNING: if the columns are mismatched then those will just be moved to the end and concatenated\n",
    "                        this_run_data = pd.concat([reSamp_m_ulSched.add_prefix('ul_'), \n",
    "                                                   #reSamp_m_dlSched.add_prefix('dl_'), \n",
    "                                                   reSamp_m_BeamMgmtWbm.add_prefix('wbm_'), \n",
    "                                                   reSamp_m_BeamMgmtNbm.add_prefix('nbm_'), \n",
    "                                                   #reSamp_m_BeamMgmtWbmNbm.add_prefix('wbmnbm_'), \n",
    "                                                   reSamp_ld_ulSched.add_prefix('loadUe_').add_prefix('ul_'), \n",
    "                                                   #reSamp_ld_dlSched.add_prefix('loadUe_').add_prefix('dl_'),\n",
    "                                                   reSamp_ld_BeamMgmtWbm.add_prefix('loadUe_').add_prefix('wbm_'), \n",
    "                                                   reSamp_ld_BeamMgmtNbm.add_prefix('loadUe_').add_prefix('nbm_'), \n",
    "                                                   #reSamp_ld_BeamMgmtWbmNbm.add_prefix('loadUe_').add_prefix('wbmnbm_'), \n",
    "                                                   reSamp_m_ulDelay, reSamp_m_dlDelay], axis=1)\n",
    "                        # Aggregate into an array the delays over LTE to plot them later \n",
    "                        if measUeDev == 'wnc':\n",
    "                            wind_aggr_uldelay_lte_wnc = np.append(wind_aggr_uldelay_lte_wnc, \n",
    "                                                                  this_run_data[this_run_data['ul_NBeamRsrpCurrent'].isnull()]['owd_ul'])\n",
    "                        else:\n",
    "                            wind_aggr_uldelay_lte_sony = np.append(wind_aggr_uldelay_lte_sony, \n",
    "                                                                   this_run_data[this_run_data['ul_NBeamRsrpCurrent'].isnull()]['owd_ul'])\n",
    "\n",
    "                        print('Number of NAs in this run wind combined log: ', this_run_data.shape[0] - this_run_data.dropna().shape[0])\n",
    "\n",
    "                        # Drop the samples where a leg switch has happened for the meas UE only since that is when delay values are over NR  \n",
    "                        # Add 'ul_loadUe_NBeamRsrpCurrent' to the drop subset if you want to drop even if loader UE disconnects \n",
    "                        print('Number of NAs dropped because of meas UE leg switch: ', \n",
    "                              this_run_data.shape[0] - this_run_data.dropna(subset=['ul_NBeamRsrpCurrent']).shape[0])\n",
    "                        # Drop the values that correspond to it going over LTE. We identify this by seeing if there are any NR ulSched RAN metrics being logged\n",
    "                        this_run_data = this_run_data.dropna(subset=['ul_NBeamRsrpCurrent'])    \n",
    "                    \n",
    "                    # -------- end of else over if step_history\n",
    "                    \n",
    "                    # Add columns for position and load info \n",
    "                    this_run_data['position'] = [int(position) for i in range(this_run_data.shape[0])]\n",
    "                    this_run_data['load_Mbps'] = [int(load) for i in range(this_run_data.shape[0])]\n",
    "                    \n",
    "                    # Drop some features that are not in all files  \n",
    "                    # This is done to accomodate when we have multiple steps of the same col\n",
    "                    to_drop = [col for col in this_run_data.columns if any(sub_col in col for sub_col in drop_features_after_processing)]\n",
    "                    print('to_drop due to features not being in all files', to_drop)\n",
    "                    this_run_data = this_run_data.drop(to_drop, axis=1, errors='ignore')\n",
    "                    print('to_drop due to constant and non-gen features: len: ', len(drop_cols))\n",
    "                    this_run_data = this_run_data.drop(drop_cols, axis=1, errors='ignore')\n",
    "                     \n",
    "                    # I am choosing to keep the rest of the NAs in the dtaset so that the model can decide what to do with it. \n",
    "                    # concatenate row wise to the variable that is being aggregated over\n",
    "                    dataset = pd.concat([dataset, this_run_data], axis=0)\n",
    "                    \n",
    "                    print('Done resampling data into windows') \n",
    "                    print('======================================================')\n",
    "                    \n",
    "                #==================================== end of if wind_parse_dataset ================================================\n",
    "                \n",
    "                # If I want to see plots for each file without any aggregation over different files\n",
    "                # Here we have plots using raw data as well as plots using resampled/windowed data \n",
    "                # Although.. plotting over windowed metrics can also be done outside of this loop in a cell below \n",
    "                if plot_single_file_metrics:\n",
    "                    # Raw data plots \n",
    "                    \n",
    "                    # Tried to plot the RSRP with the leg switch events overlayed, but realised that \n",
    "                    # all switches are not receorded so this is not a reliable way to identify the LTE samples \n",
    "                    \n",
    "                    #plt.figure(figsize=(20,5))\n",
    "                    #plt.plot(m_ulSchedGrantEvent.index, m_ulSchedGrantEvent['NBeamRsrpCurrent'], 'r.', label='meas UE')\n",
    "                    #plt.plot(ld_ulSchedGrantEvent.index, ld_ulSchedGrantEvent['NBeamRsrpCurrent'], 'g.', label='load UE')\n",
    "                    #try:\n",
    "                    #    legSwitch = pd.read_csv(ran_logs_dir+files[4], sep='\\t')\n",
    "                    #    legSwitch.drop(legSwitch.columns[-1], axis=1, inplace=True)\n",
    "                    #    legSwitch = legSwitch.rename(columns={'CRnti': 'cRnti'})\n",
    "                    #    print('legSwitch Num rows: ', legSwitch.shape[0])\n",
    "                    #    print(\"Number of rows with NaNs: \" + str(np.count_nonzero(legSwitch.isnull()))) \n",
    "                    #    legSwitch['timeStamp'] = pd.to_datetime(legSwitch['timeStamp'])\n",
    "                    #    legSwitch.set_index('timeStamp', inplace=True)\n",
    "                    #    #legSwitch = legSwitch[(legSwitch['ueTraceIdMsw'] == load_ue_id) | (legSwitch['ueTraceIdMsw'] == meas_ue_id)]\n",
    "                    #    plt.stem([legSwitch.index[0], legSwitch.index[-1]], np.ones(2)*100, bottom=0)\n",
    "                    #except FileNotFoundError:\n",
    "                    #    print('File Not Found')\n",
    "                    #plt.title('Pos: '+str(position)+' Chip: '+str(measUeDev)+' Pkt:'+delayPktSize+' Load:'+load)\n",
    "                    #plt.xlabel('UE NBeamRsrpCurrent')\n",
    "                    #plt.legend()\n",
    "                    #plt.show()\n",
    "\n",
    "                    # Histogram of raw values for this file \n",
    "                    \n",
    "                    # Substitute with m_ulSchedGrantEvent['mcsIndex'] m_dlSchedGrantEvent['cqi'] \n",
    "                    # m_ulSchedGrantEvent['NBeamRsrpCurrent'] etc. and set range if needed to get this file histograms plots \n",
    "                    #sns.set(rc={'figure.figsize':(16,4)}, font_scale = 1.5)\n",
    "                    #plt.figure()\n",
    "                    #plt.hist(ul_delay['owd_ul'], bins=50, edgecolor='k')\n",
    "                    #plt.title('Pos: '+str(position)+' Chip: '+str(measUeDev)+' Pkt:'+delayPktSize)\n",
    "                    #plt.xlabel('UL Delay (ms)')\n",
    "                    #plt.yticks([])\n",
    "                    #plt.show()\n",
    "                    \n",
    "                    # Timeseries overlay of UL delay and select metrics \n",
    "                    \n",
    "                    # measuremnet UE BSR\n",
    "                    if (position == '0'):\n",
    "                        print('Raw delay and BSRestimate samples, LTE NOT removed')\n",
    "                        sns.set(rc={'figure.figsize':(20,4)}, font_scale = 1.5)\n",
    "                        fig, ax1 = plt.subplots(figsize=(20, 4))\n",
    "                        ax2 = ax1.twinx()\n",
    "                        ax1.plot(ul_delay.index, ul_delay['owd_ul'], 'b*-', label='uplink delay')\n",
    "                        ax1.set_ylabel('Uplink delay (ms)')\n",
    "                        ax1.legend(loc='center left')\n",
    "                        ax2.plot(m_ulSchedGrantEvent.index, m_ulSchedGrantEvent['BSRestimate'], 'r.', label='buffer occupancy')\n",
    "                        #ax2.stem(m_rnti_change_index, m_rnti_change*max(m_ulSchedGrantEvent['BSRestimate']), markerfmt='k.', linefmt='k-', basefmt=None)\n",
    "                        ax2.set_xlabel('Time')\n",
    "                        ax2.set_ylabel('Meas. UE buff (bits)')\n",
    "                        ax2.legend(loc='center right')\n",
    "                        # I am likely to save only one of all the plots generated since this is per file, \n",
    "                        # so select and only save the one I want\n",
    "                        if (measUeDev == 'wnc' and delayPktSize == '1400' and load == '30'):\n",
    "                            plt.savefig('plots_for_paper/timeseries raw uldelay BSR overlay wnc.pdf')\n",
    "                            plt.savefig('plots_for_paper/timeseries raw uldelay BSR overlay wnc.png')\n",
    "                        elif (measUeDev == 'sony' and delayPktSize == '1400' and load == '30'):\n",
    "                            plt.savefig('plots_for_paper/timeseries raw uldelay BSR overlay sony.pdf')\n",
    "                            plt.savefig('plots_for_paper/timeseries raw uldelay BSR overlay sony.png')\n",
    "                        plt.show()\n",
    "                        \n",
    "                        print('Windowed delay and BSRestimate samples, LTE removed')\n",
    "                        sns.set(rc={'figure.figsize':(20,4)}, font_scale = 1.5)\n",
    "                        fig, ax1 = plt.subplots(figsize=(20, 4))\n",
    "                        ax2 = ax1.twinx()\n",
    "                        ax1.plot(this_run_data.index, this_run_data['owd_ul'], 'b*-', label='uplink delay')\n",
    "                        ax1.set_ylabel('Uplink delay (ms)')\n",
    "                        ax1.legend(loc='center left')\n",
    "                        ax2.plot(this_run_data.index, this_run_data['ul_BSRestimate'], 'r.', label='buffer occupancy')\n",
    "                        #ax2.stem(m_rnti_change_index, m_rnti_change*max(m_ulSchedGrantEvent['BSRestimate']), markerfmt='k.', linefmt='k-', basefmt=None)\n",
    "                        ax2.set_xlabel('Time')\n",
    "                        ax2.set_ylabel('Meas. UE buff (bits)')\n",
    "                        ax2.legend(loc='center right')\n",
    "                        if (measUeDev == 'wnc' and delayPktSize == '1400' and load == '30'):\n",
    "                            plt.savefig('plots_for_paper/timeseries wind uldelay BSR overlay wnc.pdf')\n",
    "                            plt.savefig('plots_for_paper/timeseries wind uldelay BSR overlay wnc.png')\n",
    "                        elif (measUeDev == 'sony' and delayPktSize == '1400' and load == '30'):\n",
    "                            plt.savefig('plots_for_paper/timeseries wind uldelay BSR overlay sony.pdf')\n",
    "                            plt.savefig('plots_for_paper/timeseries wind uldelay BSR overlay sony.png')\n",
    "                        plt.show()\n",
    "\n",
    "                    #========== Resampled/windowed data plots =======\n",
    "                    \n",
    "                    # Timeseries overlay of UL delay and meas UE ACK rate\n",
    "                    # ACK rate is a bulk metric and needs to be seen in windows, hence we use the resampled data \n",
    "                    if (position == '0'):\n",
    "                        print('Windowed delay and ACK rate samples, LTE removed')\n",
    "                        sns.set(rc={'figure.figsize':(20,4)}, font_scale = 1.5)\n",
    "                        fig, ax1 = plt.subplots(figsize=(20, 4))\n",
    "                        ax2 = ax1.twinx()\n",
    "                        ax1.plot(this_run_data.index, this_run_data['owd_ul'], 'b*-', label='uplink delay')\n",
    "                        ax1.set_ylabel('Uplink delay (ms)')\n",
    "                        ax1.legend(loc='center left')\n",
    "                        ax2.plot(this_run_data.index, this_run_data['ul_ACK'], 'r.', label='ACK rate')\n",
    "                        ax2.set_xlabel('Time')\n",
    "                        ax2.set_ylabel('Meas. UE ACK rate')\n",
    "                        ax2.legend(loc='center right')\n",
    "                        # I am likely to save only one of all the plots generated since this is per file, \n",
    "                        # so select and only save the one I want\n",
    "                        if (measUeDev == 'wnc' and delayPktSize == '1400' and load == '30'):\n",
    "                            plt.savefig('plots_for_paper/timeseries wind uldelay ACKrate overlay wnc.pdf')\n",
    "                            plt.savefig('plots_for_paper/timeseries wind uldelay ACKrate overlay wnc.png')\n",
    "                        elif (measUeDev == 'sony' and delayPktSize == '1400' and load == '30'):\n",
    "                            plt.savefig('plots_for_paper/timeseries wind uldelay ACKrate overlay sony.pdf')\n",
    "                            plt.savefig('plots_for_paper/timeseries wind uldelay ACKrate overlay sony.png')\n",
    "                        plt.show()\n",
    "\n",
    "                    print('Done plotting metrics for this file')\n",
    "                    print('======================================================')\n",
    "                print('Completed this iteration in ', time.time() - iteration_start_time)\n",
    "                #====================================================================================\n",
    "            # end load \n",
    "        # end position\n",
    "        \n",
    "        if create_dataset:\n",
    "            # Save the dataset\n",
    "            dataset.to_csv('parsed_data/dataset_'+measUeDev+'_delayPktSize_'+delayPktSize+\n",
    "                           '_timeStepSize_'+time_step_size+'.csv', index=True)\n",
    "            print('Done writing the dataset into a csv file')\n",
    "            print('======================================================')\n",
    "        \n",
    "    # end measUeDev \n",
    "# end delayPktSize\n",
    "f.close()\n",
    "print('')\n",
    "print('')\n",
    "elapsed_time = time.time() - start_time\n",
    "print('Completed in ', elapsed_time)\n",
    "print('===============================  DONE  ===================================')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
