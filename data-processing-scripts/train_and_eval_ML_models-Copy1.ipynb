{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584eb27c-c205-414b-82d8-6670bb0bf19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nn_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0656d2c2-9e30-4d9a-b939-f33ae6a6b129",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_num = 1 # start\n",
    "samp_weights = None\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "get_ipython().run_line_magic('precision', '3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cc4bea-15f6-4423-8f89-b840b0f4ee58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these values are not used anywhere, except for naming files to know what type of dataset we are working with \n",
    "# NOTE: ADD THIS INTO THE NAMING OF THE SAVED FILES ETC. \n",
    "time_step_size = '50ms'\n",
    "past_sample_step_size = '10ms'\n",
    "\n",
    "\n",
    "\n",
    "past_samples = 5\n",
    "\n",
    "fill_na_val = 0\n",
    "\n",
    "dev_list = ['wnc', 'sony'] # This comprises the train set \n",
    "delayPktSize_list = ['1400', '100']\n",
    "#dev_list = ['sony'] # This comprises the train set \n",
    "#delayPktSize_list = ['100']\n",
    "num_runs = 1 # Average over at least 10 runs\n",
    "\n",
    "baseline_pred = 8.7 # This is the mean delay for 50 ms window size \n",
    "\n",
    "# Train on both but test on only one chipset \n",
    "test_on_one_chipset = False\n",
    "test_on_chipset = 2 # 7 = sony, 2 = wnc\n",
    "\n",
    "# Train on one chipset and test on the other \n",
    "transfer_bw_dev = False\n",
    "train_on_chipset = 2 # 7 = sony, 2 = wnc\n",
    "\n",
    "# Train and test on only the data from when the UEs were stationary  \n",
    "use_only_stat_samples = False\n",
    "\n",
    "# Train and test on only the data from when the UEs were moving  \n",
    "use_only_moving_samples = False\n",
    "\n",
    "# If True then we are predicting one window ahead if False then we are predicting on the same window \n",
    "shift_samp_for_predict = False\n",
    "\n",
    "# In case I want to drop all the dl scheduling info and only focus on the ul and ABF features since the prediction is of ul delay and only ul is being loaded\n",
    "drop_dl_cols = False\n",
    "\n",
    "# Train on one dev 1400 B moving case and test on the same device 100 B pkt moving case \n",
    "# This way I do not have to shuffle the samples and can overlay the prediction and ground truth as it reached cell edge  \n",
    "train_test_on_diff_pkt_sizes = False\n",
    "# Make sure use_only_moving_samples = True so that only moving case samples used \n",
    "\n",
    "# If you want the test samples to be sorted by delay value to see the error differences for the low delay and high delay cases \n",
    "sort_test_samples = True\n",
    "\n",
    "# All delay values above this will be removed from the train and test set\n",
    "drop_outliers = True\n",
    "delay_drop_th = 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faaa2f4-1063-494a-b930-007bb05645a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If I want to do classification instead\n",
    "# 7 classes. These show the upper limit\n",
    "classification = False\n",
    "delay_class_edges = [2, 5, 10, 20, 50, 100, 2000]\n",
    "if classification:\n",
    "    IN_PARAM = IN_PARAM_C\n",
    "    OUT_PARAM = OUT_PARAM_C\n",
    "    IN_PARAM['loss'] = 'dist_penalty'# 'class_mse' # default\n",
    "else:\n",
    "    IN_PARAM = IN_PARAM_R\n",
    "    OUT_PARAM = OUT_PARAM_R\n",
    "    IN_PARAM['loss'] = 'mse' # options are mse, mae and mape \n",
    "    IN_PARAM['eval_metric'] = 'mae' # options are mae, and mape\n",
    "    \n",
    "IN_PARAM['model_type'] = 'xgb'\n",
    "IN_PARAM['model_save_num'] = model_save_num\n",
    "IN_PARAM['time_wind_size'] = time_step_size\n",
    "IN_PARAM['rand_seed'] = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c5d21a-5d8e-48eb-96fb-1d7faa93a2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the top n features of each run and add it to the top_n_features list  \n",
    "# If use_all_feats = True then thes will not be used \n",
    "feat_filter = 10 \n",
    "top_n_features = []\n",
    "use_all_feats = True \n",
    "selected_features = knowledge_based_features\n",
    "\n",
    "if use_all_feats:\n",
    "    #num_feat = 354 # keep nonGen features  \n",
    "    #num_feat = 176 # drop dl features\n",
    "    num_feat = 120*past_samples # default: drop nonGen features\n",
    "    print('Num. of non-generalizable columns: ', len(expand_cols_to_step_size(non_gen_cols, past_samples)))\n",
    "    # This is based on constant value columns that are constant over all cases and for both meas amd load UE.\n",
    "    # It can't be done on a per file basis since it won't match the columns of other cases  \n",
    "    print('Num. of constant value columns: ', len(expand_cols_to_step_size(const_val_cols, past_samples)))\n",
    "    print('Total number of columns to be dropped here: ', len(expand_cols_to_step_size(drop_cols, past_samples)))\n",
    "else:\n",
    "    num_feat = len(expand_cols_to_step_size(selected_features, past_samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea25e490-f944-4573-b26b-ad21d68edfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take as input, class labels and then convert them to delay values by mapping them to \n",
    "# the mid-points of the bins. This way I can compute mae from class labels   \n",
    "def class_mae(yhat, y, num_classes, delay_class_edges):\n",
    "    # find the mid points of bins from the bin edges\n",
    "    class_vals = (delay_class_edges + np.roll(delay_class_edges, 1))/2\n",
    "    class_vals = class_vals[1:num_classes+1]\n",
    "    # replace bin class labels with corresponding bin midpoints  \n",
    "    yhat_val = [class_vals[int(i)] for i in yhat]\n",
    "    y_val = [class_vals[int(i)] for i in y]\n",
    "    # compute mae with these mid points\n",
    "    return mean_absolute_error(yhat_val, y_val)\n",
    "\n",
    "# Take as input class labels in continuous form and convert them to class labels \n",
    "# Then compute the confusion matrix \n",
    "def value_to_class_label(vals, delay_class_edges):\n",
    "    delay_class_indices = np.digitize(vals, delay_class_edges)\n",
    "    return (delay_class_indices - 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2869e7-39fd-4934-9f4a-a371f16eacf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_seeds = range(0,num_runs)\n",
    "\n",
    "if classification:\n",
    "    avg_acc_over_runs = 0\n",
    "    avg_prec_over_runs = 0\n",
    "    avg_rec_over_runs = 0\n",
    "    \n",
    "    # I don't think this makes a lot of sense really \n",
    "    avg_err_over_runs = 0\n",
    "    avg_baseline_err_over_runs = 0\n",
    "else:\n",
    "    avg_err_over_runs = 0\n",
    "    avg_baseline_err_over_runs = 0\n",
    "    # To aggregate error values per bin to see the error distribution for different ranges of the uplink delay values \n",
    "    n_bins = 30\n",
    "    # Calculate the bin boundaries based on delay\n",
    "    bin_edges = np.logspace(np.log10(1), np.log10(1600), n_bins+1)\n",
    "    #print(bin_edges)\n",
    "\n",
    "    train_bin_uldelay_mean = np.zeros(n_bins+1)\n",
    "    train_bin_err_mean = np.zeros(n_bins+1)\n",
    "    train_bin_perc_err_mean = np.zeros(n_bins+1)\n",
    "    train_bin_baseline_err_mean = np.zeros(n_bins+1)\n",
    "    train_bin_baseline_perc_err_mean = np.zeros(n_bins+1)\n",
    "    train_bin_count = np.zeros(n_bins+1)\n",
    "\n",
    "    test_bin_uldelay_mean = np.zeros(n_bins+1)\n",
    "    test_bin_err_mean = np.zeros(n_bins+1)\n",
    "    test_bin_perc_err_mean = np.zeros(n_bins+1)\n",
    "    test_bin_baseline_err_mean = np.zeros(n_bins+1)\n",
    "    test_bin_baseline_perc_err_mean = np.zeros(n_bins+1)\n",
    "    test_bin_count = np.zeros(n_bins+1)\n",
    "\n",
    "for r in run_seeds:\n",
    "    print('Run with random seed: ', r, '------------------------------------------------------------------------')\n",
    "    X = np.empty([0,num_feat])\n",
    "    X[:] = np.nan\n",
    "    y = np.empty([0,])\n",
    "    y[:] = np.nan\n",
    "    X_train = np.empty([0,num_feat])\n",
    "    X_train[:] = np.nan\n",
    "    y_train = np.empty([0,])\n",
    "    y_train[:] = np.nan\n",
    "    X_test = np.empty([0,num_feat])\n",
    "    X_test[:] = np.nan\n",
    "    y_test = np.empty([0,])\n",
    "    y_test[:] = np.nan\n",
    "    #feat_union = []\n",
    "    for measUeDev in dev_list:\n",
    "        print('Device type: ', measUeDev)\n",
    "        for delayPktSize in delayPktSize_list:\n",
    "            print('Delay pkt size', delayPktSize)\n",
    "            IN_PARAM['rand_seed'] = r\n",
    "            # Read the dataset\n",
    "            op_col = 'owd_ul'\n",
    "            if past_samples == 1:\n",
    "                data = pd.read_csv('parsed_data/single_step_window/dataset_'+measUeDev+'_delayPktSize_'+delayPktSize+\n",
    "                                       '_timeStepSize_'+time_step_size+'.csv', delimiter=\",\")\n",
    "            else:\n",
    "                data = pd.read_csv('parsed_data/5step_window/dataset_'+measUeDev+'_delayPktSize_'+delayPktSize+\n",
    "                                       '_timeStepSize_'+time_step_size+'.csv', delimiter=\",\")\n",
    "            print('Num. rows in data before dropping NAs due to owd_ul: ', data.shape[0])\n",
    "            # Can't process samples where there is no ground truth\n",
    "            data = data.dropna(subset=['owd_ul'])\n",
    "            # don't know why I see these, but if I see any delay values of 0 then drop them\n",
    "            data = data[data['owd_ul'] > 0]\n",
    "            if drop_outliers:\n",
    "                print('NOTE: dropping all rows with delay values > ', delay_drop_th)\n",
    "                data = data[data['owd_ul'] <= delay_drop_th]\n",
    "            \n",
    "            if classification:\n",
    "                # create class labels from the delay values\n",
    "                data['owd_ul'] = value_to_class_label(data['owd_ul'], delay_class_edges)\n",
    "                print('delay class distribution', data['owd_ul'].value_counts())\n",
    "                num_classes = len(np.unique(data['owd_ul']))\n",
    "                print('Number of classes: ', num_classes)\n",
    "            \n",
    "            # For the rest of the columns if there is an NA then just fill with 0 so that the rest of the metrics can still be used\n",
    "            print('NOTE: filling NA in samples with ', fill_na_val)\n",
    "            data = data.fillna(fill_na_val)\n",
    "            \n",
    "            # Why are there windows with NAs. This needs to be explained \n",
    "            print('Num. rows in data after dropping NAs due to owd_ul: ', data.shape[0])\n",
    "            model_save_path = 'models/regressor_models/'\n",
    "            print('debug1')\n",
    "            # If needed isolate the rows that are for stationary UE before we drop the position column \n",
    "            if use_only_stat_samples:\n",
    "                data = data[data['position'] != 0]\n",
    "                print(data.shape)\n",
    "            elif use_only_moving_samples:\n",
    "                data = data[data['position'] == 0]\n",
    "                print(data.shape)\n",
    "        \n",
    "            print('debug2')\n",
    "            # Separate features and ground truth from the dataset \n",
    "            if use_all_feats:\n",
    "                print('debug3')\n",
    "                X_t = data.drop(expand_cols_to_step_size(drop_cols, past_samples), axis=1, errors='ignore')\n",
    "                print('debug4')\n",
    "                if drop_dl_cols:\n",
    "                    assert past_samples == 1, \"Including past samples can be done only with the uplink schedulinbg logs so it is not valid to use for drop_dl_cols = True\"\n",
    "                    X_t = X_t.drop([i for i in X_t.columns if 'dl_' in i], axis=1, errors='ignore')\n",
    "                print(data.shape)\n",
    "                print(X_t.shape)\n",
    "                #print(set(feat_union) - set(X_t.columns))\n",
    "                #feat_union = list(set(feat_union) | set( X_t.columns))\n",
    "                #continue\n",
    "            else:\n",
    "                X_t = data[top_n_agg]\n",
    "            \n",
    "            X_feats = np.array(X_t.columns)\n",
    "            print('debug5')\n",
    "            # save into a file\n",
    "            np.savetxt('input_feature_list.csv', X_feats, delimiter=',', fmt=\"%s\")\n",
    "            print('debug6')\n",
    "            #print(list(set(X_t.columns)-set(X_feats)))\n",
    "            #X_t = X_t.drop(list(set(X_t.columns)-set(X_feats)),axis=1)\n",
    "            #print(X_feats)\n",
    "            y_t = data[op_col]\n",
    "\n",
    "            # convert everything to numpy \n",
    "            X_t = X_t.to_numpy()\n",
    "            y_t = y_t.to_numpy()\n",
    "            print('debug7')\n",
    "            # plot the op_col histogram \n",
    "            #plt.figure(1)\n",
    "            #plt.hist(y)\n",
    "            #plt.show\n",
    "\n",
    "            # Scale data\n",
    "            #t = MinMaxScaler()\n",
    "            #t.fit(X)\n",
    "            #X = t.transform(X)\n",
    "\n",
    "            if shift_samp_for_predict: \n",
    "                y_t = np.roll(y_t,1)\n",
    "                X_t = X_t[1:]\n",
    "                y_t = y_t[1:]\n",
    "                \n",
    "            X = np.append(X, X_t, axis=0)\n",
    "            y = np.append(y, y_t, axis=0)\n",
    "            print('debug8')\n",
    "            #print(X.shape)\n",
    "            #print(y.shape)\n",
    "            \n",
    "            if train_test_on_diff_pkt_sizes:\n",
    "                if delayPktSize == '100':\n",
    "                    # Train set \n",
    "                    #X_train = X_t\n",
    "                    #y_train = y_t\n",
    "                    X_train = np.append(X_train, X_t, axis=0)\n",
    "                    y_train = np.append(y_train, y_t, axis=0)\n",
    "                elif delayPktSize == '1400':\n",
    "                    # Test set \n",
    "                    #X_test = X_t\n",
    "                    #y_test = y_t\n",
    "                    X_test = np.append(X_test, X_t, axis=0)\n",
    "                    y_test = np.append(y_test, y_t, axis=0)\n",
    "            print('debug9')\n",
    "            print('---------------------------------------------------------')        \n",
    "                    \n",
    "                    \n",
    "            \n",
    "        # end of for over delayPktSize\n",
    "    # end of for over measUeDevs\n",
    "    \n",
    "    #=============================================== Create the train and test set ==================================\n",
    "    if classification:\n",
    "            strat = y\n",
    "    else: \n",
    "            strat = None\n",
    "    #===================================\n",
    "    if transfer_bw_dev:\n",
    "        # train on samples from one dev and test on the other \n",
    "        # train test split\n",
    "        # apparently subsampling within XGBoost does shuffling so no need to shuffle before sending in the data\n",
    "        # For some reason it isnt doing so..so I am shuffling before sending it \n",
    "        # Random sampling of these samples is not doing anythig ? Why doesnt it affect the random selection of the XGB ? \n",
    "        print(np.shape(X))\n",
    "        print(np.shape(y))\n",
    "        data_temp = np.append(X, np.expand_dims(y, axis=1), axis=1)\n",
    "        print(np.shape(data_temp))\n",
    "        \n",
    "        np.random.seed(IN_PARAM['rand_seed'])\n",
    "        np.random.shuffle(data_temp)\n",
    "        \n",
    "        print(np.shape(data_temp))\n",
    "        X = data_temp[:,:-1]\n",
    "        y = data_temp[:,-1]\n",
    "        print(np.shape(X))\n",
    "        print(np.shape(y))\n",
    "        \n",
    "        ind = np.where(X_feats == 'ul_chipsetType')[0][0]\n",
    "        X_train = X[ X[:,ind] == train_on_chipset, :]\n",
    "        y_train = y[ X[:,ind] == train_on_chipset]\n",
    "        \n",
    "        X_test = X[ X[:,ind] != train_on_chipset, :]\n",
    "        y_test = y[ X[:,ind] != train_on_chipset]\n",
    "        \n",
    "    elif test_on_one_chipset:\n",
    "        # train test split\n",
    "        X_train, X_test_tmp, y_train, y_test_tmp = train_test_split(X, y, test_size=0.3, shuffle=True, random_state=IN_PARAM['rand_seed'], stratify=strat)    \n",
    "        #X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, shuffle=True, random_state=IN_PARAM['rand_seed'])\n",
    "        ind = np.where(X_feats == 'ul_chipsetType')[0][0]\n",
    "        X_test = X_test_tmp[ X_test_tmp[:,ind] == test_on_chipset, :]\n",
    "        y_test = y_test_tmp[ X_test_tmp[:,ind] == test_on_chipset]\n",
    "    elif train_test_on_diff_pkt_sizes:\n",
    "        print('Nothing to do')\n",
    "        # Do nothing train and test sets have already been prepared \n",
    "    else:\n",
    "        # train test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, random_state=IN_PARAM['rand_seed'], stratify=strat)\n",
    "        #X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, shuffle=True, random_state=IN_PARAM['rand_seed'])  \n",
    "    \n",
    "    print('data_train shape ' + str(X_train.shape))\n",
    "    #print('data_val shape ' + str(X_val.shape))\n",
    "    print('data_test shape ' + str(X_test.shape))\n",
    "\n",
    "          \n",
    "    #=============================================== Plot delay hist ================================== \n",
    "    # If classification it will just bin it\n",
    "    if r == 0:\n",
    "        plt.figure(1)\n",
    "        plt.hist(y, bins=50, edgecolor='k')\n",
    "        plt.xlabel('UL delay (ms)')\n",
    "        plt.title('Histogram of all samples of UL delay')\n",
    "        plt.show()\n",
    "    \n",
    "    #=============================================== Train and test the model ==================================\n",
    "    \n",
    "    # Model XGB\n",
    "    OUT_PARAM = dict.fromkeys(OUT_PARAM, 0)\n",
    "    model_save_num = model_save_num + 1\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model, history = evaluate_model(X_train, X_test, y_train, y_test, \n",
    "                                                                 model_save_path + str(IN_PARAM['model_save_num']) + '/', IN_PARAM, sample_weights=samp_weights)\n",
    "    end_time = time.time()\n",
    "    OUT_PARAM['runtime'] = end_time - start_time\n",
    "    print('Time to train model: ', end_time - start_time)\n",
    "    \n",
    "    yhat_test = model.predict(X_test)\n",
    "    yhat_train = model.predict(X_train)\n",
    "    # what if the regression goes out of the class range\n",
    "    \n",
    "    if classification:\n",
    "        # This actually does regression but with class labels, so it can be a bit confusing that I call this classification \n",
    "        if IN_PARAM['loss'] == 'class_mse':\n",
    "            # convert the continuous regression outputs to class labels \n",
    "            yhat_train = np.round(yhat_train)\n",
    "            yhat_train[yhat_train > (num_classes-1)] = num_classes-1\n",
    "            yhat_train[yhat_train < 0] = 0\n",
    "            yhat_test = np.round(yhat_test)\n",
    "            yhat_test[yhat_test > (num_classes-1)] = num_classes-1\n",
    "            yhat_test[yhat_test < 0] = 0\n",
    "            \n",
    "        \n",
    "        # create a confusion matrix\n",
    "        cf_matrix = confusion_matrix(y_test, yhat_test, normalize='true')\n",
    "        sns.set(rc={'figure.figsize':(8,7)}, font_scale = 1.5)\n",
    "        sns.heatmap(cf_matrix, annot=True, \n",
    "            fmt='.1%', cmap='Blues')\n",
    "        \n",
    "        # get accuracy, precision and recall\n",
    "        OUT_PARAM['train_acc'] = accuracy_score(yhat_train, y_train)\n",
    "        OUT_PARAM['train_prec'], OUT_PARAM['train_rec'], train_fscore, train_support = precision_recall_fscore_support(yhat_train, y_train, average='macro')\n",
    "        OUT_PARAM['test_acc'] = accuracy_score(yhat_test, y_test)\n",
    "        OUT_PARAM['test_prec'], OUT_PARAM['test_rec'], test_fscore, test_support = precision_recall_fscore_support(yhat_test, y_test, average='macro')\n",
    "        avg_acc_over_runs = avg_acc_over_runs + OUT_PARAM['test_acc']\n",
    "        avg_prec_over_runs = avg_prec_over_runs + OUT_PARAM['test_prec']\n",
    "        avg_rec_over_runs = avg_rec_over_runs + OUT_PARAM['test_rec']\n",
    "        #avg_baseline_err_over_runs = avg_baseline_err_over_runs + ???\n",
    "        print(OUT_PARAM)\n",
    "        print('ML model: ', IN_PARAM['eval_metric'],' (acc, prec, recall) for test set: ',  OUT_PARAM['test_acc'], OUT_PARAM['test_prec'], OUT_PARAM['test_rec'])\n",
    "        #print('Baseline: ', IN_PARAM['eval_metric'], ' err for test set: ', compute_error(IN_PARAM['eval_metric'], np.repeat(baseline_pred, len(y_test)), y_test))\n",
    "        \n",
    "        # Also get the regression type performance metrics to capture the differeces in \n",
    "        # impact on misclassification between neighbouring and other classes \n",
    "        # convert labels to mean of the delay class for both true and pred\n",
    "        #print('class_mae ', class_mae(yhat_test, y_test, num_classes, delay_class_edges))\n",
    "        #baseline_class = np.digitize(np.repeat(baseline_pred, len(y_test)), delay_class_edges) - 1\n",
    "        #print('baseline class_mae ', class_mae(baseline_class, y_test, num_classes, delay_class_edges))\n",
    "        #avg_err_over_runs = avg_err_over_runs + class_mae(yhat_test, y_test, num_classes, delay_class_edges)\n",
    "        #avg_baseline_err_over_runs = avg_baseline_err_over_runs + class_mae(baseline_class, y_test, num_classes, delay_class_edges)\n",
    "        \n",
    "    else: \n",
    "        OUT_PARAM['train_err'] = compute_error(IN_PARAM['eval_metric'], yhat_train, y_train)\n",
    "        OUT_PARAM['test_err'] = compute_error(IN_PARAM['eval_metric'], yhat_test, y_test)\n",
    "        avg_err_over_runs = avg_err_over_runs + OUT_PARAM['test_err']\n",
    "        avg_baseline_err_over_runs = avg_baseline_err_over_runs + compute_error(IN_PARAM['eval_metric'], np.repeat(baseline_pred, len(y_test)), y_test)\n",
    "        print(OUT_PARAM)\n",
    "        print('ML model: ', IN_PARAM['eval_metric'],' err for test set: ', compute_error(IN_PARAM['eval_metric'], yhat_test, y_test))\n",
    "        print('Baseline: ', IN_PARAM['eval_metric'], ' err for test set: ', compute_error(IN_PARAM['eval_metric'], np.repeat(baseline_pred, len(y_test)), y_test))\n",
    "        print('Mean of the train set is: ', np.mean(y_train))\n",
    "        print('Median of the train set is: ', np.median(y_train))\n",
    "\n",
    "        # plot the cdf of the train error \n",
    "        ecdf_train = ECDF(yhat_train - y_train)\n",
    "        plt.step(ecdf_train.x, ecdf_train.y)\n",
    "        plt.axvline(x=0, color='red', linestyle='--')\n",
    "        plt.axhline(y=ecdf_train(0), color='red', linestyle='--')\n",
    "        plt.xlabel('Pred err (truth - pred)')\n",
    "        plt.title('Train samples')\n",
    "        #plt.hist((yhat_train - y_train), bins=200, edgecolor='k')\n",
    "        #plt.xlim(-20, 50)\n",
    "        plt.show()\n",
    "        print('Train: Probability mass of pred err (truth-pred) below 0 is: ',  ecdf_train(0))\n",
    "        print('Train: Probability mass of pred err (truth-pred) above 0 is: ',  1-ecdf_train(0))\n",
    "\n",
    "        # plot the cdf of the test error \n",
    "        ecdf_test = ECDF(yhat_test - y_test)\n",
    "        plt.step(ecdf_test.x, ecdf_test.y)\n",
    "        plt.axvline(x=0, color='red', linestyle='--')\n",
    "        plt.axhline(y=ecdf_test(0), color='red', linestyle='--')\n",
    "        plt.xlabel('Pred err (truth - pred)')\n",
    "        plt.title('Test samples')\n",
    "        #plt.hist((yhat_test - y_test), bins=200, edgecolor='k')\n",
    "        #plt.xlim(-20, 50)\n",
    "        plt.show()\n",
    "        print('Test: Probability mass of pred err (truth-pred) below 0 is: ',  ecdf_test(0))\n",
    "        print('Test: Probability mass of pred err (truth-pred) above 0 is: ',  1-ecdf_test(0))\n",
    "\n",
    "        #===================================== plot sorted samples of prediction overlayed with ground truth  ==========================\n",
    "        # \n",
    "        if sort_test_samples:   \n",
    "            train_baseline_vals = np.repeat(baseline_pred, len(y_train))\n",
    "            test_baseline_vals = np.repeat(baseline_pred, len(y_test))\n",
    "\n",
    "            tmp1 = np.append(np.expand_dims(y_train, axis=1), np.expand_dims(yhat_train, axis=1), axis=1)\n",
    "            tmp1 = tmp1[tmp1[:, 0].argsort()]\n",
    "            y_train = tmp1[:,0]\n",
    "            yhat_train = tmp1[:,1]\n",
    "\n",
    "            tmp2 = np.append(np.expand_dims(y_test, axis=1), np.expand_dims(yhat_test, axis=1), axis=1)\n",
    "            tmp2 = tmp2[tmp2[:, 0].argsort()]\n",
    "            y_test = tmp2[:,0]\n",
    "            yhat_test = tmp2[:,1]\n",
    "        \n",
    "        #=============================================== bin the delay values to observe err per bin ==================================\n",
    "        #\n",
    "        # bin index for each delay value, so that we can put the values in the right bin \n",
    "        bin_indices = np.digitize(y_train, bin_edges)\n",
    "        \n",
    "        # I want to take all the delay values for each bin\n",
    "        for bin_ind in np.unique(bin_indices):\n",
    "            # these are the delay values in bin bin_edges[bin_ind]\n",
    "            train_bin_uldelay_mean[bin_ind-1] = train_bin_uldelay_mean[bin_ind-1] + np.sum(y_train[bin_indices == bin_ind])\n",
    "            train_bin_count[bin_ind-1] = train_bin_count[bin_ind-1] + len(y_train[bin_indices == bin_ind])\n",
    "            \n",
    "            # I want the corresponding err values for these delay values  \n",
    "            train_bin_err_mean[bin_ind-1] = (train_bin_err_mean[bin_ind-1] + \n",
    "                                             np.sum(np.abs(y_train[bin_indices == bin_ind] - yhat_train[bin_indices == bin_ind]) ))\n",
    "            train_bin_baseline_err_mean[bin_ind-1] = (train_bin_baseline_err_mean[bin_ind-1] + \n",
    "                                                     np.sum(np.abs(y_train[bin_indices == bin_ind] - train_baseline_vals[bin_indices == bin_ind]) ))\n",
    "            train_bin_perc_err_mean[bin_ind-1] = (train_bin_perc_err_mean[bin_ind-1] + \n",
    "                                                  np.sum(np.abs((y_train[bin_indices == bin_ind] - yhat_train[bin_indices == bin_ind]))/(y_train[bin_indices == bin_ind]) ))\n",
    "            train_bin_baseline_perc_err_mean[bin_ind-1] = (train_bin_baseline_perc_err_mean[bin_ind-1] + \n",
    "                                                          np.sum(np.abs((y_train[bin_indices == bin_ind] - train_baseline_vals[bin_indices == bin_ind]))/(y_train[bin_indices == bin_ind])) )\n",
    "        \n",
    "        # bin index for each delay value\n",
    "        bin_indices = np.digitize(y_test, bin_edges)\n",
    "        \n",
    "        # I want to take all the delay values for each bin \n",
    "        for bin_ind in np.unique(bin_indices):\n",
    "            # these are the delay values in bin bin_edges[bin_ind]\n",
    "            test_bin_uldelay_mean[bin_ind-1] = test_bin_uldelay_mean[bin_ind-1] + np.sum(y_test[bin_indices == bin_ind])\n",
    "            test_bin_count[bin_ind-1] = test_bin_count[bin_ind-1] + len(y_test[bin_indices == bin_ind])\n",
    "            \n",
    "            # I want the corresponding err values for these delay values\n",
    "            test_bin_err_mean[bin_ind-1] = (test_bin_err_mean[bin_ind-1] + \n",
    "                                            np.sum(np.abs(y_test[bin_indices == bin_ind] - yhat_test[bin_indices == bin_ind])) )\n",
    "            test_bin_baseline_err_mean[bin_ind-1] = (test_bin_baseline_err_mean[bin_ind-1] + \n",
    "                                                     np.sum(np.abs(y_test[bin_indices == bin_ind] - test_baseline_vals[bin_indices == bin_ind])))\n",
    "            test_bin_perc_err_mean[bin_ind-1] = ( test_bin_perc_err_mean[bin_ind-1] + \n",
    "                                                 np.sum(np.abs((y_test[bin_indices == bin_ind] - yhat_test[bin_indices == bin_ind]))/(y_test[bin_indices == bin_ind])) )\n",
    "            test_bin_baseline_perc_err_mean[bin_ind-1] = (test_bin_baseline_perc_err_mean[bin_ind-1] + \n",
    "                                                          np.sum(np.abs((y_test[bin_indices == bin_ind] - test_baseline_vals[bin_indices == bin_ind]))/(y_test[bin_indices == bin_ind])) ) \n",
    "    \n",
    "        # Plot\n",
    "        plot_y_yhat(y_train, yhat_train, y_test, yhat_test, model_save_path, IN_PARAM)\n",
    "    \n",
    "        # Convert the regression output to class labels and do confusion matrix\n",
    "        cf_matrix = confusion_matrix(value_to_class_label(y_test, delay_class_edges), \n",
    "                                     value_to_class_label(yhat_test, delay_class_edges), normalize='true')\n",
    "        sns.set(rc={'figure.figsize':(8,7)}, font_scale = 1.5)\n",
    "        sns.heatmap(cf_matrix, annot=True, \n",
    "            fmt='.1%', cmap='Blues')\n",
    "        \n",
    "        \n",
    "        #=============================================== plot q-q prediction versus ground truth  ==================================\n",
    "        #\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.plot(y_test, yhat_test, 'b.')\n",
    "        plt.plot([0,0], [delay_drop_th, delay_drop_th], 'k-')\n",
    "        plt.xlabel('Test samples ground truth (ms)')\n",
    "        plt.ylabel('Test samples prediction (ms)')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "    if not os.path.isdir(model_save_path):\n",
    "        os.makedirs(model_save_path)\n",
    "    #=============================================== plot feature importance ==================================\n",
    "    # \n",
    "    # The length of importances reflects the number of features used\n",
    "    importances = model.feature_importances_\n",
    "    # increasing order in value and hence decreasing order in importance \n",
    "    # sort the importances and then fetch the index value of those importances \n",
    "    indices = np.argsort(importances)\n",
    "    #This is in ascending order of \n",
    "    bar_vals = importances[np.flip(indices)[0:feat_filter]]\n",
    "    bar_names = X_feats[np.flip(indices)[0:feat_filter]]\n",
    "    #print(importances[np.flip(indices)[0:feat_filter]])\n",
    "    print(X_feats[np.flip(indices)[0:feat_filter]])\n",
    "    \n",
    "    top_n_features = list( set(top_n_features).union(set(bar_names)))\n",
    "    print('Top n feature list: ', top_n_features)\n",
    "    plt.figure()\n",
    "    plt.barh(range(len(bar_vals)), np.flip(bar_vals), color='b', align='center')\n",
    "    plt.yticks(range(len(bar_vals)), np.flip(bar_names))\n",
    "    \n",
    "    plt.title('Feature importance XGB')\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.savefig('feat_imp'+str(IN_PARAM['rand_seed'])+'.pdf', bbox_inches='tight')\n",
    "    plt.show() \n",
    "        \n",
    "print('')\n",
    "print('')\n",
    "print('===============================  DONE  ===================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05463b85-be30-4c51-97a1-0386c3e1ad18",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_PARAM['test_prec'], OUT_PARAM['test_rec'], test_fscore, test_support = precision_recall_fscore_support(yhat_test, y_test, average='macro')\n",
    "print('ML model: Macro (acc, prec, recall) for test set: ',  OUT_PARAM['test_acc'], OUT_PARAM['test_prec'], OUT_PARAM['test_rec'])\n",
    "OUT_PARAM['test_prec'], OUT_PARAM['test_rec'], test_fscore, test_support = precision_recall_fscore_support(yhat_test, y_test, average='micro')\n",
    "print('ML model: Micro (acc, prec, recall) for test set: ',  OUT_PARAM['test_acc'], OUT_PARAM['test_prec'], OUT_PARAM['test_rec'])\n",
    "OUT_PARAM['test_prec'], OUT_PARAM['test_rec'], test_fscore, test_support = precision_recall_fscore_support(yhat_test, y_test, average='weighted')\n",
    "print('ML model: Weighted (acc, prec, recall) for test set: ',  OUT_PARAM['test_acc'], OUT_PARAM['test_prec'], OUT_PARAM['test_rec'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25bd60c-fcb5-432e-a9f5-a1bbfba82fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if classification:\n",
    "    print('Average: (acc, prec, recall) for test set over runs: ', avg_acc_over_runs, avg_prec_over_runs, avg_rec_over_runs)\n",
    "else:    \n",
    "    print('Average', IN_PARAM['eval_metric'], ' over the runs for ML model is: ', avg_err_over_runs/len(run_seeds))\n",
    "    print('Average', IN_PARAM['eval_metric'], ' over the runs on the baseline is: ', avg_baseline_err_over_runs/len(run_seeds))\n",
    "    print('-----------------------------------------------------------')\n",
    "    print('Top n feature list size: ', len(top_n_features))\n",
    "    print(top_n_features)\n",
    "    print('-----------------------------------------------------------')\n",
    "\n",
    "    print('Loss fun: ', IN_PARAM['loss'])\n",
    "    print('Eval err fun: ', IN_PARAM['eval_metric'])\n",
    "    # After going over all runs     \n",
    "    fig, ax1 = plt.subplots(figsize=(16, 4))\n",
    "    ax2 = ax1.twinx()\n",
    "    ax1.plot(train_bin_uldelay_mean/train_bin_count, train_bin_err_mean/train_bin_count, 'r.-', label='XGB pred err (truth-pred)')\n",
    "    #ax1.plot(train_bin_uldelay_mean/train_bin_count, train_bin_baseline_err_mean/train_bin_count, 'm.-', label='baseline pred err (truth-pred)')\n",
    "    ax1.axhline(y=0, color='r', linestyle='--')\n",
    "    ax1.set_ylabel('error')\n",
    "    #ax1.set_ylim(-5,25)\n",
    "    ax1.legend()\n",
    "    plt.xlabel('uplink delay (ms)')\n",
    "    plt.title('Train samples')\n",
    "    plt.grid()\n",
    "    ax2.set_ylabel('relative err')\n",
    "    ax2.plot(train_bin_uldelay_mean/train_bin_count, train_bin_perc_err_mean/train_bin_count, 'g*-', label='XGB relative err')\n",
    "    #ax2.plot(train_bin_uldelay_mean/train_bin_count, train_bin_baseline_perc_err_mean/train_bin_count, 'c*-', label='baseline relative err')\n",
    "    ax2.axhline(y=0, color='g', linestyle='--')\n",
    "    plt.xscale('log')\n",
    "    ax1.legend(loc=6)\n",
    "    ax2.legend(loc=1)\n",
    "    plt.show() \n",
    "\n",
    "    plt.figure(figsize=(16, 2))\n",
    "    plt.plot(train_bin_uldelay_mean/train_bin_count, train_bin_count, 'b*-')\n",
    "    plt.xlabel('Train samples uplink delay bin (ms)')\n",
    "    plt.xscale('log')\n",
    "    plt.ylabel('bin count')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    print('Loss fun: ', IN_PARAM['loss'])\n",
    "    print('Eval err fun: ', IN_PARAM['eval_metric'])\n",
    "    fig, ax1 = plt.subplots(figsize=(16, 4))\n",
    "    ax2 = ax1.twinx()\n",
    "    ax1.plot(test_bin_uldelay_mean/test_bin_count, test_bin_err_mean/test_bin_count, 'r.-', label='XGB pred err (truth-pred)')\n",
    "    #ax1.plot(test_bin_uldelay_mean/test_bin_count, test_bin_baseline_err_mean/test_bin_count, 'm.-', label='baseline pred err (truth-pred)')\n",
    "    ax1.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.xlabel('uplink delay (ms)')\n",
    "    ax1.set_ylabel('error')\n",
    "    #ax1.set_ylim(-500,250)\n",
    "    ax1.legend()\n",
    "    plt.title('Test samples')\n",
    "    plt.grid()\n",
    "    ax2.set_ylabel('relative err')\n",
    "    ax2.plot(test_bin_uldelay_mean/test_bin_count, test_bin_perc_err_mean/test_bin_count, 'g*-', label='relative err')\n",
    "    #ax2.plot(test_bin_uldelay_mean/test_bin_count, test_bin_baseline_perc_err_mean/test_bin_count, 'c*-', label='baseline relative err')\n",
    "    ax2.axhline(y=0, color='g', linestyle='--')\n",
    "    plt.xscale('log')\n",
    "    ax1.legend(loc=6)\n",
    "    ax2.legend(loc=1)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(16, 2))\n",
    "    plt.plot(test_bin_uldelay_mean/test_bin_count, test_bin_count, 'b*-')\n",
    "    plt.xlabel('Test samples uplink delay bins (ms)')\n",
    "    plt.ylabel('bin count')\n",
    "    plt.xscale('log')\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b81397-4e4e-4414-b254-36f55d8620d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_save_str = 'clas_using_loss_default_softprob'\n",
    "os.system('cp train_and_eval_ML_models.ipynb '+'./saved_notebooks/'+notebook_save_str+'.ipynb')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8364a61b-1675-46a1-a5c0-8c343eaa43f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_union = ['ul_loadUe_weightBand', 'ul_loadUe_BSRestimate', 'ul_BSRestimateLcg6', 'ul_loadUe_deltaIcc', 'ul_nrOfCsiPart1Bits', 'ul_loadUe_RI', 'dl_pucchFormatType', 'ul_loadUe_BSValueLcg6', 'ul_rv', 'ul_tbSizeInBits', 'dl_loadUe_dciFormat', 'dl_loadUe_pucchResourceIndicator', 'nbm_loadUe_WBeamIndex', 'ul_loadUe_fda', 'dl_NBeamRsrpCurrent', 'dl_loadUe_transmissionAttempt', 'dl_loadUe_bbBearerRef7', 'ul_BSRestimateLcg4', 'dl_numOfPrbs', 'ul_loadUe_isTransformPrecoding', 'ul_loadUe_timingOffset', 'dl_numberOfSymbols', 'ul_BSValueLcg0', 'dl_tbSizeInBits', 'ul_loadUe_VBit', 'ul_loadUe_BSRestimateLcg2', 'ul_csiRequest', 'ul_loadUe_BSValueLcg4', 'dl_loadUe_weightBand', 'wbm_rsrp1', 'dl_loadUe_NBeamRsrpCurrent', 'ul_tpcCommand', 'ul_iccAchievable', 'ul_loadUe_preamblePwr', 'nbm_beam03', 'dl_drb5data', 'dl_pucchSfn', 'ul_loadUe_numOfLayers', 'nbm_rsrp01', 'dl_icc', 'ul_loadUe_iccAchievable', 'ul_fda', 'ul_loadUe_uciDecodedResult', 'ul_loadUe_BSValueLcg0', 'dl_dlCcIndex', 'nbm_loadUe_rsrp11', 'wbm_beam4', 'dl_loadUe_incrementalWeight', 'dl_loadUe_antennaPorts', 'ul_BSValueLcg5', 'ul_RI', 'dl_loadUe_fda', 'dl_loadUe_numBearers', 'ul_loadUe_csiRequest', 'dl_linkAdaptationUeMode', 'wbm_loadUe_cellId', 'ul_loadUe_ACK', 'dl_cellId', 'dl_bbBearerRef2', 'ul_loadUe_BSValueLcg7', 'dl_loadUe_cqi', 'dl_pdschHarqFeedbackTiming', 'dl_loadUe_pucchSfn', 'dl_loadUe_pdschHarqFeedbackTiming', 'ul_loadUe_isClpcSaturated', 'ul_loadUe_startSymbolPusch', 'dl_loadUe_dlCcIndex', 'dl_loadUe_drb7input', 'dl_bbBearerRef7', 'dl_loadUe_bbBearerRef2', 'ul_loadUe_beamIndex', 'dl_rank', 'dl_loadUe_drb4input', 'dl_numBearers', 'wbm_loadUe_beam2', 'nbm_loadUe_beam01', 'ul_loadUe_WBeamRsrpCurrent', 'ul_carrierAggregationUsed', 'nbm_loadUe_rsrp12', 'dl_drb4input', 'dl_mcsIndex', 'dl_sinr', 'dl_drb4data', 'ul_loadUe_harqProcessId', 'dl_transmissionAttempt', 'ul_loadUe_BSRestimateLcg7', 'dl_loadUe_isDrxEnabled', 'dl_drb0data', 'dl_loadUe_numOfPrbs', 'dl_loadUe_drb4data', 'ul_loadUe_numberOfSymbolsPusch', 'nbm_loadUe_ueTraceIdMsw', 'wbm_loadUe_beam3', 'dl_sectorIndex', 'dl_drb1input', 'dl_numberOfSrcBits', 'nbm_beam13', 'ul_clpcCarrierDemand', 'ul_wbUsedNbOverridden', 'ul_loadUe_puschTotalRxPsdAvg', 'dl_srOnPucch', 'dl_loadUe_sectorIndex', 'wbm_beam3', 'dl_drb0input', 'ul_BSRestimate', 'ul_BSRestimateLcg1', 'dl_loadUe_taValue', 'ul_maxRank', 'wbm_beam1', 'dl_loadUe_bbBearerRef3', 'dl_physicalCellId', 'ul_totalCellsReqScheduling', 'wbm_loadUe_beam4', 'nbm_loadUe_rsrp02', 'wbm_loadUe_WBeamIndexNewBest', 'dl_loadUe_WBeamRsrpCurrent', 'nbm_WBeamIndex', 'dl_loadUe_slot', 'dl_bbBearerRef1', 'dl_drb3data', 'nbm_loadUe_rsrp13', 'ul_macSduInBytes', 'dl_loadUe_srOnPucch', 'ul_measNumOfPrb', 'ul_transmissionAttempt', 'ul_loadUe_BSRestimateLcg3', 'dl_loadUe_deltaIcc', 'dl_dciFormat', 'ul_bbCellIndex', 'dl_loadUe_macCtrlElement', 'dl_weightBand', 'ul_loadUe_mcsIndex', 'ul_loadUe_antennaPorts', 'ul_timingOffset', 'dl_redundancyVersion', 'ul_loadUe_BSRestimateLcg1', 'ul_loadUe_wbUsedNbOverridden', 'dl_numberOfActivatedDlCells', 'dl_antennaPorts', 'dl_drb7input', 'dl_harqProcessId', 'nbm_loadUe_beam13', 'ul_loadUe_nrOfCsiPart1Bits', 'ul_dciFormat', 'dl_pucchSlotNo', 'nbm_NBeamIndexChosen', 'ul_precodingInfo', 'ul_loadUe_NBeamRsrpCurrent', 'ul_loadUe_harqFailure', 'dl_bbBearerRef4', 'dl_ndi', 'nbm_beam12', 'dl_loadUe_tda', 'nbm_loadUe_cellId', 'wbm_loadUe_WBeamIndexCurrent', 'ul_ndi', 'dl_loadUe_tbSizeInBits', 'wbm_cellId', 'dl_loadUe_icc', 'nbm_loadUe_beam02', 'dl_pucchResourceIndicator', 'ul_BSValueLcg2', 'ul_loadUe_macSduInBytes', 'ul_measNumOfLayers', 'dl_loadUe_drb1input', 'dl_WBeamRsrpCurrent', 'nbm_loadUe_noOfCriPerCsiReport', 'ul_antennaPorts', 'ul_loadUe_slot', 'wbm_beam2', 'ul_loadUe_startPrb', 'ul_slot', 'nbm_loadUe_rsrp01', 'ul_BSRestimateLcg0', 'ul_loadUe_numOfPrbs', 'ul_isDrxEnabled', 'ul_loadUe_BSRestimateLcg0', 'wbm_rsrp4', 'ul_BSValueLcg4', 'dl_loadUe_pucchSlotNo', 'ul_preamblePwr', 'wbm_loadUe_beam1', 'dl_ACK', 'ul_isPrimaryCell', 'dl_tda', 'ul_loadUe_postEqSinr0', 'dl_loadUe_sinr', 'ul_loadUe_pCmaxCIndex', 'ul_loadUe_cellId', 'dl_loadUe_physicalCellId', 'dl_loadUe_beamIndex', 'dl_loadUe_numberOfSymbols', 'dl_loadUe_drb2data', 'dl_loadUe_drb1data', 'ul_BSRestimateLcg7', 'ul_numOfLayers', 'ul_loadUe_numberOfActivatedUlCells', 'ul_loadUe_BSValueLcg1', 'nbm_beam11', 'ul_loadUe_BSValueLcg5', 'dl_loadUe_newData/reTx(1/0)', 'dl_loadUe_bbBearerRef6', 'ul_loadUe_bbCellIndex', 'nbm_loadUe_NBeamIndexChosen', 'dl_loadUe_numberOfActivatedDlCells', 'ul_isClpcSaturated', 'ul_loadUe_BSRestimateLcg5', 'ul_loadUe_tbSizeInBits', 'ul_BSValueLcg1', 'ul_loadUe_maxRank', 'dl_fda', 'ul_DTX', 'ul_loadUe_clpcCarrierDemand', 'dl_cqi', 'dl_dai', 'wbm_rsrp3', 'ul_numOfPrbs', 'dl_drb3input', 'ul_puschTotalRxPsdAvg', 'nbm_loadUe_NBeamIndexCurrent', 'nbm_cellId', 'ul_loadUe_postEqSinr1', 'dl_loadUe_drb3input', 'ul_pCmaxCIndex', 'ul_numberOfSymbolsPusch', 'dl_loadUe_mcsIndex', 'dl_newData/reTx(1/0)', 'ul_weightBand', 'nbm_rsrp02', 'ul_uciDecodedResult', 'ul_loadUe_BSValueLcg2', 'ul_powerHeadRoomIndex', 'ul_loadUe_rv', 'ul_incrementalWeight', 'dl_loadUe_drb2input', 'dl_loadUe_drb7data', 'dl_drb6input', 'wbm_rsrp2', 'dl_loadUe_bbCellIndex', 'dl_bbBearerRef3', 'dl_loadUe_feedbackIndex', 'ul_startPrb', 'ul_cellId', 'wbm_loadUe_rsrp1', 'dl_loadUe_rank', 'ul_ACK', 'ul_beamIndex', 'dl_isSrBitIncluded', 'ul_numberOfActivatedUlCells', 'dl_bbBearerRef6', 'ul_loadUe_transmissionAttempt', 'dl_loadUe_isPrimaryCell', 'wbm_loadUe_rsrp4', 'ul_BSValueLcg3', 'dl_macCtrlElement', 'dl_drb6data', 'dl_isPrimaryCell', 'ul_harqFailure', 'dl_bbCellIndex', 'ul_loadUe_totalCellsReqScheduling', 'ul_postEqSinr0', 'ul_loadUe_carrierAggregationUsed', 'dl_loadUe_bbBearerRef4', 'ul_loadUe_powerHeadRoomIndex', 'ul_BSValueLcg6', 'nbm_loadUe_beam03', 'dl_drb2data', 'nbm_rsrp13', 'dl_isDrxEnabled', 'ul_loadUe_measNumOfPrb', 'dl_feedbackIndex', 'ul_loadUe_isDrxEnabled', 'dl_loadUe_bbBearerRef1', 'ul_loadUe_ulRequestTypeBitmap', 'nbm_rsrp03', 'dl_slot', 'dl_loadUe_drb5input', 'ul_loadUe_BSRestimateLcg4', 'dl_beamIndex', 'dl_loadUe_isSrBitIncluded', 'ul_BSRestimateLcg3', 'ul_WBeamRsrpCurrent', 'dl_loadUe_DTX', 'nbm_rsrp11', 'ul_BSRestimateLcg5', 'dl_loadUe_redundancyVersion', 'ul_ulRequestTypeBitmap', 'wbm_WBeamIndexNewBest', 'ul_ulschIndicator', 'dl_drb1data', 'dl_drb2input', 'ul_loadUe_BSRestimateLcg6', 'nbm_loadUe_rsrp03', 'dl_loadUe_wbUsedNbOverridden', 'dl_loadUe_drb6input', 'nbm_loadUe_beam11', 'nbm_beam01', 'dl_loadUe_linkAdaptationUeMode', 'ul_loadUe_linkAdaptationUeMode', 'dl_loadUe_bbBearerRef0', 'dl_drb5input', 'dl_loadUe_drb6data', 'ul_loadUe_BSValueLcg3', 'dl_loadUe_drb0data', 'ul_loadUe_sinrAchievable', 'nbm_loadUe_beam12', 'ul_loadUe_incrementalWeight', 'dl_loadUe_dai', 'dl_loadUe_ueTraceIdMsw', 'ul_loadUe_ndi', 'ul_startSymbolPusch', 'dl_loadUe_cellId', 'ul_isTransformPrecoding', 'dl_loadUe_drb5data', 'dl_loadUe_drb3data', 'ul_deltaIcc', 'nbm_noOfCriPerCsiReport', 'dl_loadUe_drb0input', 'ul_mcsIndex', 'ul_chipsetType', 'dl_loadUe_numberOfSrcBits', 'nbm_beam02', 'ul_postEqSinr1', 'ul_loadUe_isPrimaryCell', 'dl_incrementalWeight', 'dl_loadUe_ACK', 'ul_BSRestimateLcg2', 'ul_NBeamRsrpCurrent', 'dl_DTX', 'dl_bbBearerRef5', 'dl_loadUe_bbBearerRef5', 'ul_harqProcessId', 'dl_loadUe_pucchFormatType', 'wbm_loadUe_rsrp2', 'dl_taValue', 'ul_loadUe_precodingInfo', 'ul_BSValueLcg7', 'wbm_loadUe_ueTraceIdMsw', 'dl_loadUe_ndi', 'dl_deltaIcc', 'dl_drb7data', 'ul_loadUe_DTX', 'ul_linkAdaptationUeMode', 'wbm_loadUe_rsrp3', 'nbm_NBeamIndexCurrent', 'dl_loadUe_harqProcessId', 'ul_loadUe_ulschIndicator', 'dl_wbUsedNbOverridden', 'ul_loadUe_measNumOfLayers', 'nbm_rsrp12', 'dl_bbBearerRef0', 'wbm_WBeamIndexCurrent', 'ul_sinrAchievable']\n",
    "print(feat_union)\n",
    "print(len(feat_union))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f8db5a-6547-41cb-9cdc-cf6f0587b854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of featurtes I think will not generalize \n",
    "filtered_cols = [i for i in feat_union if 'eamIndex' in i]\n",
    "print(filtered_cols)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
