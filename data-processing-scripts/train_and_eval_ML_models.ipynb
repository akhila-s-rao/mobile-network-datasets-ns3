{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "584eb27c-c205-414b-82d8-6670bb0bf19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoreload\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctions_dicts_to_parse_data\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnn_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/mobile-network-datasets-ns3/data-processing-scripts/nn_functions.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LSTM\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "# reload\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from functions_dicts_to_parse_data import *\n",
    "from nn_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0656d2c2-9e30-4d9a-b939-f33ae6a6b129",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_num = 1 # start\n",
    "samp_weights = None\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "get_ipython().run_line_magic('precision', '3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cc4bea-15f6-4423-8f89-b840b0f4ee58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these values are not used anywhere, except for naming files to know what type of dataset we are working with \n",
    "# NOTE: ADD THIS INTO THE NAMING OF THE SAVED FILES ETC. \n",
    "time_step_size = '500ms'\n",
    "#past_sample_step_size = '10ms'\n",
    "#past_samples = 1\n",
    "\n",
    "fill_na_val = 0\n",
    "\n",
    "#data_slice_list = ['macro', 'micro', 'slow', 'fast']\n",
    "data_slice_list = ['macro']\n",
    "#learning_tasks = ['dashClient_trace.txt_newBitRate_bps', 'dashClient_trace.txt_oldBitRate_bps', \n",
    "#                          'delay_trace.txt_ul_delay', 'delay_trace.txt_dl_delay']\n",
    "learning_task = 'dashClient_trace.txt_newBitRate_bps'\n",
    "classification = True\n",
    "num_runs = 1 # Average over at least 10 runs\n",
    "\n",
    "baseline_pred = 8.7 # This is the mean delay for 50 ms window size \n",
    "\n",
    "# If True then we are predicting one window ahead if False then we are predicting on the same window \n",
    "shift_samp_for_predict = False\n",
    "\n",
    "# If you want the test samples to be sorted by delay value to see the error differences for the low delay and high delay cases \n",
    "sort_test_samples = True\n",
    "\n",
    "# All delay values above this will be removed from the train and test set\n",
    "drop_outliers = False\n",
    "delay_drop_th = 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faaa2f4-1063-494a-b930-007bb05645a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if classification:\n",
    "    IN_PARAM = IN_PARAM_C\n",
    "    OUT_PARAM = OUT_PARAM_C\n",
    "    #IN_PARAM['loss'] = ''# 'class_mse' # default\n",
    "else:\n",
    "    IN_PARAM = IN_PARAM_R\n",
    "    OUT_PARAM = OUT_PARAM_R\n",
    "    IN_PARAM['loss'] = 'mse' # options are mse, mae and mape \n",
    "    IN_PARAM['eval_metric'] = 'mae' # options are mae, and mape\n",
    "    \n",
    "IN_PARAM['model_type'] = 'mlp'\n",
    "IN_PARAM['model_save_num'] = model_save_num\n",
    "IN_PARAM['time_wind_size'] = time_step_size\n",
    "IN_PARAM['rand_seed'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c5d21a-5d8e-48eb-96fb-1d7faa93a2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the top n features of each run and add it to the top_n_features list  \n",
    "# If use_all_feats = True then thes will not be used \n",
    "feat_filter = 10 \n",
    "top_n_features = []\n",
    "use_all_feats = True \n",
    "selected_features = knowledge_based_features\n",
    "model_save_path = 'models/regressor_models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2869e7-39fd-4934-9f4a-a371f16eacf1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_runs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m run_seeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[43mnum_runs\u001b[49m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m classification:\n\u001b[1;32m      4\u001b[0m     avg_acc_over_runs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_runs' is not defined"
     ]
    }
   ],
   "source": [
    "for data_slice in data_slice_list:\n",
    "    print('Data slice: ', data_slice)\n",
    "    \n",
    "    train_bin_uldelay_mean = np.zeros(n_bins+1)\n",
    "    train_bin_err_mean = np.zeros(n_bins+1)\n",
    "    train_bin_perc_err_mean = np.zeros(n_bins+1)\n",
    "    train_bin_baseline_err_mean = np.zeros(n_bins+1)\n",
    "    train_bin_baseline_perc_err_mean = np.zeros(n_bins+1)\n",
    "    train_bin_count = np.zeros(n_bins+1)\n",
    "\n",
    "    test_bin_uldelay_mean = np.zeros(n_bins+1)\n",
    "    test_bin_err_mean = np.zeros(n_bins+1)\n",
    "    test_bin_perc_err_mean = np.zeros(n_bins+1)\n",
    "    test_bin_baseline_err_mean = np.zeros(n_bins+1)\n",
    "    test_bin_baseline_perc_err_mean = np.zeros(n_bins+1)\n",
    "    test_bin_count = np.zeros(n_bins+1)\n",
    "    if classification:\n",
    "        avg_acc_over_runs = 0\n",
    "        avg_prec_over_runs = 0\n",
    "        avg_rec_over_runs = 0\n",
    "\n",
    "        # I don't think this makes a lot of sense really \n",
    "        avg_err_over_runs = 0\n",
    "        avg_baseline_err_over_runs = 0\n",
    "    else:\n",
    "        avg_err_over_runs = 0\n",
    "        avg_baseline_err_over_runs = 0\n",
    "        # To aggregate error values per bin to see the error distribution for different ranges of the uplink delay values \n",
    "        n_bins = 30\n",
    "        # Calculate the bin boundaries based on delay\n",
    "        bin_edges = np.logspace(np.log10(1), np.log10(1600), n_bins+1)\n",
    "        \n",
    "    run_seeds = range(0,num_runs)\n",
    "    for r in run_seeds:\n",
    "        print('Run with random seed: ', r, '------------------------------------------------------------------------')    \n",
    "        IN_PARAM['rand_seed'] = r\n",
    "        np.random.seed(seed_value)\n",
    "        \n",
    "        # Read the dataset\n",
    "        print('Load the dataset')\n",
    "        data = pd.read_csv('../../data_volume/logs_today/parsed_data/dataset_'+'dataset_'+data_slice+'_video_delay_'+time_step_size+'.csv', delimiter=\",\")\n",
    "        print(data.shape)\n",
    "        \n",
    "        # Drop what needs to be dropped\n",
    "        # Drop rows when the label or ground truth is NA \n",
    "        print('Dropping rows for which the label is NA, since there is no ground truth')\n",
    "        data = data.dropna(subset=[learning_task])\n",
    "        print(data.shape)\n",
    "        # Drop columns that we do not want to include in the training dataset \n",
    "        print('Drop unwanted columns')\n",
    "        if use_all_feats:\n",
    "            data = data.drop(expand_cols_to_step_size(drop_cols, past_samples), axis=1, errors='ignore')\n",
    "        else:\n",
    "            data = data[top_n_agg]\n",
    "        print(data.shape)\n",
    "        \n",
    "        # Fill with 0 the values that are missing in the input features so that the sample can still be used\n",
    "        print('Filling NA in samples with ', fill_na_val)\n",
    "        data = data.fillna(fill_na_val)\n",
    "        print(data.shape)\n",
    "        \n",
    "        # Separate the X and the y from the data\n",
    "        X = data.drop(learning_task, axis=1)\n",
    "        y = data[learning_task]\n",
    "        print(X.shape)\n",
    "        print(y.shape)\n",
    "        \n",
    "        # Save the columns to use for feature importance graphs\n",
    "        X_feats = np.array(X_t.columns)\n",
    "        np.savetxt('input_feature_list.csv', X_feats, delimiter=',', fmt=\"%s\")\n",
    "\n",
    "        # Convert everything to numpy \n",
    "        X = X.to_numpy()\n",
    "        y = y.to_numpy()\n",
    "\n",
    "        # If you want to shift the output feature window \n",
    "        if shift_samp_for_predict: \n",
    "            y = np.roll(y,1)\n",
    "            X = X[1:]\n",
    "            y = y[1:]   \n",
    "\n",
    "        if classification:\n",
    "            strat = y\n",
    "        else: \n",
    "            strat = None\n",
    "\n",
    "        # Train test split\n",
    "        # Random shift\n",
    "        sample_shift_count = np.random.randint(0, X.shape[0], size=1)\n",
    "        X = np.roll(X, sample_shift_count)\n",
    "        y = np.roll(y, sample_shift_count)\n",
    "        # Keep the first part as train set and the second part as test set  \n",
    "        train_data_size = np.floor(X.shape[0]*0.7)\n",
    "        X_train = X[0:train_data_size]\n",
    "        y_train = y[0:train_data_size]\n",
    "        X_test = X[train_data_size:-1]\n",
    "        y_test = y[train_data_size:-1]\n",
    "        # Randomly assign to train and test sets \n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, random_state=IN_PARAM['rand_seed'], stratify=strat) \n",
    "    \n",
    "        print('data train shape ' + str(X_train.shape))\n",
    "        print('data test shape ' + str(X_test.shape))\n",
    "\n",
    "        # Plot hist of output\n",
    "        # If classification it will just bin it\n",
    "        if r == 0:\n",
    "            plt.figure(1)\n",
    "            plt.hist(y, bins=50, edgecolor='k')\n",
    "            plt.xlabel(learning_task)\n",
    "            plt.title('Histogram of all samples')\n",
    "            plt.show()\n",
    "    \n",
    "        #=============================================== Train and test the model ==================================\n",
    "    \n",
    "        OUT_PARAM = dict.fromkeys(OUT_PARAM, 0)\n",
    "        model_save_num = model_save_num + 1\n",
    "        start_time = time.time()\n",
    "\n",
    "        model, history = evaluate_model(X_train, X_test, \n",
    "                                        y_train, y_test, \n",
    "                                        model_save_path + str(IN_PARAM['model_save_num']) + '/', \n",
    "                                        IN_PARAM, sample_weights=samp_weights)\n",
    "        end_time = time.time()\n",
    "        OUT_PARAM['runtime'] = end_time - start_time\n",
    "        print('Time to train model: ', end_time - start_time)\n",
    "\n",
    "        yhat_test = model.predict(X_test)\n",
    "        yhat_train = model.predict(X_train)\n",
    "    \n",
    "        if classification:\n",
    "            # This actually does regression but with class labels, so it can be a bit confusing that I call this classification \n",
    "            if IN_PARAM['loss'] == 'class_mse':\n",
    "                # convert the continuous regression outputs to class labels \n",
    "                yhat_train = np.round(yhat_train)\n",
    "                yhat_train[yhat_train > (num_classes-1)] = num_classes-1\n",
    "                yhat_train[yhat_train < 0] = 0\n",
    "                yhat_test = np.round(yhat_test)\n",
    "                yhat_test[yhat_test > (num_classes-1)] = num_classes-1\n",
    "                yhat_test[yhat_test < 0] = 0\n",
    "\n",
    "\n",
    "            # create a confusion matrix\n",
    "            cf_matrix = confusion_matrix(y_test, yhat_test, normalize='true')\n",
    "            sns.set(rc={'figure.figsize':(8,7)}, font_scale = 1.5)\n",
    "            sns.heatmap(cf_matrix, annot=True, \n",
    "                fmt='.1%', cmap='Blues')\n",
    "\n",
    "            # get accuracy, precision and recall\n",
    "            OUT_PARAM['train_acc'] = accuracy_score(yhat_train, y_train)\n",
    "            OUT_PARAM['train_prec'], OUT_PARAM['train_rec'], train_fscore, train_support = precision_recall_fscore_support(yhat_train, y_train, average='macro')\n",
    "            OUT_PARAM['test_acc'] = accuracy_score(yhat_test, y_test)\n",
    "            OUT_PARAM['test_prec'], OUT_PARAM['test_rec'], test_fscore, test_support = precision_recall_fscore_support(yhat_test, y_test, average='macro')\n",
    "            avg_acc_over_runs = avg_acc_over_runs + OUT_PARAM['test_acc']\n",
    "            avg_prec_over_runs = avg_prec_over_runs + OUT_PARAM['test_prec']\n",
    "            avg_rec_over_runs = avg_rec_over_runs + OUT_PARAM['test_rec']\n",
    "            #avg_baseline_err_over_runs = avg_baseline_err_over_runs + ???\n",
    "            print(OUT_PARAM)\n",
    "            print('ML model: ', IN_PARAM['eval_metric'],' (acc, prec, recall) for test set: ',  OUT_PARAM['test_acc'], OUT_PARAM['test_prec'], OUT_PARAM['test_rec'])\n",
    "\n",
    "        else: \n",
    "            OUT_PARAM['train_err'] = compute_error(IN_PARAM['eval_metric'], yhat_train, y_train)\n",
    "            OUT_PARAM['test_err'] = compute_error(IN_PARAM['eval_metric'], yhat_test, y_test)\n",
    "            avg_err_over_runs = avg_err_over_runs + OUT_PARAM['test_err']\n",
    "            avg_baseline_err_over_runs = avg_baseline_err_over_runs + compute_error(IN_PARAM['eval_metric'], np.repeat(baseline_pred, len(y_test)), y_test) \n",
    "            print(OUT_PARAM)\n",
    "            print('ML model: ', IN_PARAM['eval_metric'],' err for test set: ', compute_error(IN_PARAM['eval_metric'], yhat_test, y_test))\n",
    "            print('Baseline: ', IN_PARAM['eval_metric'], ' err for test set: ', compute_error(IN_PARAM['eval_metric'], np.repeat(baseline_pred, len(y_test)), y_test))\n",
    "            print('Mean of the train set is: ', np.mean(y_train))\n",
    "            print('Median of the train set is: ', np.median(y_train))\n",
    "\n",
    "            # plot the cdf of the train error \n",
    "            ecdf_train = ECDF(yhat_train - y_train)\n",
    "            plt.step(ecdf_train.x, ecdf_train.y)\n",
    "            plt.axvline(x=0, color='red', linestyle='--')\n",
    "            plt.axhline(y=ecdf_train(0), color='red', linestyle='--')\n",
    "            plt.xlabel('Pred err (truth - pred)')\n",
    "            plt.title('Train samples')\n",
    "            #plt.hist((yhat_train - y_train), bins=200, edgecolor='k')\n",
    "            #plt.xlim(-20, 50)\n",
    "            plt.show()\n",
    "            print('Train: Probability mass of pred err (truth-pred) below 0 is: ',  ecdf_train(0))\n",
    "            print('Train: Probability mass of pred err (truth-pred) above 0 is: ',  1-ecdf_train(0))\n",
    "\n",
    "            # plot the cdf of the test error \n",
    "            ecdf_test = ECDF(yhat_test - y_test)\n",
    "            plt.step(ecdf_test.x, ecdf_test.y)\n",
    "            plt.axvline(x=0, color='red', linestyle='--')\n",
    "            plt.axhline(y=ecdf_test(0), color='red', linestyle='--')\n",
    "            plt.xlabel('Pred err (truth - pred)')\n",
    "            plt.title('Test samples')\n",
    "            #plt.hist((yhat_test - y_test), bins=200, edgecolor='k')\n",
    "            #plt.xlim(-20, 50)\n",
    "            plt.show()\n",
    "            print('Test: Probability mass of pred err (truth-pred) below 0 is: ',  ecdf_test(0))\n",
    "            print('Test: Probability mass of pred err (truth-pred) above 0 is: ',  1-ecdf_test(0))\n",
    "\n",
    "        #===================================== plot sorted samples of prediction overlayed with ground truth  ==========================\n",
    "        # \n",
    "        if sort_test_samples:   \n",
    "            train_baseline_vals = np.repeat(baseline_pred, len(y_train))\n",
    "            test_baseline_vals = np.repeat(baseline_pred, len(y_test))\n",
    "\n",
    "            tmp1 = np.append(np.expand_dims(y_train, axis=1), np.expand_dims(yhat_train, axis=1), axis=1)\n",
    "            tmp1 = tmp1[tmp1[:, 0].argsort()]\n",
    "            y_train = tmp1[:,0]\n",
    "            yhat_train = tmp1[:,1]\n",
    "\n",
    "            tmp2 = np.append(np.expand_dims(y_test, axis=1), np.expand_dims(yhat_test, axis=1), axis=1)\n",
    "            tmp2 = tmp2[tmp2[:, 0].argsort()]\n",
    "            y_test = tmp2[:,0]\n",
    "            yhat_test = tmp2[:,1]\n",
    "        \n",
    "        ##=============================================== bin the delay values to observe err per bin ==================================\n",
    "        ##\n",
    "        ## bin index for each delay value, so that we can put the values in the right bin \n",
    "        #bin_indices = np.digitize(y_train, bin_edges)\n",
    "        #\n",
    "        ## I want to take all the delay values for each bin\n",
    "        #for bin_ind in np.unique(bin_indices):\n",
    "        #    # these are the delay values in bin bin_edges[bin_ind]\n",
    "        #    train_bin_uldelay_mean[bin_ind-1] = train_bin_uldelay_mean[bin_ind-1] + np.sum(y_train[bin_indices == bin_ind])\n",
    "        #    train_bin_count[bin_ind-1] = train_bin_count[bin_ind-1] + len(y_train[bin_indices == bin_ind])\n",
    "        #    \n",
    "        #    # I want the corresponding err values for these delay values  \n",
    "        #    train_bin_err_mean[bin_ind-1] = (train_bin_err_mean[bin_ind-1] + \n",
    "        #                                     np.sum(np.abs(y_train[bin_indices == bin_ind] - yhat_train[bin_indices == bin_ind]) ))\n",
    "        #    train_bin_baseline_err_mean[bin_ind-1] = (train_bin_baseline_err_mean[bin_ind-1] + \n",
    "        #                                             np.sum(np.abs(y_train[bin_indices == bin_ind] - train_baseline_vals[bin_indices == bin_ind]) ))\n",
    "        #    train_bin_perc_err_mean[bin_ind-1] = (train_bin_perc_err_mean[bin_ind-1] + \n",
    "        #                                          np.sum(np.abs((y_train[bin_indices == bin_ind] - yhat_train[bin_indices == bin_ind]))/(y_train[bin_indices == bin_ind]) ))\n",
    "        #    train_bin_baseline_perc_err_mean[bin_ind-1] = (train_bin_baseline_perc_err_mean[bin_ind-1] + \n",
    "        #                                                  np.sum(np.abs((y_train[bin_indices == bin_ind] - train_baseline_vals[bin_indices == bin_ind]))/(y_train[bin_indices == bin_ind])) )\n",
    "        #\n",
    "        ## bin index for each delay value\n",
    "        #bin_indices = np.digitize(y_test, bin_edges)\n",
    "        #\n",
    "        ## I want to take all the delay values for each bin \n",
    "        #for bin_ind in np.unique(bin_indices):\n",
    "        #    # these are the delay values in bin bin_edges[bin_ind]\n",
    "        #    test_bin_uldelay_mean[bin_ind-1] = test_bin_uldelay_mean[bin_ind-1] + np.sum(y_test[bin_indices == bin_ind])\n",
    "        #    test_bin_count[bin_ind-1] = test_bin_count[bin_ind-1] + len(y_test[bin_indices == bin_ind])\n",
    "        #    \n",
    "        #    # I want the corresponding err values for these delay values\n",
    "        #    test_bin_err_mean[bin_ind-1] = (test_bin_err_mean[bin_ind-1] + \n",
    "        #                                    np.sum(np.abs(y_test[bin_indices == bin_ind] - yhat_test[bin_indices == bin_ind])) )\n",
    "        #    test_bin_baseline_err_mean[bin_ind-1] = (test_bin_baseline_err_mean[bin_ind-1] + \n",
    "        #                                             np.sum(np.abs(y_test[bin_indices == bin_ind] - test_baseline_vals[bin_indices == bin_ind])))\n",
    "        #    test_bin_perc_err_mean[bin_ind-1] = ( test_bin_perc_err_mean[bin_ind-1] + \n",
    "        #                                         np.sum(np.abs((y_test[bin_indices == bin_ind] - yhat_test[bin_indices == bin_ind]))/(y_test[bin_indices == bin_ind])) )\n",
    "        #    test_bin_baseline_perc_err_mean[bin_ind-1] = (test_bin_baseline_perc_err_mean[bin_ind-1] + \n",
    "        #                                                  np.sum(np.abs((y_test[bin_indices == bin_ind] - test_baseline_vals[bin_indices == bin_ind]))/(y_test[bin_indices == bin_ind])) ) \n",
    "    #\n",
    "        ## Plot\n",
    "        #plot_y_yhat(y_train, yhat_train, y_test, yhat_test, model_save_path, IN_PARAM)\n",
    "    #\n",
    "        ## Convert the regression output to class labels and do confusion matrix\n",
    "        #cf_matrix = confusion_matrix(value_to_class_label(y_test, delay_class_edges), \n",
    "        #                             value_to_class_label(yhat_test, delay_class_edges), normalize='true')\n",
    "        #sns.set(rc={'figure.figsize':(8,7)}, font_scale = 1.5)\n",
    "        #sns.heatmap(cf_matrix, annot=True, \n",
    "        #    fmt='.1%', cmap='Blues')\n",
    "        #\n",
    "        #\n",
    "        #=============================================== plot q-q prediction versus ground truth  ==================================\n",
    "        #\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.plot(y_test, yhat_test, 'b.')\n",
    "        plt.plot([0,0], [delay_drop_th, delay_drop_th], 'k-')\n",
    "        plt.xlabel('Test samples ground truth (ms)')\n",
    "        plt.ylabel('Test samples prediction (ms)')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "        if not os.path.isdir(model_save_path):\n",
    "            os.makedirs(model_save_path)\n",
    "        ##=============================================== plot feature importance ==================================\n",
    "        ## \n",
    "        ## The length of importances reflects the number of features used\n",
    "        #importances = model.feature_importances_\n",
    "        ## increasing order in value and hence decreasing order in importance \n",
    "        ## sort the importances and then fetch the index value of those importances \n",
    "        #indices = np.argsort(importances)\n",
    "        ##This is in ascending order of \n",
    "        #bar_vals = importances[np.flip(indices)[0:feat_filter]]\n",
    "        #bar_names = X_feats[np.flip(indices)[0:feat_filter]]\n",
    "        ##print(importances[np.flip(indices)[0:feat_filter]])\n",
    "        #print(X_feats[np.flip(indices)[0:feat_filter]])\n",
    "        #\n",
    "        #top_n_features = list( set(top_n_features).union(set(bar_names)))\n",
    "        #print('Top n feature list: ', top_n_features)\n",
    "        #plt.figure()\n",
    "        #plt.barh(range(len(bar_vals)), np.flip(bar_vals), color='b', align='center')\n",
    "        #plt.yticks(range(len(bar_vals)), np.flip(bar_names))\n",
    "        #\n",
    "        #plt.title('Feature importance XGB')\n",
    "        #plt.xlabel('Relative Importance')\n",
    "        #plt.savefig('feat_imp'+str(IN_PARAM['rand_seed'])+'.pdf', bbox_inches='tight')\n",
    "        #plt.show() \n",
    "        \n",
    "print('')\n",
    "print('')\n",
    "print('===============================  DONE  ===================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05463b85-be30-4c51-97a1-0386c3e1ad18",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_PARAM['test_prec'], OUT_PARAM['test_rec'], test_fscore, test_support = precision_recall_fscore_support(yhat_test, y_test, average='macro')\n",
    "print('ML model: Macro (acc, prec, recall) for test set: ',  OUT_PARAM['test_acc'], OUT_PARAM['test_prec'], OUT_PARAM['test_rec'])\n",
    "OUT_PARAM['test_prec'], OUT_PARAM['test_rec'], test_fscore, test_support = precision_recall_fscore_support(yhat_test, y_test, average='micro')\n",
    "print('ML model: Micro (acc, prec, recall) for test set: ',  OUT_PARAM['test_acc'], OUT_PARAM['test_prec'], OUT_PARAM['test_rec'])\n",
    "OUT_PARAM['test_prec'], OUT_PARAM['test_rec'], test_fscore, test_support = precision_recall_fscore_support(yhat_test, y_test, average='weighted')\n",
    "print('ML model: Weighted (acc, prec, recall) for test set: ',  OUT_PARAM['test_acc'], OUT_PARAM['test_prec'], OUT_PARAM['test_rec'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25bd60c-fcb5-432e-a9f5-a1bbfba82fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if classification:\n",
    "    print('Average: (acc, prec, recall) for test set over runs: ', avg_acc_over_runs, avg_prec_over_runs, avg_rec_over_runs)\n",
    "else:    \n",
    "    print('Average', IN_PARAM['eval_metric'], ' over the runs for ML model is: ', avg_err_over_runs/len(run_seeds))\n",
    "    print('Average', IN_PARAM['eval_metric'], ' over the runs on the baseline is: ', avg_baseline_err_over_runs/len(run_seeds))\n",
    "    print('-----------------------------------------------------------')\n",
    "    print('Top n feature list size: ', len(top_n_features))\n",
    "    print(top_n_features)\n",
    "    print('-----------------------------------------------------------')\n",
    "\n",
    "    print('Loss fun: ', IN_PARAM['loss'])\n",
    "    print('Eval err fun: ', IN_PARAM['eval_metric'])\n",
    "    # After going over all runs     \n",
    "    fig, ax1 = plt.subplots(figsize=(16, 4))\n",
    "    ax2 = ax1.twinx()\n",
    "    ax1.plot(train_bin_uldelay_mean/train_bin_count, train_bin_err_mean/train_bin_count, 'r.-', label='XGB pred err (truth-pred)')\n",
    "    #ax1.plot(train_bin_uldelay_mean/train_bin_count, train_bin_baseline_err_mean/train_bin_count, 'm.-', label='baseline pred err (truth-pred)')\n",
    "    ax1.axhline(y=0, color='r', linestyle='--')\n",
    "    ax1.set_ylabel('error')\n",
    "    #ax1.set_ylim(-5,25)\n",
    "    ax1.legend()\n",
    "    plt.xlabel('uplink delay (ms)')\n",
    "    plt.title('Train samples')\n",
    "    plt.grid()\n",
    "    ax2.set_ylabel('relative err')\n",
    "    ax2.plot(train_bin_uldelay_mean/train_bin_count, train_bin_perc_err_mean/train_bin_count, 'g*-', label='XGB relative err')\n",
    "    #ax2.plot(train_bin_uldelay_mean/train_bin_count, train_bin_baseline_perc_err_mean/train_bin_count, 'c*-', label='baseline relative err')\n",
    "    ax2.axhline(y=0, color='g', linestyle='--')\n",
    "    plt.xscale('log')\n",
    "    ax1.legend(loc=6)\n",
    "    ax2.legend(loc=1)\n",
    "    plt.show() \n",
    "\n",
    "    plt.figure(figsize=(16, 2))\n",
    "    plt.plot(train_bin_uldelay_mean/train_bin_count, train_bin_count, 'b*-')\n",
    "    plt.xlabel('Train samples uplink delay bin (ms)')\n",
    "    plt.xscale('log')\n",
    "    plt.ylabel('bin count')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    print('Loss fun: ', IN_PARAM['loss'])\n",
    "    print('Eval err fun: ', IN_PARAM['eval_metric'])\n",
    "    fig, ax1 = plt.subplots(figsize=(16, 4))\n",
    "    ax2 = ax1.twinx()\n",
    "    ax1.plot(test_bin_uldelay_mean/test_bin_count, test_bin_err_mean/test_bin_count, 'r.-', label='XGB pred err (truth-pred)')\n",
    "    #ax1.plot(test_bin_uldelay_mean/test_bin_count, test_bin_baseline_err_mean/test_bin_count, 'm.-', label='baseline pred err (truth-pred)')\n",
    "    ax1.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.xlabel('uplink delay (ms)')\n",
    "    ax1.set_ylabel('error')\n",
    "    #ax1.set_ylim(-500,250)\n",
    "    ax1.legend()\n",
    "    plt.title('Test samples')\n",
    "    plt.grid()\n",
    "    ax2.set_ylabel('relative err')\n",
    "    ax2.plot(test_bin_uldelay_mean/test_bin_count, test_bin_perc_err_mean/test_bin_count, 'g*-', label='relative err')\n",
    "    #ax2.plot(test_bin_uldelay_mean/test_bin_count, test_bin_baseline_perc_err_mean/test_bin_count, 'c*-', label='baseline relative err')\n",
    "    ax2.axhline(y=0, color='g', linestyle='--')\n",
    "    plt.xscale('log')\n",
    "    ax1.legend(loc=6)\n",
    "    ax2.legend(loc=1)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(16, 2))\n",
    "    plt.plot(test_bin_uldelay_mean/test_bin_count, test_bin_count, 'b*-')\n",
    "    plt.xlabel('Test samples uplink delay bins (ms)')\n",
    "    plt.ylabel('bin count')\n",
    "    plt.xscale('log')\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b81397-4e4e-4414-b254-36f55d8620d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_save_str = 'clas_using_loss_default_softprob'\n",
    "os.system('cp train_and_eval_ML_models.ipynb '+'./saved_notebooks/'+notebook_save_str+'.ipynb')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8364a61b-1675-46a1-a5c0-8c343eaa43f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_union = ['ul_loadUe_weightBand', 'ul_loadUe_BSRestimate', 'ul_BSRestimateLcg6', 'ul_loadUe_deltaIcc', 'ul_nrOfCsiPart1Bits', 'ul_loadUe_RI', 'dl_pucchFormatType', 'ul_loadUe_BSValueLcg6', 'ul_rv', 'ul_tbSizeInBits', 'dl_loadUe_dciFormat', 'dl_loadUe_pucchResourceIndicator', 'nbm_loadUe_WBeamIndex', 'ul_loadUe_fda', 'dl_NBeamRsrpCurrent', 'dl_loadUe_transmissionAttempt', 'dl_loadUe_bbBearerRef7', 'ul_BSRestimateLcg4', 'dl_numOfPrbs', 'ul_loadUe_isTransformPrecoding', 'ul_loadUe_timingOffset', 'dl_numberOfSymbols', 'ul_BSValueLcg0', 'dl_tbSizeInBits', 'ul_loadUe_VBit', 'ul_loadUe_BSRestimateLcg2', 'ul_csiRequest', 'ul_loadUe_BSValueLcg4', 'dl_loadUe_weightBand', 'wbm_rsrp1', 'dl_loadUe_NBeamRsrpCurrent', 'ul_tpcCommand', 'ul_iccAchievable', 'ul_loadUe_preamblePwr', 'nbm_beam03', 'dl_drb5data', 'dl_pucchSfn', 'ul_loadUe_numOfLayers', 'nbm_rsrp01', 'dl_icc', 'ul_loadUe_iccAchievable', 'ul_fda', 'ul_loadUe_uciDecodedResult', 'ul_loadUe_BSValueLcg0', 'dl_dlCcIndex', 'nbm_loadUe_rsrp11', 'wbm_beam4', 'dl_loadUe_incrementalWeight', 'dl_loadUe_antennaPorts', 'ul_BSValueLcg5', 'ul_RI', 'dl_loadUe_fda', 'dl_loadUe_numBearers', 'ul_loadUe_csiRequest', 'dl_linkAdaptationUeMode', 'wbm_loadUe_cellId', 'ul_loadUe_ACK', 'dl_cellId', 'dl_bbBearerRef2', 'ul_loadUe_BSValueLcg7', 'dl_loadUe_cqi', 'dl_pdschHarqFeedbackTiming', 'dl_loadUe_pucchSfn', 'dl_loadUe_pdschHarqFeedbackTiming', 'ul_loadUe_isClpcSaturated', 'ul_loadUe_startSymbolPusch', 'dl_loadUe_dlCcIndex', 'dl_loadUe_drb7input', 'dl_bbBearerRef7', 'dl_loadUe_bbBearerRef2', 'ul_loadUe_beamIndex', 'dl_rank', 'dl_loadUe_drb4input', 'dl_numBearers', 'wbm_loadUe_beam2', 'nbm_loadUe_beam01', 'ul_loadUe_WBeamRsrpCurrent', 'ul_carrierAggregationUsed', 'nbm_loadUe_rsrp12', 'dl_drb4input', 'dl_mcsIndex', 'dl_sinr', 'dl_drb4data', 'ul_loadUe_harqProcessId', 'dl_transmissionAttempt', 'ul_loadUe_BSRestimateLcg7', 'dl_loadUe_isDrxEnabled', 'dl_drb0data', 'dl_loadUe_numOfPrbs', 'dl_loadUe_drb4data', 'ul_loadUe_numberOfSymbolsPusch', 'nbm_loadUe_ueTraceIdMsw', 'wbm_loadUe_beam3', 'dl_sectorIndex', 'dl_drb1input', 'dl_numberOfSrcBits', 'nbm_beam13', 'ul_clpcCarrierDemand', 'ul_wbUsedNbOverridden', 'ul_loadUe_puschTotalRxPsdAvg', 'dl_srOnPucch', 'dl_loadUe_sectorIndex', 'wbm_beam3', 'dl_drb0input', 'ul_BSRestimate', 'ul_BSRestimateLcg1', 'dl_loadUe_taValue', 'ul_maxRank', 'wbm_beam1', 'dl_loadUe_bbBearerRef3', 'dl_physicalCellId', 'ul_totalCellsReqScheduling', 'wbm_loadUe_beam4', 'nbm_loadUe_rsrp02', 'wbm_loadUe_WBeamIndexNewBest', 'dl_loadUe_WBeamRsrpCurrent', 'nbm_WBeamIndex', 'dl_loadUe_slot', 'dl_bbBearerRef1', 'dl_drb3data', 'nbm_loadUe_rsrp13', 'ul_macSduInBytes', 'dl_loadUe_srOnPucch', 'ul_measNumOfPrb', 'ul_transmissionAttempt', 'ul_loadUe_BSRestimateLcg3', 'dl_loadUe_deltaIcc', 'dl_dciFormat', 'ul_bbCellIndex', 'dl_loadUe_macCtrlElement', 'dl_weightBand', 'ul_loadUe_mcsIndex', 'ul_loadUe_antennaPorts', 'ul_timingOffset', 'dl_redundancyVersion', 'ul_loadUe_BSRestimateLcg1', 'ul_loadUe_wbUsedNbOverridden', 'dl_numberOfActivatedDlCells', 'dl_antennaPorts', 'dl_drb7input', 'dl_harqProcessId', 'nbm_loadUe_beam13', 'ul_loadUe_nrOfCsiPart1Bits', 'ul_dciFormat', 'dl_pucchSlotNo', 'nbm_NBeamIndexChosen', 'ul_precodingInfo', 'ul_loadUe_NBeamRsrpCurrent', 'ul_loadUe_harqFailure', 'dl_bbBearerRef4', 'dl_ndi', 'nbm_beam12', 'dl_loadUe_tda', 'nbm_loadUe_cellId', 'wbm_loadUe_WBeamIndexCurrent', 'ul_ndi', 'dl_loadUe_tbSizeInBits', 'wbm_cellId', 'dl_loadUe_icc', 'nbm_loadUe_beam02', 'dl_pucchResourceIndicator', 'ul_BSValueLcg2', 'ul_loadUe_macSduInBytes', 'ul_measNumOfLayers', 'dl_loadUe_drb1input', 'dl_WBeamRsrpCurrent', 'nbm_loadUe_noOfCriPerCsiReport', 'ul_antennaPorts', 'ul_loadUe_slot', 'wbm_beam2', 'ul_loadUe_startPrb', 'ul_slot', 'nbm_loadUe_rsrp01', 'ul_BSRestimateLcg0', 'ul_loadUe_numOfPrbs', 'ul_isDrxEnabled', 'ul_loadUe_BSRestimateLcg0', 'wbm_rsrp4', 'ul_BSValueLcg4', 'dl_loadUe_pucchSlotNo', 'ul_preamblePwr', 'wbm_loadUe_beam1', 'dl_ACK', 'ul_isPrimaryCell', 'dl_tda', 'ul_loadUe_postEqSinr0', 'dl_loadUe_sinr', 'ul_loadUe_pCmaxCIndex', 'ul_loadUe_cellId', 'dl_loadUe_physicalCellId', 'dl_loadUe_beamIndex', 'dl_loadUe_numberOfSymbols', 'dl_loadUe_drb2data', 'dl_loadUe_drb1data', 'ul_BSRestimateLcg7', 'ul_numOfLayers', 'ul_loadUe_numberOfActivatedUlCells', 'ul_loadUe_BSValueLcg1', 'nbm_beam11', 'ul_loadUe_BSValueLcg5', 'dl_loadUe_newData/reTx(1/0)', 'dl_loadUe_bbBearerRef6', 'ul_loadUe_bbCellIndex', 'nbm_loadUe_NBeamIndexChosen', 'dl_loadUe_numberOfActivatedDlCells', 'ul_isClpcSaturated', 'ul_loadUe_BSRestimateLcg5', 'ul_loadUe_tbSizeInBits', 'ul_BSValueLcg1', 'ul_loadUe_maxRank', 'dl_fda', 'ul_DTX', 'ul_loadUe_clpcCarrierDemand', 'dl_cqi', 'dl_dai', 'wbm_rsrp3', 'ul_numOfPrbs', 'dl_drb3input', 'ul_puschTotalRxPsdAvg', 'nbm_loadUe_NBeamIndexCurrent', 'nbm_cellId', 'ul_loadUe_postEqSinr1', 'dl_loadUe_drb3input', 'ul_pCmaxCIndex', 'ul_numberOfSymbolsPusch', 'dl_loadUe_mcsIndex', 'dl_newData/reTx(1/0)', 'ul_weightBand', 'nbm_rsrp02', 'ul_uciDecodedResult', 'ul_loadUe_BSValueLcg2', 'ul_powerHeadRoomIndex', 'ul_loadUe_rv', 'ul_incrementalWeight', 'dl_loadUe_drb2input', 'dl_loadUe_drb7data', 'dl_drb6input', 'wbm_rsrp2', 'dl_loadUe_bbCellIndex', 'dl_bbBearerRef3', 'dl_loadUe_feedbackIndex', 'ul_startPrb', 'ul_cellId', 'wbm_loadUe_rsrp1', 'dl_loadUe_rank', 'ul_ACK', 'ul_beamIndex', 'dl_isSrBitIncluded', 'ul_numberOfActivatedUlCells', 'dl_bbBearerRef6', 'ul_loadUe_transmissionAttempt', 'dl_loadUe_isPrimaryCell', 'wbm_loadUe_rsrp4', 'ul_BSValueLcg3', 'dl_macCtrlElement', 'dl_drb6data', 'dl_isPrimaryCell', 'ul_harqFailure', 'dl_bbCellIndex', 'ul_loadUe_totalCellsReqScheduling', 'ul_postEqSinr0', 'ul_loadUe_carrierAggregationUsed', 'dl_loadUe_bbBearerRef4', 'ul_loadUe_powerHeadRoomIndex', 'ul_BSValueLcg6', 'nbm_loadUe_beam03', 'dl_drb2data', 'nbm_rsrp13', 'dl_isDrxEnabled', 'ul_loadUe_measNumOfPrb', 'dl_feedbackIndex', 'ul_loadUe_isDrxEnabled', 'dl_loadUe_bbBearerRef1', 'ul_loadUe_ulRequestTypeBitmap', 'nbm_rsrp03', 'dl_slot', 'dl_loadUe_drb5input', 'ul_loadUe_BSRestimateLcg4', 'dl_beamIndex', 'dl_loadUe_isSrBitIncluded', 'ul_BSRestimateLcg3', 'ul_WBeamRsrpCurrent', 'dl_loadUe_DTX', 'nbm_rsrp11', 'ul_BSRestimateLcg5', 'dl_loadUe_redundancyVersion', 'ul_ulRequestTypeBitmap', 'wbm_WBeamIndexNewBest', 'ul_ulschIndicator', 'dl_drb1data', 'dl_drb2input', 'ul_loadUe_BSRestimateLcg6', 'nbm_loadUe_rsrp03', 'dl_loadUe_wbUsedNbOverridden', 'dl_loadUe_drb6input', 'nbm_loadUe_beam11', 'nbm_beam01', 'dl_loadUe_linkAdaptationUeMode', 'ul_loadUe_linkAdaptationUeMode', 'dl_loadUe_bbBearerRef0', 'dl_drb5input', 'dl_loadUe_drb6data', 'ul_loadUe_BSValueLcg3', 'dl_loadUe_drb0data', 'ul_loadUe_sinrAchievable', 'nbm_loadUe_beam12', 'ul_loadUe_incrementalWeight', 'dl_loadUe_dai', 'dl_loadUe_ueTraceIdMsw', 'ul_loadUe_ndi', 'ul_startSymbolPusch', 'dl_loadUe_cellId', 'ul_isTransformPrecoding', 'dl_loadUe_drb5data', 'dl_loadUe_drb3data', 'ul_deltaIcc', 'nbm_noOfCriPerCsiReport', 'dl_loadUe_drb0input', 'ul_mcsIndex', 'ul_chipsetType', 'dl_loadUe_numberOfSrcBits', 'nbm_beam02', 'ul_postEqSinr1', 'ul_loadUe_isPrimaryCell', 'dl_incrementalWeight', 'dl_loadUe_ACK', 'ul_BSRestimateLcg2', 'ul_NBeamRsrpCurrent', 'dl_DTX', 'dl_bbBearerRef5', 'dl_loadUe_bbBearerRef5', 'ul_harqProcessId', 'dl_loadUe_pucchFormatType', 'wbm_loadUe_rsrp2', 'dl_taValue', 'ul_loadUe_precodingInfo', 'ul_BSValueLcg7', 'wbm_loadUe_ueTraceIdMsw', 'dl_loadUe_ndi', 'dl_deltaIcc', 'dl_drb7data', 'ul_loadUe_DTX', 'ul_linkAdaptationUeMode', 'wbm_loadUe_rsrp3', 'nbm_NBeamIndexCurrent', 'dl_loadUe_harqProcessId', 'ul_loadUe_ulschIndicator', 'dl_wbUsedNbOverridden', 'ul_loadUe_measNumOfLayers', 'nbm_rsrp12', 'dl_bbBearerRef0', 'wbm_WBeamIndexCurrent', 'ul_sinrAchievable']\n",
    "print(feat_union)\n",
    "print(len(feat_union))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f8db5a-6547-41cb-9cdc-cf6f0587b854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of featurtes I think will not generalize \n",
    "filtered_cols = [i for i in feat_union if 'eamIndex' in i]\n",
    "print(filtered_cols)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
