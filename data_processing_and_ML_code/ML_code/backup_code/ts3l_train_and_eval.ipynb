{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c4ee135-3899-4642-8cdb-7b50955efdf1",
   "metadata": {},
   "source": [
    "# Load libraries and functions and Initialize  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78b018a7-8f64-468a-bea0-affb438d8d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Run with random seed:  561\n",
      "GPU is available.\n",
      "1\n",
      "True\n",
      "12.1\n"
     ]
    }
   ],
   "source": [
    "# reload\n",
    "#%reset\n",
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "\n",
    "import warnings\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# DEBUG MODE\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.getLogger('tensorflow').setLevel(logging.FATAL)\n",
    "    \n",
    "from helper_functions import *\n",
    "\n",
    "# Sets the random seed\n",
    "# If you want to set a specific seed then pass it as an argument \n",
    "initialize(561)\n",
    "iteration = 'iteration2'\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "pt_type = 'scarf'#['dae', 'vime', 'scarf', 'subtab', 'switchtab']\n",
    "\n",
    "freeze_encoder = True \n",
    "\n",
    "# If True then we are predicting one window ahead if False then we are predicting on the same window \n",
    "shift_samp_for_predict = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1584762f-0cc1-42ac-a4b8-7fe47d1afcbe",
   "metadata": {},
   "source": [
    "# Set string for saving results and code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbb488fa-0ce3-4a50-9997-49dad143407c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#'sup_singleStep_nextWindow_mlp' \n",
    "#'sup_past5Steps_nextWindow_xgb'\n",
    "notebook_save_str = iteration+'_s3l_unfrozen_'+pt_type\n",
    "\n",
    "models_folder = 'models/'+pt_type+'_pretrain/'\n",
    "train_results_filepath = notebook_save_str+'_train_results.csv'\n",
    "test_results_filepath = notebook_save_str+'_test_results.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2b412b-e4db-47f5-8748-ffd21395ec49",
   "metadata": {},
   "source": [
    "# Set Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35cc4bea-15f6-4423-8f89-b840b0f4ee58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================================\n",
    "# Experiment Parameters: Check carefully!\n",
    "#========================================\n",
    "time_step_size = '500ms'\n",
    "\n",
    "# This is where models are saved and loaded from\n",
    "#dataset_folder = '../../../dataset_ver1/parsed_data_'+time_step_size+'_5steps/' # 500 ms with 5 100ms steps \n",
    "dataset_folder = '../../../dataset_ver1/parsed_data_'+time_step_size+'_singleStep/'\n",
    "\n",
    "pretrain_slice = 'all' #['macro', 'micro', 'slow', 'fast', 'all']\n",
    "train_slice = 'all' #['macro', 'micro', 'slow', 'fast', 'all', 'only_delay']\n",
    "test_slice = 'all' #['macro', 'micro', 'slow', 'fast', 'all']\n",
    "\n",
    "EXP_PARAM = {\n",
    "    'scaler': 'standard', #'minmax', 'standard', 'robust', 'maxabs', 'l2norm'\n",
    "    'num_rand_runs': 3 # number of runs with each run doing a different random sample of size label_no from the set of labeled samples \n",
    "}\n",
    "\n",
    "\n",
    "num_samples_list = [100, 1*K, 10*K, 20*K] \n",
    "#num_samples_list = [20*K]\n",
    "\n",
    "#==================================\n",
    "# Pretraining Experiment Parameters\n",
    "#==================================\n",
    "\n",
    "pretrain = False\n",
    "pretrain_type = 's3l_'+pt_type\n",
    "pretrain_model_to_save_name = pretrain_slice+'_'+pretrain_type\n",
    "\n",
    "scaler_to_save_name = pretrain_type+'_'+EXP_PARAM['scaler']+'_scaler'\n",
    "\n",
    "#==========================================\n",
    "# Supervised Training Experiment Parameters\n",
    "#==========================================\n",
    "\n",
    "# Load an existing pretrained model to use as encoding for sup model \n",
    "use_pretrained_model = False\n",
    "pretrain_model_to_load_type = 's3l_'+pt_type\n",
    "pretrain_model_to_load_name = pretrain_slice+'_'+pretrain_type\n",
    "scaler_to_load_name = pretrain_type+'_'+EXP_PARAM['scaler']+'_scaler.pkl'\n",
    "\n",
    "# Train a sup model with or without using a pretrained model \n",
    "sup_model_type = 'mlp' # xgb\n",
    "suptrain_model_to_save_name = iteration+'_'+train_slice+'_'+pretrain_model_to_load_type+'_'+sup_model_type # could also be sup_model_with_pretrain \n",
    "\n",
    "\n",
    "#==================================================\n",
    "# Experiment Parameters: Not often changed\n",
    "#==================================================\n",
    "\n",
    "# When input features are NA \n",
    "# Could experiment with forward fill imputation \n",
    "# If the label is NA during supervised training then the sample is dropped  \n",
    "impute_method = 'forward_fill'# ['forward_fill', 'zero_fill']\n",
    "                              \n",
    "# These are the ones we have chosen to work with  \n",
    "#learning_tasks = ['httpClientRtt_trace.txt_page_load_time']\n",
    "learning_tasks = [#'dashClient_trace.txt_newBitRate_bps', \n",
    "                  'vrFragment_trace.txt_vr_frag_thput_mbps', 'vrFragment_trace.txt_vr_burst_thput_mbps',\n",
    "                  'vrFragment_trace.txt_vr_frag_time', 'vrFragment_trace.txt_vr_burst_time', \n",
    "                  #'httpClientRtt_trace.txt_page_load_time',\n",
    "                  'delay_trace.txt_ul_delay', 'delay_trace.txt_dl_delay']\n",
    "# index matched with the learning_tasks above\n",
    "#learning_task_types = ['reg']\n",
    "learning_task_types = [#'clas', \n",
    "                       'reg', 'reg', \n",
    "                       'reg', 'reg',\n",
    "                       #'reg',\n",
    "                       'reg', 'reg']\n",
    "\n",
    "    \n",
    "# If you want the test samples to be sorted by delay value to see the error differences for the low delay and high delay cases \n",
    "sort_test_samples = False\n",
    "\n",
    "use_all_feats = True\n",
    "# take the top n features of each run and add it to the top_n_features list  \n",
    "# If use_all_feats = True then thes will not be used \n",
    "feat_filter = 10 \n",
    "top_n_features = []\n",
    "# Only valid when use_all_feats = False \n",
    "selected_features = []\n",
    "\n",
    "# All delay values above this will be clipped to the threshold value\n",
    "clip_outliers = True\n",
    "delay_clip_th = 5000 # ms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c8836e-fcf1-49f0-b0e0-5bf2f9c84784",
   "metadata": {},
   "source": [
    "# Set Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d13f460-1281-4373-984e-32176a8450de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for SSL using DAE \n",
    "#print('WARNING !!! HYPERPARAMETERS IN DEBUG MODE')\n",
    "# Hyperparameters for supervised XGB training \n",
    "hypp_sup_xgb={\n",
    "    'loss':{'reg':'mse',\n",
    "            'clas':'categorical_crossentropy'}\n",
    "} \n",
    "\n",
    "# Hyperparameters for supervised MLP training\n",
    "hypp_sup_mlp={\n",
    "    'fc_layers': [500], # the hidden layers\n",
    "    'batch_size': 64,  \n",
    "    'max_epochs': 500,\n",
    "    'patience': 15,\n",
    "    'learning_rate': 0.0001,\n",
    "    'loss':{'reg':'mse',#'mean_absolute_error' 'mean_absolute_percentage_error'\n",
    "            'clas':'categorical_crossentropy'},\n",
    "    'metrics':{'reg':['MeanAbsolutePercentageError'],\n",
    "               'clas':['Recall']},#'F1Score' does nto work needs a different dimension \n",
    "    'out_activation':{'reg':'linear', \n",
    "                      'clas':'softmax'} \n",
    "} \n",
    "\n",
    "s3l_sup_mlp={\n",
    "    'batch_size': 32,\n",
    "    'max_epochs': 30,\n",
    "    'patience': 5,\n",
    "    'loss':{'reg':'MSELoss',\n",
    "            'clas':'CrossEntropyLoss'},\n",
    "    'metrics':{'reg':['MeanAbsolutePercentageError'],\n",
    "               'clas':['Recall']},#'F1Score' does not work needs a different dimension \n",
    "}\n",
    "\n",
    "s3l_hyp_ssl_dae={\n",
    "    'metric': \"mean_absolute_percentage_error\",\n",
    "    'hidden_dim': 500,\n",
    "    'encoder_depth': 4,\n",
    "    'head_depth': 2,\n",
    "    'noise_type': \"Swap\",\n",
    "    'noise_ratio': 0.3,\n",
    "    'max_epochs': 20,\n",
    "    'batch_size': 128\n",
    "}\n",
    "\n",
    "s3l_hyp_ssl_vime={\n",
    "    'metric': \"mean_absolute_percentage_error\",\n",
    "    'hidden_dim': 500,\n",
    "    'p_m': 0.3, # Corruption probability for self-supervised learning\n",
    "    'alpha1': 2.0, # Hyper-parameter to control the weights of feature and mask losses\n",
    "    'alpha2': 2.0, # Hyper-parameter to control the weights of feature and mask losses\n",
    "    'K': 3, # Number of augmented samples\n",
    "    'beta': 1.0, # Hyperparameter to control supervised and unsupervised losses\n",
    "    'max_epochs': 20,\n",
    "    'batch_size': 128\n",
    "}\n",
    "\n",
    "s3l_hyp_ssl_scarf={\n",
    "    'metric': \"mean_absolute_percentage_error\",\n",
    "    'hidden_dim': 500,\n",
    "    'encoder_depth': 4,\n",
    "    'head_depth': 2,\n",
    "    'corruption_rate': 0.6,\n",
    "    'max_epochs': 20,\n",
    "    'batch_size': 128,\n",
    "    'dropout_rate': 0.04\n",
    "}\n",
    "\n",
    "s3l_hyp_ssl_subtab={\n",
    "    'metric': \"mean_absolute_percentage_error\",\n",
    "    'hidden_dim': 500,\n",
    "    'encoder_depth': 4,\n",
    "    'head_depth': 2,\n",
    "    'tau': 1.0,\n",
    "    'use_cosine_similarity': True,\n",
    "    'use_contrastive': True,\n",
    "    'use_distance': True,\n",
    "    'n_subsets': 4,\n",
    "    'overlap_ratio': 0.75,\n",
    "    'mask_ratio': 0.1,\n",
    "    'noise_type': \"Swap\",\n",
    "    'noise_level': 0.1,\n",
    "    'max_epochs': 20,\n",
    "    'batch_size': 128    \n",
    "}\n",
    "\n",
    "s3l_hyp_ssl_switchtab={\n",
    "    'metric': \"mean_absolute_percentage_error\",\n",
    "    'hidden_dim': 500,\n",
    "    'encoder_depth': 4,\n",
    "    'n_head': 2,\n",
    "    'u_label': -1,\n",
    "    'max_epochs': 20,\n",
    "    'batch_size': 128    \n",
    "}\n",
    "\n",
    "# Hyperparameters for SSL using TabNet\n",
    "s3l_hyp_ssl_tabnet={\n",
    "    #'lambda_sparse': , # default = 1e-3\n",
    "    # This is the extra sparsity loss coefficient as proposed in the original paper. \n",
    "    # The bigger this coefficient is, the sparser your model will be in terms of feature selection. \n",
    "    # Depending on the difficulty of your problem, reducing this value could help.\n",
    "    'mask_type': 'entmax', # 'entmax' # default='sparsemax'\n",
    "    'n_da': 8, # between 8-64 # default=8\n",
    "    'n_steps': 3, # between 3-10 # default=3\n",
    "    'n_independent': 2, # between 1-5 # default=2\n",
    "    'n_shared': 2, # between 1-5 # default=2\n",
    "    'n_shared_decoder': 1, # default=1\n",
    "    'n_indep_decoder': 1, # default=1\n",
    "    'noise_ratio': 0.30,\n",
    "    'batch_size': 1024, #default=1024\n",
    "    'max_epochs': 200, # default=200\n",
    "    'patience': 15 # default=10\n",
    "}\n",
    "\n",
    "\n",
    "if sup_model_type == 'mlp':\n",
    "    sup_hyper_params=hypp_sup_mlp\n",
    "elif sup_model_type == 'xgb':\n",
    "    sup_hyper_params=hypp_sup_xgb    \n",
    "\n",
    "\n",
    "#pretrain_runs = range(1, 10 + 1)\n",
    "#train_runs = range(11, 17 + 1)\n",
    "#test_runs = range(18, 20 + 1)\n",
    "\n",
    "num_pretrain_runs = 10 # 10\n",
    "num_train_runs = 7 # 7 \n",
    "num_test_runs = 3 # 3\n",
    "train_test_run_nums = np.array(range(11, 20+1))\n",
    "#train_test_run_nums = np.array(range(11, 12+1))\n",
    "\n",
    "\n",
    "pretrain_runs = range(1, 10 + 1)\n",
    "#pretrain_runs = range(1, 2 + 1)\n",
    "\n",
    "\n",
    "# Create the models_folder if it does not already exist\n",
    "if not os.path.isdir(models_folder):\n",
    "    os.makedirs(models_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8bcc97-fe15-45a3-a7c0-bbf0bdc3205e",
   "metadata": {},
   "source": [
    "# Load pretraining runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acaa45de-7ba8-4b21-958f-1654f4b9f0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pretrain:\n",
    "    pretrain_data = read_and_concatenate_runs(pretrain_runs, dataset_folder, pretrain_slice, network_info, time_step_size, \n",
    "                                              use_all_feats, drop_col_substr, learning_tasks, shift_samp_for_predict, \n",
    "                                              impute_method, sum_cols_substr, all_learning_tasks_in_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e9150b-52f3-427d-97fe-de8e6fa8074b",
   "metadata": {},
   "source": [
    "# Pretrain using TabularS3L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6931092-f399-418f-8f45-155e2561aa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First phase\n",
    "\n",
    "if pretrain:\n",
    "    # Remove the labels of all prediction tasks which are also in the datset \n",
    "    X_pretrain = pretrain_data.drop(all_learning_tasks_in_data, axis=1, errors='ignore')\n",
    "    X_pretrain = X_pretrain.dropna()\n",
    "    X_cols = X_pretrain.columns\n",
    "    continuous_cols, categorical_cols = get_cont_and_cat_cols(X_pretrain)\n",
    "    print(X_pretrain.shape)\n",
    "    # Create, save and use scaler\n",
    "    val_scaler = create_scaler(X_pretrain, EXP_PARAM['scaler'])\n",
    "    with open(models_folder + scaler_to_save_name +'.pkl', 'wb') as f:\n",
    "        pickle.dump(val_scaler, f)\n",
    "    \n",
    "    # transform returns a numpy even when you pass a pandas dataframe \n",
    "    X_pretrain = val_scaler.transform(X_pretrain).copy()\n",
    "    X_pretrain = pd.DataFrame(X_pretrain, columns=X_cols)\n",
    "\n",
    "    assert (pretrain_type in ['s3l_dae', 's3l_vime', 's3l_scarf', 's3l_subtab', 's3l_switchtab'], \n",
    "                        f\"Invalid pretrain_type: {pretrain_type}.\")\n",
    "    \n",
    "    if pretrain_type == 's3l_dae':\n",
    "        pretrain_model, trainer = s3l_pretrain_with_dae(X_pretrain, continuous_cols, categorical_cols, s3l_hyp_ssl_dae, \n",
    "                                                    models_folder+pretrain_model_to_save_name)\n",
    "    elif pretrain_type == 's3l_vime':\n",
    "        pretrain_model, trainer = s3l_pretrain_with_vime(X_pretrain, continuous_cols, categorical_cols, s3l_hyp_ssl_vime, \n",
    "                                                    models_folder+pretrain_model_to_save_name)\n",
    "    elif pretrain_type == 's3l_scarf':\n",
    "        pretrain_model, trainer = s3l_pretrain_with_scarf(X_pretrain, continuous_cols, categorical_cols, s3l_hyp_ssl_scarf, \n",
    "                                                    models_folder+pretrain_model_to_save_name)\n",
    "    elif pretrain_type == 's3l_subtab':\n",
    "        pretrain_model, trainer = s3l_pretrain_with_subtab(X_pretrain, continuous_cols, categorical_cols, s3l_hyp_ssl_subtab, \n",
    "                                                    models_folder+pretrain_model_to_save_name)\n",
    "    elif pretrain_type == 's3l_switchtab':\n",
    "        pretrain_model, trainer = s3l_pretrain_with_switchtab(X_pretrain, continuous_cols, categorical_cols, s3l_hyp_ssl_switchtab, \n",
    "                                                    models_folder+pretrain_model_to_save_name)\n",
    "        \n",
    "    # save this pretrained model for later use\n",
    "    trainer.save_checkpoint(models_folder+pretrain_model_to_save_name+'.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc2f281-d27a-4a89-aa5f-37ed55f63ddd",
   "metadata": {},
   "source": [
    "# Pretrain using OP libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f10d499c-46c2-445b-82cf-c97aa7468605",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pretrain:\n",
    "    # Separate the X and the ys from the data\n",
    "    # remove the labels of all prediction tasks which are also in the datset \n",
    "    X_pretrain = pretrain_data.drop(all_learning_tasks_in_data, axis=1, errors='ignore')\n",
    "    print(X_pretrain.shape)\n",
    "  \n",
    "    val_scaler = create_scaler(X_pretrain, EXP_PARAM['scaler'])\n",
    "    # Save the scaler for later use when doing supervised training \n",
    "    with open(models_folder + scaler_to_save_name +'.pkl', 'wb') as f:\n",
    "        pickle.dump(val_scaler, f)\n",
    "\n",
    "    # Use the scaler to scale\n",
    "    X_pretrain = val_scaler.transform(X_pretrain).copy()\n",
    "    \n",
    "    if pretrain_type == 'dae':\n",
    "        pretrain_model = pretrain_with_dae(pd.DataFrame(X_pretrain))\n",
    "        # save this pretrained model for later use \n",
    "        pretrain_model.save(models_folder+pretrain_model_to_save_name)\n",
    "    elif pretrain_type == 'tabnet':\n",
    "        X_pretrain, X_pretrain_val = train_test_split(X_pretrain, test_size=0.2, shuffle=True) \n",
    "        pretrain_model = pretrain_with_tabnet(pd.DataFrame(X_pretrain),  pd.DataFrame(X_pretrain_val))\n",
    "        pretrain_model.save_model(models_folder+pretrain_model_to_save_name)\n",
    "        # show the results of feature importance\n",
    "        tabnet_explain(pretrain_model, X_pretrain_val)\n",
    "    elif pretrain_type == 'vime':\n",
    "        # Train VIME-Self\n",
    "        vime_self_parameters = dict()\n",
    "        vime_self_parameters['batch_size'] = hyp_ssl_semi_vime['batch_size']\n",
    "        vime_self_parameters['epochs'] = hyp_ssl_semi_vime['epochs'] \n",
    "        vime_self_encoder = vime_self(X_pretrain, hyp_ssl_semi_vime['p_m'], hyp_ssl_semi_vime['alpha'], vime_self_parameters)  \n",
    "        # Save encoder\n",
    "        vime_self_encoder.save(models_folder+pretrain_model_to_save_name+'.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a8177f-b6ab-42b0-8925-7ca60abadc5a",
   "metadata": {},
   "source": [
    "# Load Tabnet pretrained model for explainability results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35c34521-0f09-4c03-a0b9-deb1d4ac0803",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Load a pretrained model for getting explainability results\n",
    "# Load the pretrained model and scaler \n",
    "#with open(models_folder + scaler_to_load_name + '.pkl', 'rb') as f:\n",
    "#    val_scaler = pickle.load(f)\n",
    "#\n",
    "#\n",
    "## remove the labels of all prediction tasks which are also in the datset \n",
    "#X_pretrain_val = test_data.drop(all_learning_tasks_in_data, axis=1, errors='ignore')\n",
    "#print(X_pretrain_val.shape)\n",
    "#\n",
    "## Fill with 0 the values that are missing in the input features so that the sample can still be used\n",
    "#X_pretrain_val = impute_data(X_pretrain_val, impute_method, sum_cols_substr)\n",
    "#\n",
    "#X_pretrain_val = val_scaler.transform(X_pretrain_val).copy()\n",
    "# \n",
    "#if pretrain_model_to_load_type == 'tabnet':\n",
    "#    pretrain_model = TabNetPretrainer()\n",
    "#    pretrain_model.load_model(models_folder+pretrain_model_to_load_name)\n",
    "#    X_pretrain_val = pretrain_model.predict(X_pretrain_val)[0]\n",
    "#    tabnet_explain(pretrain_model, X_pretrain_val)\n",
    "#else:\n",
    "#    print('Do not have explainability functionality for this model type')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44bd068-5356-4f57-97c0-42eb9fec0d25",
   "metadata": {},
   "source": [
    "# Iterate over num. labeled samples\n",
    "# Iterate over different random initializations  \n",
    "# Iterate over learning_tasks\n",
    "\n",
    "# Pre-process data \n",
    "# Train model\n",
    "# Evaluate model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9424b44-9151-46fa-9de6-a510c9081546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================\n",
      "NUM. LABELLED SAMPLES:  100\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "Random iteration:  0\n",
      "==========================================================================\n",
      "# train runs used  7\n",
      "# test runs used  3\n",
      "Concatenating runs:  [15 18 16 13 17 14 11]\n",
      "Time to read csv file for run:  3.001783609390259\n",
      "Loaded run 15\n",
      "Time to read csv file for run:  2.9613730907440186\n",
      "Loaded run 18\n",
      "Time to read csv file for run:  2.893611192703247\n",
      "Loaded run 16\n",
      "Time to read csv file for run:  2.812739849090576\n",
      "Loaded run 13\n",
      "Time to read csv file for run:  2.9138967990875244\n",
      "Loaded run 17\n",
      "Time to read csv file for run:  2.8742990493774414\n",
      "Loaded run 14\n",
      "Time to read csv file for run:  2.790099620819092\n",
      "Loaded run 11\n",
      "Concatenating runs:  [12 19 20]\n",
      "Time to read csv file for run:  2.916918992996216\n",
      "Loaded run 12\n",
      "Time to read csv file for run:  2.7056949138641357\n",
      "Loaded run 19\n",
      "Time to read csv file for run:  2.8722691535949707\n",
      "Loaded run 20\n",
      "Time to read data:  79.03659152984619\n",
      "================================================================================\n",
      "Learning task:  vrFragment_trace.txt_vr_frag_thput_mbps task type:  reg\n",
      "================================================================================\n",
      "Dropping rows for which this learning_tasks label is NA, since there is no ground truth\n",
      "(23124, 118)\n",
      "NOTE: removing samples that are above the 99th percentile\n",
      "NOTE: stratifying regression samples using 5 bins\n",
      "bin_and_remove_outliers\n",
      "Before removing bins that are not populated enough\n",
      "[0 1 2 3 4] [17193  1627  1404  1773   881]\n",
      "After removing bins that are not populated enough\n",
      "[0 1 2 3 4] [17193  1627  1404  1773   881]\n",
      "(22878, 108)\n",
      "(22878,)\n",
      "(22878,)\n",
      "Dropping rows for which this learning_tasks label is NA, since there is no ground truth\n",
      "(9862, 118)\n",
      "NOTE: removing samples that are above the 99th percentile\n",
      "NOTE: stratifying regression samples using 5 bins\n",
      "bin_and_remove_outliers\n",
      "Before removing bins that are not populated enough\n",
      "[0 1 2 3 4] [5062 1152 1285 1528  717]\n",
      "After removing bins that are not populated enough\n",
      "[0 1 2 3 4] [5062 1152 1285 1528  717]\n",
      "(9744, 108)\n",
      "(9744,)\n",
      "(9744,)\n",
      "Train data shape (100, 108)\n",
      "Test data shape (7308, 108)\n",
      "Val data shape (2436, 108)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to process data for one learning task:  0.4734807014465332\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'restore_best_weights'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 318\u001b[0m\n\u001b[1;32m    315\u001b[0m X_val \u001b[38;5;241m=\u001b[39m val_scaler\u001b[38;5;241m.\u001b[39mtransform(X_val)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# Need to train the model\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;66;03m# Train and save the model       \u001b[39;00m\n\u001b[0;32m--> 318\u001b[0m model, history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m                            \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m                            \u001b[49m\u001b[43msup_model_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_task_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mmodels_folder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msuptrain_model_to_save_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m                            \u001b[49m\u001b[43msup_hyper_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m learning_task_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    324\u001b[0m     yhat_test \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\u001b[38;5;241m.\u001b[39mflatten()\n",
      "File \u001b[0;32m~/mobile-network-datasets-ns3/data_processing_and_ML_code/ML_code/helper_functions.py:911\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(X_train, X_val, y_train, y_val, sup_model_type, learning_task_type, model_to_save_name, hyper_params, sample_weights, save_str)\u001b[0m\n\u001b[1;32m    908\u001b[0m y_val \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(y_val, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    910\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sup_model_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmlp\u001b[39m\u001b[38;5;124m'\u001b[39m: \n\u001b[0;32m--> 911\u001b[0m     model, history \u001b[38;5;241m=\u001b[39m \u001b[43mget_mlp_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_to_save_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyper_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_task_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    912\u001b[0m     plot_model_train_info (history)\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m sup_model_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxgb\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/mobile-network-datasets-ns3/data_processing_and_ML_code/ML_code/helper_functions.py:875\u001b[0m, in \u001b[0;36mget_mlp_model\u001b[0;34m(X_train, y_train, X_val, y_val, model_to_save_name, hyper_params, learning_task_type)\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[38;5;66;03m# Fit the model \u001b[39;00m\n\u001b[1;32m    874\u001b[0m print_every_n_callback \u001b[38;5;241m=\u001b[39m LambdaCallback(on_epoch_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m epoch, logs: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 875\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m \u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyper_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpatience\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestore_best_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    876\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \n\u001b[1;32m    877\u001b[0m                     epochs\u001b[38;5;241m=\u001b[39mhyper_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m    878\u001b[0m                     batch_size\u001b[38;5;241m=\u001b[39mhyper_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    881\u001b[0m                     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[1;32m    882\u001b[0m                     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    884\u001b[0m model\u001b[38;5;241m.\u001b[39msave(model_to_save_name)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'restore_best_weights'"
     ]
    }
   ],
   "source": [
    "# Erase the contents of the results file into which we append as we loop through \n",
    "open(train_results_filepath, 'w').close()\n",
    "open(test_results_filepath, 'w').close()\n",
    "\n",
    "# Iterate over different number of labeled samples \n",
    "for label_no in num_samples_list:\n",
    "    \n",
    "    EXP_PARAM['label_no'] = label_no\n",
    "\n",
    "    # Aggregate results over runs\n",
    "    train_results = pd.DataFrame(index=learning_tasks, columns=['MAE', 'MAPE', 'R2']) # or ['ACC', 'F1', 'ROC_AUC']\n",
    "    test_results = pd.DataFrame(index=learning_tasks, columns=['MAE', 'MAPE', 'R2']) # or ['ACC', 'F1', 'ROC_AUC']\n",
    "    # Apply the function to each cell in the DataFrame\n",
    "    train_results = train_results.applymap(lambda x: create_nan_array(EXP_PARAM['num_rand_runs']))\n",
    "    test_results = test_results.applymap(lambda x: create_nan_array(EXP_PARAM['num_rand_runs']))\n",
    "\n",
    "    # Iterate over different random initializations\n",
    "    for rs in range(0, EXP_PARAM['num_rand_runs']):\n",
    "        read_data_start_time = time.time()\n",
    "        print('==========================================================================')\n",
    "        print('NUM. LABELLED SAMPLES: ', label_no)\n",
    "        print('==========================================================================')\n",
    "        print('==========================================================================')\n",
    "        print('Random iteration: ', rs)\n",
    "        print('==========================================================================')\n",
    "                    \n",
    "        # Randomly select which runs shall be train and which shall be test \n",
    "        shuffled_run_ind = np.random.permutation(len(train_test_run_nums))\n",
    "        train_runs = train_test_run_nums[shuffled_run_ind[:num_train_runs]]\n",
    "        test_runs = train_test_run_nums[shuffled_run_ind[num_train_runs:]]\n",
    "        print('# train runs used ', len(train_runs))\n",
    "        print('# test runs used ', len(test_runs))\n",
    "        \n",
    "        # Read the runs from files and creater a train and test dataset \n",
    "        train_data = read_and_concatenate_runs (train_runs, dataset_folder, train_slice, network_info, time_step_size, \n",
    "                                               use_all_feats, drop_col_substr, learning_tasks, shift_samp_for_predict, \n",
    "                                               impute_method, sum_cols_substr, all_learning_tasks_in_data)\n",
    "        test_data = read_and_concatenate_runs (test_runs, dataset_folder, test_slice, network_info, time_step_size, \n",
    "                                              use_all_feats, drop_col_substr, learning_tasks, shift_samp_for_predict, \n",
    "                                              impute_method, sum_cols_substr, all_learning_tasks_in_data)\n",
    "        print('Time to read data: ', time.time() - read_data_start_time)\n",
    "        for idx, learning_task in enumerate(learning_tasks):\n",
    "            \n",
    "            task_it_start_time = time.time()\n",
    "            print('================================================================================')\n",
    "            print('Learning task: ', learning_task, 'task type: ', learning_task_types[idx] )\n",
    "            print('================================================================================')\n",
    "    \n",
    "            learning_task_type = learning_task_types[idx]\n",
    "            is_regression = learning_task_type == 'reg'\n",
    "            learning_task_type_ff = 'regression'\n",
    "            if learning_task_type == 'clas':\n",
    "                learning_task_type_ff = 'classification'\n",
    "\n",
    "            X_train, y_train, train_strat_array, X_feats, continuous_cols, categorical_cols = make_data_sup_model_ready (train_data, learning_task, learning_task_type,\n",
    "                                                                                     all_learning_tasks_in_data, bitrate_levels, \n",
    "                                                                                     clip_outliers, delay_clip_th)\n",
    "            X_test, y_test, test_strat_array, _ , _, _= make_data_sup_model_ready (test_data, learning_task, learning_task_type, \n",
    "                                                             all_learning_tasks_in_data, bitrate_levels, \n",
    "                                                             clip_outliers, delay_clip_th)\n",
    "            \n",
    "            # Sample a subset of the train samples to be equal to the num of labelled samples we need\n",
    "            sample_size = EXP_PARAM['label_no']\n",
    "            if len(y_train) <= EXP_PARAM['label_no']:\n",
    "                print(\"\\n\\nWARNING !!!!\\n\\nAsked to sample from train set of size \"+str(len(y_train))+ \n",
    "                      \" a number greater than or equal to its size \"+str(EXP_PARAM['label_no']))\n",
    "                print('Going to just take as many samples as available, No need to random sample')\n",
    "            else:\n",
    "                # This functions helps me random sample with stratification \n",
    "                X_train, _, y_train, _ = train_test_split(X_train, y_train,\n",
    "                                                                      train_size=EXP_PARAM['label_no'], shuffle=True,\n",
    "                                                                      stratify=train_strat_array)\n",
    "                \n",
    "            # create a validation set from the test set \n",
    "            X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, \n",
    "                                                          test_size=0.25, shuffle=True, \n",
    "                                                          stratify=test_strat_array)  \n",
    "            \n",
    "            print('Train data shape ' + str(X_train.shape))\n",
    "            print('Test data shape ' + str(X_test.shape))\n",
    "            print('Val data shape ' + str(X_val.shape))\n",
    "    \n",
    "            plot_hist_of_y(y_train, y_test, learning_task)\n",
    "             \n",
    "            print('Time to process data for one learning task: ', time.time() - task_it_start_time)\n",
    "            \n",
    "            #=============================================== Train and test the model ==================================\n",
    "        \n",
    "            start_time = time.time()\n",
    "    \n",
    "            if use_pretrained_model:\n",
    "                \n",
    "                assert (pretrain_model_to_load_type in ['s3l_dae', 's3l_vime', 's3l_scarf', 's3l_subtab', 's3l_switchtab'], \n",
    "                        f\"Invalid pretrain_model_to_load_type: {pretrain_model_to_load_type}.\")\n",
    "                # get the latent representations from pretrained model\n",
    "                # Load the pretrained model and scaler \n",
    "                # Load the saved MinMaxScaler object from the file\n",
    "                with open(models_folder + scaler_to_load_name, 'rb') as f:\n",
    "                    val_scaler = pickle.load(f)\n",
    "\n",
    "                X_train = val_scaler.transform(X_train).copy()\n",
    "                X_test = val_scaler.transform(X_test).copy()\n",
    "                X_val = val_scaler.transform(X_val).copy()\n",
    "                # transform returns numpy. I need to convert back to pandas dataframe\n",
    "                X_train = pd.DataFrame(X_train, columns=X_feats)\n",
    "                X_val = pd.DataFrame(X_val, columns=X_feats)\n",
    "                X_test = pd.DataFrame(X_test, columns=X_feats)\n",
    "                \n",
    "                # load the model\n",
    "                model = s3l_load(models_folder+pretrain_model_to_load_name+'.ckpt', pretrain_model_to_load_type)\n",
    "                # model is of type class 'ts3l.pl_modules.dae_lightning.DAELightning'\n",
    "                print(ModelSummary(model, max_depth=-1))\n",
    "                # set model to second phase\n",
    "                model.set_second_phase(freeze_encoder=freeze_encoder)\n",
    "                # prepare a trainer \n",
    "                early_stopping = EarlyStopping(monitor='val_loss', patience=s3l_sup_mlp['patience'], verbose=False, mode='min')\n",
    "                model_checkpoint = ModelCheckpoint(\n",
    "                    monitor='val_loss',   # Metric to monitor\n",
    "                    save_top_k=1,         # Save only the best model\n",
    "                    mode='min',           # Mode should be 'min' for metrics that should decrease\n",
    "                    filename=notebook_save_str+'_{epoch}-{val_loss:.2f}',  # Format of saved files\n",
    "                    save_last=True        # Save the last model\n",
    "                )\n",
    "                # Mode should be 'min' for metrics that should decrease\n",
    "                sup_trainer = Trainer(accelerator = 'gpu', \n",
    "                                          max_epochs = s3l_sup_mlp['max_epochs'], \n",
    "                                          num_sanity_val_steps = 2, \n",
    "                                          callbacks=[MyProgressBar(), early_stopping, model_checkpoint], \n",
    "                                          #callbacks=[MyProgressBar()], \n",
    "                                          default_root_dir=models_folder+suptrain_model_to_save_name)\n",
    "                # Need to reset DAE config for every learning task \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                if pretrain_model_to_load_type == 's3l_dae':\n",
    " \n",
    "                    train_ds = DAEDataset(X = X_train, Y = y_train, is_regression=is_regression,\n",
    "                                          continuous_cols=continuous_cols, category_cols=categorical_cols)\n",
    "                    valid_ds = DAEDataset(X = X_val, Y = y_val, is_regression=is_regression,\n",
    "                                          continuous_cols=continuous_cols, category_cols=categorical_cols)\n",
    "                    datamodule = TS3LDataModule(train_ds, valid_ds, is_regression=is_regression,\n",
    "                                                batch_size = s3l_sup_mlp['batch_size'], train_sampler=\"random\")\n",
    "                    \n",
    "                    sup_trainer.fit(model, datamodule)\n",
    "                    print(ModelSummary(model, max_depth=-1))\n",
    "                    # Load the best model \n",
    "                    #best_model_path = model_checkpoint.best_model_path\n",
    "                    #print(f\"Best model path: {best_model_path}\")\n",
    "                    #model = DAELightning.load_from_checkpoint(best_model_path)\n",
    "                    \n",
    "                    # Evaluation\n",
    "                    test_ds = DAEDataset(X_test, category_cols=categorical_cols, \n",
    "                                         continuous_cols=continuous_cols, is_regression=is_regression)\n",
    "                    test_dl = DataLoader(test_ds, s3l_sup_mlp['batch_size'], \n",
    "                                         shuffle=False, sampler = SequentialSampler(test_ds), num_workers=4)\n",
    "                    train_ds = DAEDataset(X_train, continuous_cols=continuous_cols, \n",
    "                                          category_cols=categorical_cols, is_regression=is_regression)\n",
    "                    train_dl = DataLoader(train_ds, s3l_sup_mlp['batch_size'], \n",
    "                                         shuffle=False, sampler = SequentialSampler(train_ds), num_workers=4)\n",
    "                \n",
    "                elif pretrain_model_to_load_type == 's3l_vime':\n",
    "                    \n",
    "                    train_ds = VIMEDataset(X = X_train, Y = y_train, is_regression=is_regression,\n",
    "                                           continuous_cols=continuous_cols, category_cols=categorical_cols, \n",
    "                                           is_second_phase=True)\n",
    "                    valid_ds = VIMEDataset(X = X_val, Y = y_val, is_regression=is_regression,\n",
    "                                           continuous_cols=continuous_cols, category_cols=categorical_cols, \n",
    "                                           is_second_phase=True)\n",
    "                            \n",
    "                    datamodule = TS3LDataModule(train_ds, valid_ds, batch_size = s3l_sup_mlp['batch_size'], is_regression=is_regression,\n",
    "                                                train_sampler=\"random\", train_collate_fn=VIMESecondPhaseCollateFN())\n",
    "                    \n",
    "                    sup_trainer.fit(model, datamodule)\n",
    "                    print(ModelSummary(model, max_depth=-1))\n",
    "                    # Load the best model \n",
    "                    #best_model_path = model_checkpoint.best_model_path\n",
    "                    #print(f\"Best model path: {best_model_path}\")\n",
    "                    #model = VIMELightning.load_from_checkpoint(best_model_path)\n",
    "\n",
    "                    # Evaluation\n",
    "                    test_ds = VIMEDataset(X_test, category_cols=categorical_cols, is_regression=is_regression,\n",
    "                                         continuous_cols=continuous_cols, is_second_phase=True)\n",
    "                    test_dl = DataLoader(test_ds, s3l_sup_mlp['batch_size'], \n",
    "                                         shuffle=False, sampler = SequentialSampler(test_ds), num_workers=4)\n",
    "                    train_ds = VIMEDataset(X_train, continuous_cols=continuous_cols, is_regression=is_regression, \n",
    "                                          category_cols=categorical_cols, is_second_phase=True)\n",
    "                    train_dl = DataLoader(train_ds, s3l_sup_mlp['batch_size'], \n",
    "                                         shuffle=False, sampler = SequentialSampler(train_ds), num_workers=4)\n",
    "\n",
    "                elif pretrain_model_to_load_type == 's3l_scarf':\n",
    "\n",
    "                    \n",
    "                    train_ds = SCARFDataset(X_train, y_train, is_regression=is_regression, is_second_phase=True)\n",
    "                    valid_ds = SCARFDataset(X_val, y_val, is_regression=is_regression, is_second_phase=True)\n",
    "                    \n",
    "                    datamodule = TS3LDataModule(train_ds, valid_ds, batch_size = s3l_sup_mlp['batch_size'], is_regression=is_regression, \n",
    "                                                train_sampler=\"random\")\n",
    "                    \n",
    "                    sup_trainer.fit(model, datamodule)\n",
    "                    print(ModelSummary(model, max_depth=-1))\n",
    "                    # Load the best model \n",
    "                    #best_model_path = model_checkpoint.best_model_path\n",
    "                    #print(f\"Best model path: {best_model_path}\")\n",
    "                    #model = SCARFLightning.load_from_checkpoint(best_model_path)\n",
    "\n",
    "                    # Evaluation\n",
    "                    test_ds = SCARFDataset(X_test, is_regression=is_regression, is_second_phase=True)\n",
    "                    test_dl = DataLoader(test_ds, s3l_sup_mlp['batch_size'], \n",
    "                                         shuffle=False, sampler = SequentialSampler(test_ds), num_workers=4)\n",
    "                    train_ds = SCARFDataset(X_train, is_regression=is_regression, is_second_phase=True)\n",
    "                    train_dl = DataLoader(train_ds, s3l_sup_mlp['batch_size'], \n",
    "                                         shuffle=False, sampler = SequentialSampler(train_ds), num_workers=4)\n",
    "\n",
    "                elif pretrain_model_to_load_type == 's3l_subtab':\n",
    "                    \n",
    "                    # Only subtab seems to demand that config be passed into it again, so doing it here \n",
    "                    \n",
    "                    config = SubTabConfig( task=\"regression\", loss_fn=\"MSELoss\", metric=s3l_hyp_ssl_subtab['metric'], metric_hparams={}, \n",
    "                                          input_dim=X_train.shape[1], hidden_dim=s3l_hyp_ssl_subtab['hidden_dim'], output_dim=1, \n",
    "                                          tau=s3l_hyp_ssl_subtab['tau'], use_cosine_similarity=s3l_hyp_ssl_subtab['use_cosine_similarity'], \n",
    "                                          use_contrastive=s3l_hyp_ssl_subtab['use_contrastive'], use_distance=s3l_hyp_ssl_subtab['use_distance'], \n",
    "                                          n_subsets=s3l_hyp_ssl_subtab['n_subsets'], overlap_ratio=s3l_hyp_ssl_subtab['overlap_ratio'], \n",
    "                                          mask_ratio=s3l_hyp_ssl_subtab['mask_ratio'], noise_type=s3l_hyp_ssl_subtab['noise_type'], \n",
    "                                          noise_level=s3l_hyp_ssl_subtab['noise_level'])\n",
    "                    \n",
    "                    train_ds = SubTabDataset(X_train, y_train, is_regression=is_regression)\n",
    "                    valid_ds = SubTabDataset(X_val, y_val, is_regression=is_regression)\n",
    "                    \n",
    "                    datamodule = TS3LDataModule(train_ds, valid_ds, batch_size = s3l_sup_mlp['batch_size'], is_regression=is_regression, \n",
    "                                                train_sampler=\"random\", train_collate_fn=SubTabCollateFN(config), \n",
    "                                                valid_collate_fn=SubTabCollateFN(config))\n",
    "                    #datamodule = TS3LDataModule(train_ds, valid_ds, batch_size = s3l_sup_mlp['batch_size'], is_regression=is_regression, \n",
    "                    #                            train_sampler=\"random\")\n",
    "                    \n",
    "                    sup_trainer.fit(model, datamodule)\n",
    "                    print(ModelSummary(model, max_depth=-1))\n",
    "                    # Load the best model \n",
    "                    #best_model_path = model_checkpoint.best_model_path\n",
    "                    #print(f\"Best model path: {best_model_path}\")\n",
    "                    #model = SubTabLightning.load_from_checkpoint(best_model_path)\n",
    "\n",
    "                    # Evaluation\n",
    "                    test_ds = SubTabDataset(X_test, is_regression=is_regression)\n",
    "                    test_dl = DataLoader(test_ds, s3l_sup_mlp['batch_size'], collate_fn=SubTabCollateFN(config),\n",
    "                                         shuffle=False, sampler = SequentialSampler(test_ds), num_workers=4)\n",
    "                    train_ds = SubTabDataset(X_train, is_regression=is_regression)\n",
    "                    train_dl = DataLoader(train_ds, s3l_sup_mlp['batch_size'], collate_fn=SubTabCollateFN(config),\n",
    "                                         shuffle=False, sampler = SequentialSampler(train_ds), num_workers=4)\n",
    "\n",
    "                elif pretrain_model_to_load_type == 's3l_switchtab':\n",
    "                 \n",
    "                    train_ds = SwitchTabDataset(X_train, y_train, continuous_cols=continuous_cols, category_cols=categorical_cols, \n",
    "                                                is_regression=is_regression, is_second_phase=True)\n",
    "                    valid_ds = SwitchTabDataset(X_val, y_val, continuous_cols=continuous_cols, category_cols=categorical_cols, \n",
    "                                                is_regression=is_regression, is_second_phase=True)\n",
    "                    \n",
    "                    datamodule = TS3LDataModule(train_ds, valid_ds, batch_size = s3l_sup_mlp['batch_size'], is_regression=is_regression, \n",
    "                                                train_sampler=\"random\")\n",
    "                    \n",
    "                    sup_trainer.fit(model, datamodule)\n",
    "                    print(ModelSummary(model, max_depth=-1))\n",
    "                    # Load the best model \n",
    "                    #best_model_path = model_checkpoint.best_model_path\n",
    "                    #print(f\"Best model path: {best_model_path}\")\n",
    "                    #model = SwitchTabLightning.load_from_checkpoint(best_model_path)\n",
    "\n",
    "                    # Evaluation\n",
    "                    test_ds = SwitchTabDataset(X_test, continuous_cols=continuous_cols, category_cols=category_cols, \n",
    "                                               is_regression=is_regression, is_second_phase=True)\n",
    "                    test_dl = DataLoader(test_ds, s3l_sup_mlp['batch_size'], \n",
    "                                         shuffle=False, sampler = SequentialSampler(test_ds), num_workers=4)\n",
    "                    train_ds = SwitchTabDataset(X_train, continuous_cols=continuous_cols, category_cols=category_cols, \n",
    "                                                is_regression=is_regression, is_second_phase=True)\n",
    "                    train_dl = DataLoader(train_ds, s3l_sup_mlp['batch_size'], \n",
    "                                         shuffle=False, sampler = SequentialSampler(train_ds), num_workers=4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    \n",
    "                if learning_task_type == 'reg':\n",
    "                    yhat_test = sup_trainer.predict(model, test_dl)\n",
    "                    yhat_test = torch.concat([out.cpu() for out in yhat_test]).squeeze()\n",
    "                    yhat_train = sup_trainer.predict(model, train_dl)\n",
    "                    yhat_train = torch.concat([out.cpu() for out in yhat_train]).squeeze()\n",
    "                    #print(y_train)\n",
    "                    #print(yhat_train)\n",
    "                    # Clip predictions to the range of the training data\n",
    "                    print('NOTE: Clipping the predictions to be within the range of the ground-truth values')\n",
    "                    yhat_train = np.clip(yhat_train, y_train.min(), y_train.max())\n",
    "                    yhat_test = np.clip(yhat_test, y_train.min(), y_train.max())\n",
    "                    #print(yhat_train)\n",
    "                     \n",
    "                else: #'clas'\n",
    "                    yhat_train_a = sup_trainer.predict(model, train_dl)\n",
    "                    yhat_train_proba = F.softmax(torch.concat([out.cpu() for out in yhat_train_a]).squeeze(),dim=1)\n",
    "                    yhat_train = yhat_train_proba.argmax(1)\n",
    "                    yhat_test_a = sup_trainer.predict(model, test_dl)\n",
    "                    yhat_test_proba = F.softmax(torch.concat([out.cpu() for out in yhat_test_a]).squeeze(),dim=1)\n",
    "                    yhat_test = yhat_test_proba.argmax(1)\n",
    "                    print(y_train)\n",
    "                    print(yhat_train)\n",
    "                    print(yhat_train_proba)        \n",
    "            #accuracy = accuracy_score(y_test, yhat_test_proba.argmax(1))\n",
    "                \n",
    "            else: # only supervised training \n",
    "                val_scaler = create_scaler(X_train, EXP_PARAM['scaler'])\n",
    "                # save the scaler model to a file\n",
    "                with open(models_folder + scaler_to_save_name + '.pkl', 'wb') as f:\n",
    "                    pickle.dump(val_scaler, f)\n",
    "                \n",
    "                X_train = val_scaler.transform(X_train).copy()\n",
    "                X_test = val_scaler.transform(X_test).copy()\n",
    "                X_val = val_scaler.transform(X_val).copy()\n",
    "                # Need to train the model\n",
    "                # Train and save the model       \n",
    "                model, history = train_model(X_train, X_val, \n",
    "                                            y_train, y_val, \n",
    "                                            sup_model_type, learning_task_type,\n",
    "                                            models_folder + suptrain_model_to_save_name,\n",
    "                                            sup_hyper_params)\n",
    "                if learning_task_type == 'reg':\n",
    "                    yhat_test = model.predict(X_test).flatten()\n",
    "                    yhat_train = model.predict(X_train).flatten()\n",
    "                    # Clip predictions to the range of the training data\n",
    "                    print('NOTE: Clipping the predictions to be within the range of the ground-truth values')\n",
    "                    yhat_train = np.clip(yhat_train, y_train.min(), y_train.max())\n",
    "                    yhat_test = np.clip(yhat_test, y_train.min(), y_train.max())\n",
    "                    \n",
    "                else: #learning_task_type == 'clas':\n",
    "                    if sup_model_type == 'xgb':\n",
    "                        yhat_test = model.predict(X_test).flatten()\n",
    "                        yhat_train = model.predict(X_train).flatten()\n",
    "                        yhat_test_proba = model.predict_proba(X_test)\n",
    "                        yhat_train_proba = model.predict_proba(X_train)\n",
    "                    else:\n",
    "                        yhat_test = np.argmax(model.predict(X_test), axis=1)\n",
    "                        yhat_train = np.argmax(model.predict(X_train), axis=1)\n",
    "                        yhat_test_proba = model.predict(X_test)\n",
    "                        yhat_train_proba = model.predict(X_train)\n",
    "                        \n",
    "            \n",
    "                end_time = time.time()\n",
    "                print('Time to train model: ', end_time - start_time)\n",
    "                if not use_pretrained_model:\n",
    "                    # Feature importance\n",
    "                    if ((sup_model_type == 'tabnet') or (sup_model_type == 'xgb')):\n",
    "                        plot_feature_importance(model.feature_importances_, X_feats, feat_filter)\n",
    "            \n",
    "            #====================================\n",
    "            # Predict using the supervised model \n",
    "            #====================================\n",
    "            \n",
    "            if learning_task_type == 'reg':\n",
    "                if np.isnan(yhat_train).any():\n",
    "                    print('WARNING: nans in the predicted train vals')\n",
    "                    print(yhat_train)\n",
    "                train_results.loc[learning_task, 'MAE'][rs] = compute_error(y_train, yhat_train, 'mae')\n",
    "                train_results.loc[learning_task, 'MAPE'][rs] = compute_error(y_train, yhat_train, 'mape')\n",
    "                train_results.loc[learning_task, 'R2'][rs] = compute_error(y_train, yhat_train, 'r2')\n",
    "                # Compute and Print Test set errors\n",
    "                if np.isnan(yhat_test).any():\n",
    "                    print('WARNING: NaNs in the predicted test vals')\n",
    "                    print(yhat_test)\n",
    "                test_results.loc[learning_task, 'MAE'][rs] = compute_error(y_test, yhat_test, 'mae')\n",
    "                test_results.loc[learning_task, 'MAPE'][rs] = compute_error(y_test, yhat_test, 'mape')\n",
    "                test_results.loc[learning_task, 'R2'][rs] = compute_error(y_test, yhat_test, 'r2')\n",
    "\n",
    "                # Predicted versus ground-truth plots\n",
    "                fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
    "                fig.suptitle(learning_task, fontsize=16)\n",
    "                y = np.concatenate((y_train, y_test))\n",
    "                yhat = np.concatenate((yhat_train, yhat_test))\n",
    "                bounds=[min( min(y),min(yhat) ), max( max(y),max(yhat) )]        \n",
    "                # Configure each subplot\n",
    "                setup_axes(axes[0], y_train, yhat_train, 'Train', COLOUR_HEX[0], bounds)  # Adjust color as needed\n",
    "                setup_axes(axes[1], y_test, yhat_test, 'Test', COLOUR_HEX[1], bounds)  # Adjust color as needed\n",
    "                #plt.tight_layout()\n",
    "                plt.show()\n",
    "            else: # clas\n",
    "                if np.isnan(yhat_train).any():\n",
    "                    print('WARNING: nans in the predicted train vals')\n",
    "                    print(yhat_train)                \n",
    "                \n",
    "                train_results.loc[learning_task, 'MAE'][rs] = compute_error(y_train, yhat_train, 'acc')\n",
    "                train_results.loc[learning_task, 'MAPE'][rs] = compute_error(y_train, yhat_train, 'f1score')\n",
    "                train_results.loc[learning_task, 'R2'][rs] = compute_error(y_train, yhat_train_proba, 'roc_auc')\n",
    "            \n",
    "                # Compute and Print Test set errors\n",
    "                if np.isnan(yhat_test).any():\n",
    "                    print('WARNING: NaNs in the predicted test vals')\n",
    "                    print(yhat_test)\n",
    "                test_results.loc[learning_task, 'MAE'][rs] = compute_error(y_test, yhat_test, 'acc')\n",
    "                test_results.loc[learning_task, 'MAPE'][rs] = compute_error(y_test, yhat_test, 'f1score')\n",
    "                test_results.loc[learning_task, 'R2'][rs] = compute_error(y_test, yhat_test_proba, 'roc_auc')\n",
    "                \n",
    "                # Confusion matrix plots\n",
    "                draw_confusion_matrix(y_train, yhat_train, 'Train: Confusion Matrix')\n",
    "                draw_confusion_matrix(y_test, yhat_test, 'Test: Confusion Matrix')\n",
    "           \n",
    "            print('')\n",
    "            print('')\n",
    "            print('===============================  DONE  ===================================================')\n",
    "            \n",
    "    print('============================= Train set mean over runs ===================================')\n",
    "    mean_train_results = train_results.applymap(mean_array)\n",
    "    print(mean_train_results)\n",
    "    print('=============================== Test set results =========================================')\n",
    "    print(test_results.applymap(format_array))\n",
    "    print('============================= Test set mean over runs ====================================')\n",
    "    mean_test_results = test_results.applymap(mean_array)\n",
    "    print(test_results.applymap(mean_array))\n",
    "    print('==========================================================================================')\n",
    "    # Add number of samples as a column\n",
    "    mean_train_results.insert(0, 'num_samples', EXP_PARAM['label_no'])\n",
    "    mean_test_results.insert(0, 'num_samples', EXP_PARAM['label_no'])\n",
    "    # Append to file\n",
    "    mean_train_results.to_csv(train_results_filepath, mode='a', header=False)    \n",
    "    mean_test_results.to_csv(test_results_filepath, mode='a', header=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7ece10-6186-444f-85f3-b0dcda818ea7",
   "metadata": {},
   "source": [
    "# Save the notebook and save the results in the model folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650d475f-aa4f-49ef-8008-ed402aa1184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('cp ts3l_train_and_eval.ipynb '+models_folder+notebook_save_str+'.ipynb')\n",
    "os.system('cp '+train_results_filepath+' '+models_folder+'.')\n",
    "os.system('cp '+test_results_filepath+' '+models_folder+'.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
