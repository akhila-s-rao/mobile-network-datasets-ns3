{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ed58de6-5ac0-4bbb-95e7-c77e64dfaf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload\n",
    "#%reset\n",
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "\n",
    "import warnings\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# DEBUG MODE\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.getLogger('tensorflow').setLevel(logging.FATAL)\n",
    "    \n",
    "from s3l_training import s3l_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26eb320c-c17f-40dd-8264-6dcbd0933f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run with random seed:  420\n",
      "GPU is available.\n",
      "1\n",
      "True\n",
      "12.1\n",
      "Concatenating runs:  range(1, 11)\n",
      "Time to read csv file for run:  3.0728185176849365\n",
      "Loaded run 1\n",
      "Time to read csv file for run:  3.030400514602661\n",
      "Loaded run 2\n",
      "Time to read csv file for run:  2.955641984939575\n",
      "Loaded run 3\n",
      "Time to read csv file for run:  2.998786687850952\n",
      "Loaded run 4\n",
      "Time to read csv file for run:  2.8941457271575928\n",
      "Loaded run 5\n",
      "Time to read csv file for run:  2.8659348487854004\n",
      "Loaded run 6\n",
      "Time to read csv file for run:  2.989391803741455\n",
      "Loaded run 7\n",
      "Time to read csv file for run:  2.956474542617798\n",
      "Loaded run 8\n",
      "Time to read csv file for run:  2.9765281677246094\n",
      "Loaded run 9\n",
      "Time to read csv file for run:  2.9298837184906006\n",
      "Loaded run 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrain_data, before removing rows that dont have traffic  (1799100, 102)\n",
      "pretrain_data, after removing rows that dont have traffic  (33012, 102)\n",
      "X_pretrain  (32691, 92)\n",
      "refresh rate:  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type       | Params | Mode \n",
      "--------------------------------------------------------\n",
      "0 | task_loss_fn     | MSELoss    | 0      | train\n",
      "1 | contrastive_loss | NTXentLoss | 0      | train\n",
      "2 | model            | SCARF      | 322 K  | train\n",
      "--------------------------------------------------------\n",
      "322 K     Trainable params\n",
      "0         Non-trainable params\n",
      "322 K     Total params\n",
      "1.288     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|███████████████████████████████████████████████████████████████████████████████████| 232/232 [00:02<00:00, 79.12it/s, v_num=2]\n",
      "Epoch 1: 100%|█████████████████████████████████████████████████| 232/232 [00:03<00:00, 64.82it/s, v_num=2, train_loss=2.460, val_loss=2.000]\u001b[A\n",
      "Epoch 2: 100%|█████████████████████████████████████████████████| 232/232 [00:03<00:00, 68.41it/s, v_num=2, train_loss=1.600, val_loss=1.640]\u001b[A\n",
      "Epoch 3: 100%|█████████████████████████████████████████████████| 232/232 [00:03<00:00, 68.72it/s, v_num=2, train_loss=1.290, val_loss=1.460]\u001b[A\n",
      "Epoch 4: 100%|█████████████████████████████████████████████████| 232/232 [00:03<00:00, 64.59it/s, v_num=2, train_loss=1.130, val_loss=1.340]\u001b[A\n",
      "Epoch 5: 100%|█████████████████████████████████████████████████| 232/232 [00:03<00:00, 66.87it/s, v_num=2, train_loss=1.020, val_loss=1.290]\u001b[A\n",
      "Epoch 6: 100%|█████████████████████████████████████████████████| 232/232 [00:03<00:00, 62.41it/s, v_num=2, train_loss=0.932, val_loss=1.240]\u001b[A\n",
      "Epoch 7: 100%|█████████████████████████████████████████████████| 232/232 [00:05<00:00, 44.87it/s, v_num=2, train_loss=0.874, val_loss=1.260]\u001b[A\n",
      "Epoch 8: 100%|█████████████████████████████████████████████████| 232/232 [00:05<00:00, 41.39it/s, v_num=2, train_loss=0.818, val_loss=1.160]\u001b[A\n",
      "Epoch 9: 100%|█████████████████████████████████████████████████| 232/232 [00:04<00:00, 47.35it/s, v_num=2, train_loss=0.789, val_loss=1.130]\u001b[A\n",
      "Epoch 10: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 44.37it/s, v_num=2, train_loss=0.759, val_loss=1.120]\u001b[A\n",
      "Epoch 11: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 44.73it/s, v_num=2, train_loss=0.734, val_loss=1.140]\u001b[A\n",
      "Epoch 12: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 40.09it/s, v_num=2, train_loss=0.712, val_loss=1.120]\u001b[A\n",
      "Epoch 13: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 40.81it/s, v_num=2, train_loss=0.682, val_loss=1.080]\u001b[A\n",
      "Epoch 14: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 43.20it/s, v_num=2, train_loss=0.673, val_loss=1.080]\u001b[A\n",
      "Epoch 15: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 39.90it/s, v_num=2, train_loss=0.658, val_loss=1.130]\u001b[A\n",
      "Epoch 16: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 43.94it/s, v_num=2, train_loss=0.647, val_loss=1.070]\u001b[A\n",
      "Epoch 17: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 40.31it/s, v_num=2, train_loss=0.634, val_loss=1.020]\u001b[A\n",
      "Epoch 18: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 42.76it/s, v_num=2, train_loss=0.632, val_loss=1.030]\u001b[A\n",
      "Epoch 19: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 40.19it/s, v_num=2, train_loss=0.618, val_loss=1.020]\u001b[A\n",
      "Epoch 20: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 39.24it/s, v_num=2, train_loss=0.611, val_loss=1.050]\u001b[A\n",
      "Epoch 21: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 43.08it/s, v_num=2, train_loss=0.597, val_loss=1.010]\u001b[A\n",
      "Epoch 22: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 39.56it/s, v_num=2, train_loss=0.598, val_loss=0.959]\u001b[A\n",
      "Epoch 23: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 43.20it/s, v_num=2, train_loss=0.589, val_loss=1.010]\u001b[A\n",
      "Epoch 24: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 39.55it/s, v_num=2, train_loss=0.587, val_loss=0.972]\u001b[A\n",
      "Epoch 25: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 41.95it/s, v_num=2, train_loss=0.582, val_loss=1.060]\u001b[A\n",
      "Epoch 26: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 39.65it/s, v_num=2, train_loss=0.568, val_loss=0.961]\u001b[A\n",
      "Epoch 27: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 40.34it/s, v_num=2, train_loss=0.568, val_loss=1.000]\u001b[A\n",
      "Epoch 28: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 40.90it/s, v_num=2, train_loss=0.561, val_loss=0.995]\u001b[A\n",
      "Epoch 29: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 39.64it/s, v_num=2, train_loss=0.552, val_loss=1.000]\u001b[A\n",
      "Epoch 30: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 44.24it/s, v_num=2, train_loss=0.555, val_loss=0.953]\u001b[A\n",
      "Epoch 31: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 40.24it/s, v_num=2, train_loss=0.543, val_loss=0.978]\u001b[A\n",
      "Epoch 32: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 43.00it/s, v_num=2, train_loss=0.540, val_loss=0.940]\u001b[A\n",
      "Epoch 33: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 40.14it/s, v_num=2, train_loss=0.547, val_loss=0.951]\u001b[A\n",
      "Epoch 34: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 40.65it/s, v_num=2, train_loss=0.538, val_loss=0.950]\u001b[A\n",
      "Epoch 35: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 41.61it/s, v_num=2, train_loss=0.533, val_loss=0.926]\u001b[A\n",
      "Epoch 36: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 39.92it/s, v_num=2, train_loss=0.535, val_loss=0.951]\u001b[A\n",
      "Epoch 37: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 43.49it/s, v_num=2, train_loss=0.531, val_loss=0.898]\u001b[A\n",
      "Epoch 38: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 38.98it/s, v_num=2, train_loss=0.526, val_loss=0.930]\u001b[A\n",
      "Epoch 39: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 42.89it/s, v_num=2, train_loss=0.522, val_loss=0.930]\u001b[A\n",
      "Epoch 40: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 39.92it/s, v_num=2, train_loss=0.520, val_loss=0.924]\u001b[A\n",
      "Epoch 41: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 41.08it/s, v_num=2, train_loss=0.524, val_loss=0.903]\u001b[A\n",
      "Epoch 42: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 39.79it/s, v_num=2, train_loss=0.512, val_loss=0.931]\u001b[A\n",
      "Epoch 43: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 39.53it/s, v_num=2, train_loss=0.515, val_loss=0.951]\u001b[A\n",
      "Epoch 44: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 42.37it/s, v_num=2, train_loss=0.509, val_loss=0.883]\u001b[A\n",
      "Epoch 45: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 40.22it/s, v_num=2, train_loss=0.510, val_loss=0.938]\u001b[A\n",
      "Epoch 46: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 42.31it/s, v_num=2, train_loss=0.501, val_loss=0.869]\u001b[A\n",
      "Epoch 47: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 39.41it/s, v_num=2, train_loss=0.504, val_loss=0.897]\u001b[A\n",
      "Epoch 48: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 42.92it/s, v_num=2, train_loss=0.506, val_loss=0.923]\u001b[A\n",
      "Epoch 49: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 39.27it/s, v_num=2, train_loss=0.503, val_loss=0.880]\u001b[A\n",
      "Epoch 50: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 39.54it/s, v_num=2, train_loss=0.499, val_loss=0.881]\u001b[A\n",
      "Epoch 51: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 41.44it/s, v_num=2, train_loss=0.495, val_loss=0.864]\u001b[A\n",
      "Epoch 52: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 39.70it/s, v_num=2, train_loss=0.502, val_loss=0.900]\u001b[A\n",
      "Epoch 53: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 43.57it/s, v_num=2, train_loss=0.488, val_loss=0.871]\u001b[A\n",
      "Epoch 54: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 39.32it/s, v_num=2, train_loss=0.493, val_loss=0.880]\u001b[A\n",
      "Epoch 55: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 43.32it/s, v_num=2, train_loss=0.494, val_loss=0.889]\u001b[A\n",
      "Epoch 56: 100%|████████████████████████████████████████████████| 232/232 [00:06<00:00, 37.27it/s, v_num=2, train_loss=0.498, val_loss=0.865]\u001b[A\n",
      "Epoch 57: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 43.20it/s, v_num=2, train_loss=0.492, val_loss=0.876]\u001b[A\n",
      "Epoch 58: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 39.85it/s, v_num=2, train_loss=0.490, val_loss=0.868]\u001b[A\n",
      "Epoch 59: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 39.14it/s, v_num=2, train_loss=0.487, val_loss=0.842]\u001b[A\n",
      "Epoch 60: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 41.65it/s, v_num=2, train_loss=0.483, val_loss=0.859]\u001b[A\n",
      "Epoch 61: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 40.31it/s, v_num=2, train_loss=0.485, val_loss=0.849]\u001b[A\n",
      "Epoch 62: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 43.73it/s, v_num=2, train_loss=0.480, val_loss=0.874]\u001b[A\n",
      "Epoch 63: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 40.09it/s, v_num=2, train_loss=0.473, val_loss=0.838]\u001b[A\n",
      "Epoch 64: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 43.51it/s, v_num=2, train_loss=0.479, val_loss=0.823]\u001b[A\n",
      "Epoch 65: 100%|████████████████████████████████████████████████| 232/232 [00:06<00:00, 38.56it/s, v_num=2, train_loss=0.478, val_loss=0.872]\u001b[A\n",
      "Epoch 66: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 40.18it/s, v_num=2, train_loss=0.478, val_loss=0.848]\u001b[A\n",
      "Epoch 67: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 40.44it/s, v_num=2, train_loss=0.468, val_loss=0.847]\u001b[A\n",
      "Epoch 68: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 41.02it/s, v_num=2, train_loss=0.469, val_loss=0.857]\u001b[A\n",
      "Epoch 69: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 42.13it/s, v_num=2, train_loss=0.468, val_loss=0.828]\u001b[A\n",
      "Epoch 70: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 40.17it/s, v_num=2, train_loss=0.470, val_loss=0.836]\u001b[A\n",
      "Epoch 71: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 42.83it/s, v_num=2, train_loss=0.463, val_loss=0.807]\u001b[A\n",
      "Epoch 72: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 41.15it/s, v_num=2, train_loss=0.472, val_loss=0.838]\u001b[A\n",
      "Epoch 73: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 42.03it/s, v_num=2, train_loss=0.465, val_loss=0.801]\u001b[A\n",
      "Epoch 74: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 40.84it/s, v_num=2, train_loss=0.467, val_loss=0.806]\u001b[A\n",
      "Epoch 75: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 40.72it/s, v_num=2, train_loss=0.462, val_loss=0.875]\u001b[A\n",
      "Epoch 76: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 45.25it/s, v_num=2, train_loss=0.463, val_loss=0.840]\u001b[A\n",
      "Epoch 77: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 40.68it/s, v_num=2, train_loss=0.462, val_loss=0.788]\u001b[A\n",
      "Epoch 78: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 44.97it/s, v_num=2, train_loss=0.458, val_loss=0.815]\u001b[A\n",
      "Epoch 79: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 40.96it/s, v_num=2, train_loss=0.459, val_loss=0.816]\u001b[A\n",
      "Epoch 80: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 44.29it/s, v_num=2, train_loss=0.458, val_loss=0.821]\u001b[A\n",
      "Epoch 81: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 41.88it/s, v_num=2, train_loss=0.452, val_loss=0.820]\u001b[A\n",
      "Epoch 82: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 42.26it/s, v_num=2, train_loss=0.453, val_loss=0.805]\u001b[A\n",
      "Epoch 83: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 40.94it/s, v_num=2, train_loss=0.458, val_loss=0.802]\u001b[A\n",
      "Epoch 84: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 40.85it/s, v_num=2, train_loss=0.450, val_loss=0.778]\u001b[A\n",
      "Epoch 85: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 45.32it/s, v_num=2, train_loss=0.445, val_loss=0.782]\u001b[A\n",
      "Epoch 86: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 41.02it/s, v_num=2, train_loss=0.447, val_loss=0.766]\u001b[A\n",
      "Epoch 87: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 45.78it/s, v_num=2, train_loss=0.446, val_loss=0.797]\u001b[A\n",
      "Epoch 88: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 41.25it/s, v_num=2, train_loss=0.451, val_loss=0.762]\u001b[A\n",
      "Epoch 89: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 45.03it/s, v_num=2, train_loss=0.447, val_loss=0.769]\u001b[A\n",
      "Epoch 90: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 40.73it/s, v_num=2, train_loss=0.445, val_loss=0.805]\u001b[A\n",
      "Epoch 91: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 41.03it/s, v_num=2, train_loss=0.451, val_loss=0.737]\u001b[A\n",
      "Epoch 92: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 39.22it/s, v_num=2, train_loss=0.448, val_loss=0.762]\u001b[A\n",
      "Epoch 93: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 40.51it/s, v_num=2, train_loss=0.446, val_loss=0.811]\u001b[A\n",
      "Epoch 94: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 42.51it/s, v_num=2, train_loss=0.443, val_loss=0.773]\u001b[A\n",
      "Epoch 95: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 41.34it/s, v_num=2, train_loss=0.440, val_loss=0.728]\u001b[A\n",
      "Epoch 96: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 44.98it/s, v_num=2, train_loss=0.441, val_loss=0.753]\u001b[A\n",
      "Epoch 97: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 40.20it/s, v_num=2, train_loss=0.441, val_loss=0.768]\u001b[A\n",
      "Epoch 98: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 44.64it/s, v_num=2, train_loss=0.441, val_loss=0.748]\u001b[A\n",
      "Epoch 99: 100%|████████████████████████████████████████████████| 232/232 [00:05<00:00, 39.07it/s, v_num=2, train_loss=0.434, val_loss=0.714]\u001b[A\n",
      "Epoch 99: 100%|████████████████████████████████████████████████| 232/232 [00:06<00:00, 35.83it/s, v_num=2, train_loss=0.436, val_loss=0.765]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|████████████████████████████████████████████████| 232/232 [00:06<00:00, 35.71it/s, v_num=2, train_loss=0.436, val_loss=0.765]\n",
      "DONE SAVING PRETRAINED MODEL\n",
      "   | Name                                             | Type        | Params | Mode \n",
      "------------------------------------------------------------------------------------------\n",
      "0  | task_loss_fn                                     | MSELoss     | 0      | train\n",
      "1  | contrastive_loss                                 | NTXentLoss  | 0      | train\n",
      "2  | model                                            | SCARF       | 322 K  | train\n",
      "3  | model._SCARF__encoder                            | MLP         | 140 K  | train\n",
      "4  | model._SCARF__encoder.linear_0                   | Linear      | 18.6 K | train\n",
      "5  | model._SCARF__encoder.batchnorm_0                | BatchNorm1d | 400    | train\n",
      "6  | model._SCARF__encoder.relu_0                     | ReLU        | 0      | train\n",
      "7  | model._SCARF__encoder.dropout_0                  | Dropout     | 0      | train\n",
      "8  | model._SCARF__encoder.linear_1                   | Linear      | 40.2 K | train\n",
      "9  | model._SCARF__encoder.batchnorm_1                | BatchNorm1d | 400    | train\n",
      "10 | model._SCARF__encoder.relu_1                     | ReLU        | 0      | train\n",
      "11 | model._SCARF__encoder.dropout_1                  | Dropout     | 0      | train\n",
      "12 | model._SCARF__encoder.linear_2                   | Linear      | 40.2 K | train\n",
      "13 | model._SCARF__encoder.batchnorm_2                | BatchNorm1d | 400    | train\n",
      "14 | model._SCARF__encoder.relu_2                     | ReLU        | 0      | train\n",
      "15 | model._SCARF__encoder.dropout_2                  | Dropout     | 0      | train\n",
      "16 | model._SCARF__encoder.linear_n_layers            | Linear      | 40.2 K | train\n",
      "17 | model.pretraining_head                           | MLP         | 80.8 K | train\n",
      "18 | model.pretraining_head.linear_0                  | Linear      | 40.2 K | train\n",
      "19 | model.pretraining_head.batchnorm_0               | BatchNorm1d | 400    | train\n",
      "20 | model.pretraining_head.relu_0                    | ReLU        | 0      | train\n",
      "21 | model.pretraining_head.dropout_0                 | Dropout     | 0      | train\n",
      "22 | model.pretraining_head.linear_n_layers           | Linear      | 40.2 K | train\n",
      "23 | model.one_layer_prediction_head                  | Sequential  | 40.4 K | train\n",
      "24 | model.one_layer_prediction_head.head_linear_hid  | Linear      | 40.2 K | train\n",
      "25 | model.one_layer_prediction_head.head_activation  | ReLU        | 0      | train\n",
      "26 | model.one_layer_prediction_head.head_linear_out  | Linear      | 201    | train\n",
      "27 | model.two_layer_prediction_head                  | Sequential  | 60.4 K | train\n",
      "28 | model.two_layer_prediction_head.head_linear_hid1 | Linear      | 40.2 K | train\n",
      "29 | model.two_layer_prediction_head.head_activation1 | ReLU        | 0      | train\n",
      "30 | model.two_layer_prediction_head.head_linear_hid2 | Linear      | 20.1 K | train\n",
      "31 | model.two_layer_prediction_head.head_activation2 | ReLU        | 0      | train\n",
      "32 | model.two_layer_prediction_head.head_linear_out  | Linear      | 101    | train\n",
      "------------------------------------------------------------------------------------------\n",
      "322 K     Trainable params\n",
      "0         Non-trainable params\n",
      "322 K     Total params\n",
      "1.288     Total estimated model params size (MB)\n",
      "DONE PRETRAINING\n"
     ]
    }
   ],
   "source": [
    "s3l_training(pretrain=True, # if True First Phase training\n",
    "             use_pretrained_model=False, # if True Second Phase learning\n",
    "             pt_type='scarf', \n",
    "             pt_folder='FP3_only_traffic_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7160f9-4db4-4688-9556-ba884a04eafb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
