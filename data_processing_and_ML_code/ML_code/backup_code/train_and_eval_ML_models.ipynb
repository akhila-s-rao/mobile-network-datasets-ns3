{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c4ee135-3899-4642-8cdb-7b50955efdf1",
   "metadata": {},
   "source": [
    "# Load libraries. Set precision and fonts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78b018a7-8f64-468a-bea0-affb438d8d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# reload\n",
    "#%reset\n",
    "%load_ext autoreload\n",
    "%autoreload 2    \n",
    "    \n",
    "from helper_functions import *\n",
    "from tabular_dae.model import DAE\n",
    "from tabular_dae.model import load as dae_load\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "from torch.optim import Adam\n",
    "\n",
    "get_ipython().run_line_magic('precision', '3')\n",
    "plt.rcParams.update({'font.size': 22})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4213bc34-7fd9-44cb-bfef-650b43d1b307",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available.\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available.\")\n",
    "else:\n",
    "    print(\"No GPU found.\")\n",
    "print(torch.cuda.device_count())\n",
    "#print(torch.cuda.get_device_name(0))\n",
    "#print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2b412b-e4db-47f5-8748-ffd21395ec49",
   "metadata": {},
   "source": [
    "# Set Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35cc4bea-15f6-4423-8f89-b840b0f4ee58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================================\n",
    "# Experiment Parameters: Check carefully!\n",
    "#========================================\n",
    "\n",
    "model_save_path = 'models/without_pretrain/'\n",
    "train_slice = 'all' #['macro', 'micro', 'slow', 'fast', 'all', 'only_delay']\n",
    "test_slice = 'all' #['macro', 'micro', 'slow', 'fast', 'all']\n",
    "EXP_PARAM = {\n",
    "    'scaler': 'standard' #'minmax', 'standard', 'robust', 'maxabs', 'l2norm'\n",
    "} \n",
    "scaler_to_save_name = EXP_PARAM['scaler']+'_scaler'\n",
    "\n",
    "#==================================\n",
    "# Pretraining Experiment Parameters\n",
    "#==================================\n",
    "\n",
    "pretrain = False\n",
    "pretrain_type = 'tabnet' #['dae', 'tabnet']\n",
    "pretrain_model_to_save_name = train_slice+'_tabnet_default_hyperp'\n",
    "\n",
    "#==========================================\n",
    "# Supervised Training Experiment Parameters\n",
    "#==========================================\n",
    "# Load an existing sup model to evaluate \n",
    "load_sup_model = False\n",
    "sup_model_to_load_type = 'tabnet'\n",
    "sup_model_to_load_name = train_slice+'_tabnet_suptrain_no_pretrain'\n",
    "\n",
    "# Load an existing pretrained model to use as encoding for sup model \n",
    "use_pretrained_model = False\n",
    "pretrain_model_to_load_type = 'tabnet' #['dae', 'tabnet']\n",
    "pretrain_model_to_load_name = 'all_pretrain_tabnet_default_hyperp.zip'\n",
    "scaler_to_load_name = 'minMaxScaler_pretrain_tabnet'\n",
    "\n",
    "# Train a sup model with or without using a pretrained model \n",
    "sup_model_type = 'xgb' #['mlp', 'xgb', 'tabnet']\n",
    "suptrain_model_to_save_name = train_slice+'_xgb_suptrain_no_pretrain_standard_scaler' # could also be sup_model_with_pretrain \n",
    "\n",
    "\n",
    "#==================================================\n",
    "# Experiment Parameters: Not often changed\n",
    "#==================================================\n",
    "\n",
    "pretrain_runs = range(1, 12 + 1) # 1-12\n",
    "train_runs = range(1, 12 + 1)\n",
    "test_runs = range(13, 15 + 1)\n",
    "\n",
    "time_step_size = '500ms'\n",
    "#past_sample_step_size = '10ms'\n",
    "past_samples = 1\n",
    "\n",
    "fill_na_val = 0\n",
    "\n",
    "# Column names in the dataset that contain the ground truth labels for our prediction tasks of interest\n",
    "all_learning_tasks_in_data = ['dashClient_trace.txt_newBitRate_bps', # predictable with 0.3516 mape # most imp feature DlPdcpStats.txt_PduSize\n",
    "                              'dashClient_trace.txt_oldBitRate_bps', \n",
    "                              'delay_trace.txt_ul_delay', # predictable with 0.06 mape # most imp feature UlPdcpStats.txt_delay\n",
    "                              'delay_trace.txt_dl_delay', # predictable with 0.07 mape # most imp feature DlPdcpStats.txt_delay\n",
    "                              'vrFragment_trace.txt_vr_frag_time', # predictable with 0.0394 mape # most imp feature DlPdcpStats.txt_delay\n",
    "                              'vrFragment_trace.txt_vr_frag_thput_mbps', # predictable with 0.1150 mape # most imp feature DlPdcpStats.txt_delay\n",
    "                              'vrFragment_trace.txt_vr_burst_time', # predictable with 0.0869 mape # most imp feature DlPdcpStats.txt_max\n",
    "                              'vrFragment_trace.txt_vr_burst_thput_mbps']# predictable with 0.2122 mape # most imp feature DlPdcpStats.txt_delay \n",
    "learning_tasks = ['vrFragment_trace.txt_vr_frag_thput_mbps', 'vrFragment_trace.txt_vr_burst_thput_mbps',\n",
    "                  'vrFragment_trace.txt_vr_frag_time', 'vrFragment_trace.txt_vr_burst_time', \n",
    "                  'dashClient_trace.txt_newBitRate_bps']\n",
    "                  #'delay_trace.txt_ul_delay',\n",
    "                  #'delay_trace.txt_dl_delay']\n",
    "\n",
    "classification = False \n",
    "\n",
    "# If True then we are predicting one window ahead if False then we are predicting on the same window \n",
    "shift_samp_for_predict = False\n",
    "\n",
    "# If you want the test samples to be sorted by delay value to see the error differences for the low delay and high delay cases \n",
    "sort_test_samples = False\n",
    "\n",
    "# CHECK\n",
    "# Drop the other Y columns for other learning tasks\n",
    "drop_cols = ['timestamp']\n",
    "\n",
    "use_all_feats = True\n",
    "# take the top n features of each run and add it to the top_n_features list  \n",
    "# If use_all_feats = True then thes will not be used \n",
    "feat_filter = 10 \n",
    "top_n_features = []\n",
    "# Only valid when use_all_feats = False \n",
    "selected_features = []\n",
    "\n",
    "if classification:\n",
    "    IN_PARAM = IN_PARAM_C\n",
    "    OUT_PARAM = OUT_PARAM_C\n",
    "else:\n",
    "    IN_PARAM = IN_PARAM_R\n",
    "    OUT_PARAM = OUT_PARAM_R\n",
    "    IN_PARAM['loss'] = 'mse' # options are mse, mae and mape \n",
    "    IN_PARAM['eval_metric'] = 'mape' # options are mae, and mape\n",
    "    \n",
    "IN_PARAM['model_type'] = sup_model_type\n",
    "IN_PARAM['time_wind_size'] = time_step_size\n",
    "\n",
    "# All delay values above this will be removed from the train and test set\n",
    "clip_outliers = True\n",
    "delay_clip_th = 5000\n",
    "\n",
    "# Read these from the ue_groups file later and also rename ue_groups to something else\n",
    "network_info={}\n",
    "network_info['total_num_ues'] = 90\n",
    "network_info['macro_cells'] = [1,2,3]\n",
    "network_info['micro_cells'] = [4,5,6]\n",
    "network_info['macro_imsis'] = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30]\n",
    "network_info['fast_imsis'] = [1,2,3,4,5,6]\n",
    "network_info['only_delay_imsis'] = [3,5,7,9,13,15,17,19,23,25,27,29,33,35,37,39,43,45,47,49,53,55,57,59,63,65,67,69,73,75,77,79,83,85,87]\n",
    "network_info['micro_imsis'] = list(set(range(1, network_info['total_num_ues']+1)) - set(network_info['macro_imsis']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c8836e-fcf1-49f0-b0e0-5bf2f9c84784",
   "metadata": {},
   "source": [
    "# Set Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ffef5f1b-3cde-49ac-a79b-fe6c99321bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for supervised TabNet training\n",
    "hypp_sup_tabnet={\n",
    "    #'lambda_sparse': , # default = 1e-3\n",
    "    # This is the extra sparsity loss coefficient as proposed in the original paper. The bigger this coefficient is, the sparser your model will be in terms of feature selection. Depending on the difficulty of your problem, reducing this value could help.\n",
    "    'mask_type': 'sparsemax', # 'entmax' # default='sparsemax'\n",
    "    'n_da': 8, # between 8-64 # default=8\n",
    "    'n_steps': 3, # between 3-10 # default=3\n",
    "    'n_independent': 2, # between 1-5 # default=2\n",
    "    'n_shared': 2, # between 1-5 # default=2 \n",
    "    'batch_size': 1024, #default=1024\n",
    "    'max_epochs': 200, # default=200\n",
    "    'patience': 10 # default=10\n",
    "} \n",
    "\n",
    "# Hyperparameters for SSL using TabNet\n",
    "hypp_ssl_tabnet={\n",
    "    #'lambda_sparse': , # default = 1e-3\n",
    "    # This is the extra sparsity loss coefficient as proposed in the original paper. The bigger this coefficient is, the sparser your model will be in terms of feature selection. Depending on the difficulty of your problem, reducing this value could help.\n",
    "    'mask_type': 'entmax', # 'entmax' # default='sparsemax'\n",
    "    'n_da': 8, # between 8-64 # default=8\n",
    "    'n_steps': 3, # between 3-10 # default=3\n",
    "    'n_independent': 2, # between 1-5 # default=2\n",
    "    'n_shared': 2, # between 1-5 # default=2\n",
    "    'n_shared_decoder': 1, # default=1\n",
    "    'n_indep_decoder': 1, # default=1\n",
    "    'noise_ratio': 0.30,\n",
    "    'batch_size': 1024, #default=1024\n",
    "    'max_epochs': 200, # default=200\n",
    "    'patience': 10 # default=10\n",
    "} \n",
    "\n",
    "# Hyperparameters for SSL using DAE \n",
    "hypp_ssl_dae={\n",
    "    'swap_noise_prob': 0.50,\n",
    "    'batch_size': 128,   \n",
    "\n",
    "    # used if 'deepbottleneck'\n",
    "    #'arch': 'deepbottleneck',\n",
    "    #'hid_size': 150,\n",
    "    #'bottle_neck_size': 40,\n",
    "    #'num_layers': 5\n",
    "    \n",
    "    # used if 'deepstack'\n",
    "    'arch': 'deepstack',\n",
    "    'hid_size': 414,\n",
    "    'num_layers': 3\n",
    "} \n",
    "\n",
    "# Hyperparameters for supervised MLP training\n",
    "hypp_sup_mlp={\n",
    "    'fc_layers': [500, 100], # the hidden layers\n",
    "    'batch_size': 128,  \n",
    "    'max_epochs': 200,\n",
    "    'patience': 10\n",
    "} \n",
    "# Hyperparameters for supervised XGB training \n",
    "hypp_sup_xgb={\n",
    "    \n",
    "} \n",
    "\n",
    "if sup_model_type == 'tabnet':\n",
    "    sup_hyper_params=hypp_sup_tabnet\n",
    "elif sup_model_type == 'mlp':\n",
    "    sup_hyper_params=hypp_sup_mlp\n",
    "elif sup_model_type == 'xgb':\n",
    "    sup_hyper_params=hypp_sup_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039c61eb-b0d0-47b7-bc4d-89e03f0ee717",
   "metadata": {},
   "source": [
    "# Process data frame before separating out learning task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "acaa45de-7ba8-4b21-958f-1654f4b9f0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run with random seed:  865 ------------------------------------------------------------------------\n",
      "Train runs:  range(1, 13)\n",
      "Loaded run 1\n",
      "Loaded run 2\n",
      "Loaded run 3\n",
      "Loaded run 4\n",
      "Loaded run 5\n",
      "Loaded run 6\n",
      "Loaded run 7\n",
      "Loaded run 8\n",
      "Loaded run 9\n",
      "Loaded run 10\n",
      "Loaded run 11\n",
      "Loaded run 12\n",
      "Drop unwanted columns\n",
      "(2158920, 215)\n",
      "Test runs:  range(13, 16)\n",
      "Loaded run 13\n",
      "Loaded run 14\n",
      "Loaded run 15\n",
      "Drop unwanted columns\n",
      "(539730, 215)\n"
     ]
    }
   ],
   "source": [
    "# Set random seed\n",
    "r = np.random.randint(0, 1000)\n",
    "print('Run with random seed: ', r, '------------------------------------------------------------------------')    \n",
    "IN_PARAM['rand_seed'] = r\n",
    "np.random.seed(r)\n",
    "\n",
    "# Read the dataset\n",
    "\n",
    "if pretrain:\n",
    "    pretrain_data = pd.DataFrame()\n",
    "    print('Pretrain runs: ', pretrain_runs)\n",
    "    for run in pretrain_runs:\n",
    "        this_run_data = pd.read_csv('../../../dataset_ver1/parsed_data/run'+str(run)+'_dataslice_all_video_delay_vr_'+time_step_size+'.csv', delimiter=\",\")\n",
    "        # separate the rows that belong to the desired data slice  \n",
    "        this_run_data = data_slice_filter_samples(train_slice, this_run_data, network_info)\n",
    "        pretrain_data = pd.concat([pretrain_data, this_run_data], axis=0)\n",
    "        print('Loaded run', str(run))\n",
    "        \n",
    "    # Name the first column as timestamp\n",
    "    pretrain_data.rename(columns={pretrain_data.columns[0]: 'timestamp'}, inplace=True)\n",
    "    # Drop columns that we do not want to include in the dataset\n",
    "    print('Drop unwanted columns')\n",
    "    if use_all_feats:\n",
    "        pretrain_data = pretrain_data.drop(expand_cols_to_step_size(drop_cols, past_samples), axis=1, errors='ignore')\n",
    "    else:\n",
    "        pretrain_data = pretrain_data[top_n_agg]\n",
    "    print(pretrain_data.shape)\n",
    "\n",
    "\n",
    "print('Train runs: ', train_runs)\n",
    "train_data = pd.DataFrame()\n",
    "for run in train_runs:\n",
    "    this_run_data = pd.read_csv('../../../dataset_ver1/parsed_data/run'+str(run)+'_dataslice_all_video_delay_vr_'+time_step_size+'.csv', delimiter=\",\")\n",
    "    # separate the rows that belong to the desired data slice  \n",
    "    this_run_data = data_slice_filter_samples(train_slice, this_run_data, network_info)\n",
    "    train_data = pd.concat([train_data, this_run_data], axis=0)\n",
    "    print('Loaded run', str(run))\n",
    "# Name the first column as timestamp\n",
    "train_data.rename(columns={train_data.columns[0]: 'timestamp'}, inplace=True)\n",
    "# Drop columns that we do not want to include in the dataset\n",
    "print('Drop unwanted columns')\n",
    "if use_all_feats:\n",
    "    train_data = train_data.drop(expand_cols_to_step_size(drop_cols, past_samples), axis=1, errors='ignore')\n",
    "else:\n",
    "    train_data = train_data[top_n_agg]\n",
    "print(train_data.shape)\n",
    "\n",
    "print('Test runs: ', test_runs)\n",
    "test_data = pd.DataFrame()\n",
    "for run in test_runs:\n",
    "    this_run_data = pd.read_csv('../../../dataset_ver1/parsed_data/run'+str(run)+'_dataslice_all_video_delay_vr_'+time_step_size+'.csv', delimiter=\",\")\n",
    "    # separate the rows that belong to the desired data slice  \n",
    "    this_run_data = data_slice_filter_samples(test_slice, this_run_data, network_info)\n",
    "    test_data = pd.concat([test_data, this_run_data], axis=0)\n",
    "    print('Loaded run', str(run))\n",
    "# Name the first column as timestamp\n",
    "test_data.rename(columns={test_data.columns[0]: 'timestamp'}, inplace=True)\n",
    "# Drop columns that we do not want to include in the dataset\n",
    "print('Drop unwanted columns')\n",
    "if use_all_feats:\n",
    "    test_data = test_data.drop(expand_cols_to_step_size(drop_cols, past_samples), axis=1, errors='ignore')\n",
    "else:\n",
    "    test_data = test_data[top_n_agg]\n",
    "print(test_data.shape)\n",
    "\n",
    "\n",
    "# Save the column names\n",
    "#np.savetxt('all_columns_list.csv', np.array(dae_pretrain_data.columns), delimiter=',', fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc2f281-d27a-4a89-aa5f-37ed55f63ddd",
   "metadata": {},
   "source": [
    "# Pretrain a new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f10d499c-46c2-445b-82cf-c97aa7468605",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2158920, 207)\n",
      "Filling NA in samples with  0\n",
      "Number of rows with NA values in the input features:  109754\n",
      "Fraction of rows with NA values in the input features:  0.0508374557649195\n",
      "NOT imputing NA values and just keeing them as NAs instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 6465.10605| val_0_unsup_loss_numpy: 0.610480010509491|  0:02:10s\n",
      "epoch 1  | loss: 0.82321 | val_0_unsup_loss_numpy: 0.4271799921989441|  0:04:19s\n",
      "epoch 2  | loss: 0.58634 | val_0_unsup_loss_numpy: 0.42021000385284424|  0:06:27s\n",
      "epoch 3  | loss: 0.50954 | val_0_unsup_loss_numpy: 0.363180011510849|  0:08:37s\n",
      "epoch 4  | loss: 0.48055 | val_0_unsup_loss_numpy: 0.3509899973869324|  0:10:46s\n",
      "epoch 5  | loss: 0.42669 | val_0_unsup_loss_numpy: 0.35231998562812805|  0:12:56s\n",
      "epoch 6  | loss: 0.42188 | val_0_unsup_loss_numpy: 0.5714300274848938|  0:15:06s\n",
      "epoch 7  | loss: 0.40899 | val_0_unsup_loss_numpy: 0.34619998931884766|  0:17:15s\n",
      "epoch 8  | loss: 0.40128 | val_0_unsup_loss_numpy: 0.3446600139141083|  0:19:24s\n",
      "epoch 9  | loss: 0.3885  | val_0_unsup_loss_numpy: 0.3690600097179413|  0:21:34s\n",
      "epoch 10 | loss: 0.37973 | val_0_unsup_loss_numpy: 0.37501001358032227|  0:23:44s\n",
      "epoch 11 | loss: 0.37538 | val_0_unsup_loss_numpy: 0.3400599956512451|  0:25:54s\n",
      "epoch 12 | loss: 0.36892 | val_0_unsup_loss_numpy: 0.33643001317977905|  0:28:08s\n",
      "epoch 13 | loss: 0.36853 | val_0_unsup_loss_numpy: 0.3746599853038788|  0:30:21s\n",
      "epoch 14 | loss: 0.36454 | val_0_unsup_loss_numpy: 1.367319941520691|  0:32:32s\n",
      "epoch 15 | loss: 0.35799 | val_0_unsup_loss_numpy: 0.3531799912452698|  0:34:47s\n",
      "epoch 16 | loss: 0.357   | val_0_unsup_loss_numpy: 0.3508400022983551|  0:37:00s\n",
      "epoch 17 | loss: 0.36235 | val_0_unsup_loss_numpy: 0.3294599950313568|  0:39:12s\n",
      "epoch 18 | loss: 0.35402 | val_0_unsup_loss_numpy: 0.3316900134086609|  0:41:22s\n",
      "epoch 19 | loss: 0.35319 | val_0_unsup_loss_numpy: 0.3288800120353699|  0:43:27s\n",
      "epoch 20 | loss: 0.35636 | val_0_unsup_loss_numpy: 0.35109999775886536|  0:45:28s\n",
      "epoch 21 | loss: 0.3472  | val_0_unsup_loss_numpy: 0.3266200125217438|  0:47:30s\n",
      "epoch 22 | loss: 0.35476 | val_0_unsup_loss_numpy: 0.40178999304771423|  0:49:32s\n",
      "epoch 23 | loss: 0.34692 | val_0_unsup_loss_numpy: 0.33392998576164246|  0:51:34s\n",
      "epoch 24 | loss: 0.34745 | val_0_unsup_loss_numpy: 0.3301500082015991|  0:53:45s\n",
      "epoch 25 | loss: 0.34569 | val_0_unsup_loss_numpy: 0.3255299925804138|  0:55:53s\n",
      "epoch 26 | loss: 0.34166 | val_0_unsup_loss_numpy: 0.32453998923301697|  0:58:05s\n",
      "epoch 27 | loss: 0.34013 | val_0_unsup_loss_numpy: 0.32666999101638794|  1:00:12s\n",
      "epoch 28 | loss: 0.34078 | val_0_unsup_loss_numpy: 0.32416000962257385|  1:02:24s\n",
      "epoch 29 | loss: 0.34263 | val_0_unsup_loss_numpy: 0.33278998732566833|  1:04:31s\n",
      "epoch 30 | loss: 0.34606 | val_0_unsup_loss_numpy: 0.32784000039100647|  1:06:34s\n",
      "epoch 31 | loss: 0.3404  | val_0_unsup_loss_numpy: 0.3228900134563446|  1:08:46s\n",
      "epoch 32 | loss: 0.33914 | val_0_unsup_loss_numpy: 0.33623000979423523|  1:11:01s\n",
      "epoch 33 | loss: 0.33896 | val_0_unsup_loss_numpy: 0.32234999537467957|  1:13:03s\n",
      "epoch 34 | loss: 0.34151 | val_0_unsup_loss_numpy: 0.3250100016593933|  1:15:14s\n",
      "epoch 35 | loss: 0.33742 | val_0_unsup_loss_numpy: 0.32826998829841614|  1:17:34s\n",
      "epoch 36 | loss: 0.33621 | val_0_unsup_loss_numpy: 0.3265799880027771|  1:19:37s\n",
      "epoch 37 | loss: 0.33547 | val_0_unsup_loss_numpy: 0.3362700045108795|  1:21:50s\n",
      "epoch 38 | loss: 0.33818 | val_0_unsup_loss_numpy: 0.3246299922466278|  1:23:54s\n",
      "epoch 39 | loss: 0.34047 | val_0_unsup_loss_numpy: 1184662552576.0|  1:26:05s\n",
      "epoch 40 | loss: 0.33662 | val_0_unsup_loss_numpy: 0.3755599856376648|  1:28:16s\n",
      "epoch 41 | loss: 0.33674 | val_0_unsup_loss_numpy: 0.3627200126647949|  1:30:26s\n",
      "epoch 42 | loss: 0.33496 | val_0_unsup_loss_numpy: 0.33118999004364014|  1:32:38s\n",
      "epoch 43 | loss: 0.5767  | val_0_unsup_loss_numpy: 0.35433000326156616|  1:34:55s\n",
      "\n",
      "Early stopping occurred at epoch 43 with best_epoch = 33 and best_val_0_unsup_loss_numpy = 0.32234999537467957\n",
      "Successfully saved model at models/all_tabnet_default_hyperp.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "if pretrain:\n",
    "    # Separate the X and the ys from the data\n",
    "    # remove the labels of all prediction tasks which are also in the datset \n",
    "    X_pretrain = pretrain_data.drop(all_learning_tasks_in_data, axis=1, errors='ignore')\n",
    "    print(X_pretrain.shape)\n",
    "    \n",
    "    # Fill with 0 the values that are missing in the input features so that the sample can still be used\n",
    "    print('Filling NA in samples with ', fill_na_val)\n",
    "    print('Number of rows with NA values in the input features: ', len(X_pretrain[X_pretrain.isna().any(axis=1)]))\n",
    "    print('Fraction of rows with NA values in the input features: ', len(X_pretrain[X_pretrain.isna().any(axis=1)])/X_pretrain.shape[0])\n",
    "    X_pretrain = X_pretrain.fillna(fill_na_val)\n",
    "    print('NOT imputing NA values and just keeing them as NAs instead')\n",
    "    \n",
    "    val_scaler = create_scaler(X_pretrain, EXP_PARAM['scaler'])\n",
    "    # Save the scaler for later use when doing supervised training \n",
    "    with open(model_save_path + scaler_to_save_name +'.pkl', 'wb') as f:\n",
    "        pickle.dump(val_scaler, f)\n",
    "    \n",
    "    if pretrain_type == 'dae':\n",
    "        pretrain_model = pretrain_with_dae(pd.DataFrame(val_scaler.transform(X_pretrain).copy()))\n",
    "        # save this pretrained model for later use \n",
    "        pretrain_model.save(model_save_path+pretrain_model_to_save_name)\n",
    "    elif pretrain_type == 'tabnet':\n",
    "        X_pretrain, X_pretrain_val = train_test_split(X_pretrain, test_size=0.2, random_state=IN_PARAM['rand_seed'], shuffle=True) \n",
    "        pretrain_model = pretrain_with_tabnet(pd.DataFrame(val_scaler.transform(X_pretrain).copy()),  pd.DataFrame(val_scaler.transform(X_pretrain_val).copy()))\n",
    "        pretrain_model.save_model(model_save_path+pretrain_model_to_save_name)\n",
    "        # show the results of feature importance\n",
    "        tabnet_explain(pretrain_model, X_pretrain_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a8177f-b6ab-42b0-8925-7ca60abadc5a",
   "metadata": {},
   "source": [
    "# Load Tabnet pretrained model for explainability results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c34521-0f09-4c03-a0b9-deb1d4ac0803",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Load a pretrained model for getting explainability results\n",
    "# Load the pretrained model and scaler \n",
    "#with open(model_save_path + scaler_to_load_name + '.pkl', 'rb') as f:\n",
    "#    val_scaler = pickle.load(f)\n",
    "#\n",
    "#\n",
    "## remove the labels of all prediction tasks which are also in the datset \n",
    "#X_pretrain_val = test_data.drop(all_learning_tasks_in_data, axis=1, errors='ignore')\n",
    "#print(X_pretrain_val.shape)\n",
    "#\n",
    "## Fill with 0 the values that are missing in the input features so that the sample can still be used\n",
    "#X_pretrain_val = X_pretrain_val.fillna(fill_na_val)\n",
    "#\n",
    "#X_pretrain_val = val_scaler.transform(X_pretrain_val).copy()\n",
    "# \n",
    "#if pretrain_model_to_load_type == 'tabnet':\n",
    "#    pretrain_model = TabNetPretrainer()\n",
    "#    pretrain_model.load_model(model_save_path+pretrain_model_to_load_name)\n",
    "#    X_pretrain_val = pretrain_model.predict(X_pretrain_val)[0]\n",
    "#    tabnet_explain(pretrain_model, X_pretrain_val)\n",
    "#else:\n",
    "#    print('Do not have explainability functionality for this model type')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44bd068-5356-4f57-97c0-42eb9fec0d25",
   "metadata": {},
   "source": [
    "# Train and test model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9424b44-9151-46fa-9de6-a510c9081546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning task:  vrFragment_trace.txt_vr_frag_thput_mbps\n",
      "Dropping rows for which the label is NA, since there is no ground truth\n",
      "(39613, 215)\n",
      "(9904, 215)\n",
      "(39613, 207)\n",
      "(39613,)\n",
      "(9904, 207)\n",
      "(9904,)\n",
      "Filling NA in samples with  0\n",
      "train data shape (39613, 207)\n",
      "test data shape (7428, 207)\n",
      "val data shape (2476, 207)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'StandardScaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 141\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28mprint\u001b[39m(X_train\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;66;03m# only supervised training \u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m     val_scaler \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_scaler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEXP_PARAM\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscaler\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;66;03m# save the MinMaxscaler model to a file\u001b[39;00m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(model_save_path \u001b[38;5;241m+\u001b[39m scaler_to_save_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/mobile-network-datasets-ns3/data-processing-scripts/ML_code/helper_functions.py:300\u001b[0m, in \u001b[0;36mcreate_scaler\u001b[0;34m(train, scaler_type)\u001b[0m\n\u001b[1;32m    298\u001b[0m     val_scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler()\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m scaler_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstandard\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 300\u001b[0m     val_scaler \u001b[38;5;241m=\u001b[39m \u001b[43mStandardScaler\u001b[49m()\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m scaler_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrobust\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    302\u001b[0m     val_scaler \u001b[38;5;241m=\u001b[39m RobustScaler()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'StandardScaler' is not defined"
     ]
    }
   ],
   "source": [
    "for learning_task in learning_tasks:\n",
    "    \n",
    "    print('Learning task: ', learning_task)\n",
    "\n",
    "    # Prepare the train and test sets\n",
    "    \n",
    "    # Drop rows when the label or ground truth is NA \n",
    "    print('Dropping rows for which the label is NA, since there is no ground truth')\n",
    "    train_data_na_dropped = train_data.dropna(subset=[learning_task])\n",
    "    test_data_na_dropped = test_data.dropna(subset=[learning_task])\n",
    "    print(train_data_na_dropped.shape)\n",
    "    print(test_data_na_dropped.shape)\n",
    "    \n",
    "    # Separate the X and the y from the data\n",
    "    # separate out the prediction task label column \n",
    "    y_train = train_data_na_dropped[learning_task]\n",
    "    y_test = test_data_na_dropped[learning_task]\n",
    "    # remove the labels of this and other prediction tasks which are also in the datset \n",
    "    X_train = train_data_na_dropped.drop(all_learning_tasks_in_data, axis=1, errors='ignore')\n",
    "    X_test = test_data_na_dropped.drop(all_learning_tasks_in_data, axis=1, errors='ignore')\n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)\n",
    "    print(X_test.shape)\n",
    "    print(y_test.shape)\n",
    "    \n",
    "    # Fill with 0 the values that are missing in the input features so that the sample can still be used\n",
    "    print('Filling NA in samples with ', fill_na_val)\n",
    "    X_train = X_train.fillna(fill_na_val)\n",
    "    X_test = X_test.fillna(fill_na_val)\n",
    "\n",
    "    if (learning_task in ['delay_trace.txt_ul_delay', 'delay_trace.txt_dl_delay']) and clip_outliers:\n",
    "        print('NOTE: clipping all rows with delay values > ', delay_clip_th)\n",
    "        y_train.loc[y_train > delay_clip_th] = delay_clip_th\n",
    "        y_test.loc[y_test > delay_clip_th] = delay_clip_th\n",
    "        #print('NOTE: dropping all rows with delay values > ', delay_drop_th)\n",
    "        #data = data[data['owd_ul'] <= delay_drop_th]\n",
    "    \n",
    "    # Save the columns to use for feature importance graphs\n",
    "    X_feats = np.array(X_train.columns)\n",
    "    np.savetxt('input_feature_list.csv', X_feats, delimiter=',', fmt=\"%s\")\n",
    "\n",
    "    # Convert everything to numpy \n",
    "    X_train = X_train.to_numpy()\n",
    "    y_train = y_train.to_numpy()\n",
    "    X_test = X_test.to_numpy()\n",
    "    y_test = y_test.to_numpy()\n",
    "\n",
    "    # If you want to shift the output feature window \n",
    "    # I am taking away this option since we are combining multiple runs and the values at the edge between 2 runs will be wrong\n",
    "    # I need to shift before combining runs and drop the last sample if I want to do this here \n",
    "    #if shift_samp_for_predict: \n",
    "    #    y = np.roll(y,1)\n",
    "    #    X = X[1:]\n",
    "    #    y = y[1:]   \n",
    "\n",
    "    strat = None\n",
    "\n",
    "    # Train test split\n",
    "    # Random shift\n",
    "    #sample_shift_count = np.random.randint(0, X.shape[0], size=1)\n",
    "    #X = np.roll(X, sample_shift_count)\n",
    "    #y = np.roll(y, sample_shift_count)\n",
    "    \n",
    "    # Randomly assign to train and test sets \n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, \n",
    "    #                                                    random_state=IN_PARAM['rand_seed'], stratify=strat) \n",
    "    X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, \n",
    "                                                  test_size=0.25, shuffle=True, \n",
    "                                                  random_state=IN_PARAM['rand_seed'],\n",
    "                                                  stratify=strat)  \n",
    "    \n",
    "    print('train data shape ' + str(X_train.shape))\n",
    "    print('test data shape ' + str(X_test.shape))\n",
    "    print('val data shape ' + str(X_val.shape))\n",
    "    \n",
    "    # Plot hist of output\n",
    "    # If classification it will just bin it\n",
    "    #plt.figure(1)\n",
    "    #plt.hist(y_train, bins=50, color='r', edgecolor='k', label='train samples')\n",
    "    #plt.xlabel(learning_task)\n",
    "    #plt.title('Histogram of train and test samples')\n",
    "    #plt.legend()\n",
    "    #plt.show()\n",
    "    #plt.figure(2)\n",
    "    #plt.hist(y_test, bins=50, color='b', edgecolor='k', label='test_samples')\n",
    "    #plt.xlabel(learning_task)\n",
    "    #plt.title('Histogram of train and test samples')\n",
    "    #plt.legend()\n",
    "    #plt.show()\n",
    "    \n",
    "    #=============================================== Train and test the model ==================================\n",
    "\n",
    "    OUT_PARAM = dict.fromkeys(OUT_PARAM, 0)\n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "    if use_pretrained_model:\n",
    "        # get the latent representations from pretrained model\n",
    "        # Load the pretrained model and scaler \n",
    "        # Load the saved MinMaxScaler object from the file\n",
    "        with open(model_save_path + scaler_to_load_name + '.pkl', 'rb') as f:\n",
    "            val_scaler = pickle.load(f)\n",
    "            \n",
    "        X_train = val_scaler.transform(X_train).copy()\n",
    "        X_test = val_scaler.transform(X_test).copy()\n",
    "        X_val = val_scaler.transform(X_val).copy()\n",
    "        \n",
    "        print('Before passing through pretrained model')\n",
    "        print(X_train.shape)\n",
    "        print(X_train)\n",
    "        print('Number of rows with NAs in the train set: ', np.sum(np.any(np.isnan(X_train), axis=1)))\n",
    "        print('Number of rows with NAs in the val set: ', np.sum(np.any(np.isnan(X_val), axis=1)))\n",
    "        \n",
    "        # Load the pretrained model\n",
    "        if pretrain_model_to_load_type == 'dae':\n",
    "            pretrain_model = dae_load(model_save_path+pretrain_model_to_load_name)\n",
    "            X_train = pretrain_model.transform(pd.DataFrame(X_train))#[:, -hid_size:]\n",
    "            X_test = pretrain_model.transform(pd.DataFrame(X_test))#[:, -hid_size:]\n",
    "            X_val = pretrain_model.transform(pd.DataFrame(X_val))#[:, -hid_size:]\n",
    "        elif pretrain_model_to_load_type == 'tabnet':\n",
    "            pretrain_model = TabNetPretrainer()\n",
    "            pretrain_model.load_model(model_save_path+pretrain_model_to_load_name)\n",
    "            X_train = pretrain_model.predict(X_train)[0]\n",
    "            X_test = pretrain_model.predict(X_test)[0]\n",
    "            X_val = pretrain_model.predict(X_val)[0]\n",
    "        else:\n",
    "            print('Do not know model type')\n",
    "        \n",
    "        print('After pretrainer transform')\n",
    "        print(X_train)\n",
    "        print(X_train.shape)\n",
    "        \n",
    "        #print('Fraction of rows with NA values in the input features: ', len(X_pretrain[X_pretrain.isna().any(axis=1)])/X_pretrain.shape[0])\n",
    "\n",
    "        print('Number of rows with NAs in the train set: ', np.sum(np.any(np.isnan(X_train), axis=1)))\n",
    "        print('Number of rows with NAs in the val set: ', np.sum(np.any(np.isnan(X_val), axis=1)))\n",
    "        print(X_train)\n",
    "        print(X_train.shape)\n",
    "        \n",
    "    else: # only supervised training \n",
    "        val_scaler = create_scaler(X_train, EXP_PARAM['scaler'])\n",
    "        # save the MinMaxscaler model to a file\n",
    "        with open(model_save_path + scaler_to_save_name + '.pkl', 'wb') as f:\n",
    "            pickle.dump(val_scaler, f)\n",
    "        \n",
    "        X_train = val_scaler.transform(X_train).copy()\n",
    "        X_test = val_scaler.transform(X_test).copy()\n",
    "        X_val = val_scaler.transform(X_val).copy()\n",
    "    \n",
    "    if load_sup_model:\n",
    "        # Load the supervised trained model\n",
    "        if sup_model_to_load_type == 'mlp':\n",
    "            with open(model_save_path+sup_model_to_load_name+'.pkl', 'rb') as file:\n",
    "                model = pickle.load(file)\n",
    "        elif sup_model_to_load_type == 'tabnet':\n",
    "            model = TabNetRegressor()\n",
    "            model.load_model(model_save_path+sup_model_to_load_name+'.zip')\n",
    "            # Feature importance \n",
    "            train_dataloader, valid_dataloaders = model._construct_loaders(X_train, y_train, list(zip(X_test, y_test)))\n",
    "            plot_feature_importance(model._compute_feature_importances(train_dataloader), X_feats, feat_filter)\n",
    "            #tabnet_explain(model, X_test, feat_filter, X_feats)\n",
    "        elif sup_model_to_load_type == 'xgb':\n",
    "            model = XGBRegressor()\n",
    "            model.load_model(model_save_path+sup_model_to_load_name+'.json')\n",
    "            # Feature importance \n",
    "            plot_feature_importance(model.feature_importances_, X_feats, feat_filter)\n",
    "        else:\n",
    "            print('Do not know model type')\n",
    "    else: # Need to train the model\n",
    "        # Train and save the model         \n",
    "        model, history = train_model(X_train, X_val, \n",
    "                                        y_train, y_val, \n",
    "                                        IN_PARAM,\n",
    "                                        model_save_path + suptrain_model_to_save_name,\n",
    "                                        sup_hyper_params)\n",
    "        end_time = time.time()\n",
    "        OUT_PARAM['runtime'] = end_time - start_time\n",
    "        print('Time to train model: ', end_time - start_time)   \n",
    "        # Feature importance\n",
    "        if (IN_PARAM['model_type'] == 'tabnet') or (IN_PARAM['model_type'] == 'xgb'):\n",
    "            plot_feature_importance(model.feature_importances_, X_feats, feat_filter)\n",
    "    \n",
    "    #====================================\n",
    "    # Predict using the supervised model \n",
    "    #====================================\n",
    "    \n",
    "    yhat_test = model.predict(X_test).flatten()\n",
    "    yhat_train = model.predict(X_train).flatten()\n",
    "    \n",
    "    print(IN_PARAM['eval_metric'])\n",
    "    OUT_PARAM['train_err'] = compute_error(y_train, yhat_train, IN_PARAM['eval_metric'])\n",
    "    OUT_PARAM['test_err'] = compute_error(y_test, yhat_test, IN_PARAM['eval_metric'])\n",
    "    print(OUT_PARAM)\n",
    "    print('ML model: ', IN_PARAM['eval_metric'],' err for test set: ', OUT_PARAM['test_err'])\n",
    "    print('Mean of the train set is: ', np.mean(y_train))\n",
    "    print('Median of the train set is: ', np.median(y_train))\n",
    "\n",
    "    print(y_test)\n",
    "    print(yhat_test)\n",
    "    bounds=[min( min(y_test),min(yhat_test) ), max( max(y_test),max(yhat_test) )]\n",
    "    print(bounds)\n",
    "    # Q-Q plot for the prediction and ground truth \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.plot(y_test, yhat_test, 'b.')\n",
    "    #plt.plot([0,0], [delay_drop_th, delay_drop_th], 'k-')\n",
    "    plt.xlabel('Ground truth test samples')\n",
    "    plt.ylabel('Prediction test samples')\n",
    "    plt.xlim(bounds)\n",
    "    plt.ylim(bounds)\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.plot(bounds, bounds, 'k-')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    if not os.path.isdir(model_save_path):\n",
    "        os.makedirs(model_save_path)\n",
    "    \n",
    "            \n",
    "    print('')\n",
    "    print('')\n",
    "    print('===============================  DONE  ===================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1deed7-72f0-456a-b141-2419893dd196",
   "metadata": {},
   "source": [
    "# Some potentially useful code for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d728021-c26b-4816-ba28-91d3b4c8165a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# plot the cdf of the train error \n",
    "print(yhat_train.shape) # ,1\n",
    "print(y_train.shape) # ,\n",
    "yhat_train_a = np.squeeze(yhat_train)\n",
    "yhat_test_a = np.squeeze(yhat_test)\n",
    "ecdf_train = ECDF(yhat_train_a - y_train)\n",
    "plt.step(ecdf_train.x, ecdf_train.y)\n",
    "plt.axvline(x=0, color='red', linestyle='--')\n",
    "plt.axhline(y=ecdf_train(0), color='red', linestyle='--')\n",
    "plt.xlabel('Pred err (truth - pred)')\n",
    "plt.title('Train samples')\n",
    "#plt.hist((yhat_train - y_train), bins=200, edgecolor='k')\n",
    "#plt.xlim(-20, 50)\n",
    "plt.show()\n",
    "print('Train: Probability mass of pred err (truth-pred) below 0 is: ',  ecdf_train(0))\n",
    "print('Train: Probability mass of pred err (truth-pred) above 0 is: ',  1-ecdf_train(0))\n",
    "\n",
    "# plot the cdf of the test error \n",
    "ecdf_test = ECDF(yhat_test_a - y_test)\n",
    "plt.step(ecdf_test.x, ecdf_test.y)\n",
    "plt.axvline(x=0, color='red', linestyle='--')\n",
    "plt.axhline(y=ecdf_test(0), color='red', linestyle='--')\n",
    "plt.xlabel('Pred err (truth - pred)')\n",
    "plt.title('Test samples')\n",
    "#plt.hist((yhat_test - y_test), bins=200, edgecolor='k')\n",
    "#plt.xlim(-20, 50)\n",
    "plt.show()\n",
    "print('Test: Probability mass of pred err (truth-pred) below 0 is: ',  ecdf_test(0))\n",
    "print('Test: Probability mass of pred err (truth-pred) above 0 is: ',  1-ecdf_test(0))\n",
    "\n",
    "#===================================== plot sorted samples of prediction overlayed with ground truth  ==========================\n",
    "# \n",
    "#if sort_test_samples:   \n",
    "#    #train_baseline_vals = np.repeat(baseline_pred, len(y_train))\n",
    "#    #test_baseline_vals = np.repeat(baseline_pred, len(y_test))\n",
    "#\n",
    "#    tmp1 = np.append(np.expand_dims(y_train, axis=1), np.expand_dims(yhat_train, axis=1), axis=1)\n",
    "#    tmp1 = tmp1[tmp1[:, 0].argsort()]\n",
    "#    y_train = tmp1[:,0]\n",
    "#    yhat_train = tmp1[:,1]\n",
    "#\n",
    "#    tmp2 = np.append(np.expand_dims(y_test, axis=1), np.expand_dims(yhat_test, axis=1), axis=1)\n",
    "#    tmp2 = tmp2[tmp2[:, 0].argsort()]\n",
    "#    y_test = tmp2[:,0]\n",
    "#    yhat_test = tmp2[:,1]\n",
    "\n",
    "##=============================================== bin the delay values to observe err per bin ==================================\n",
    "##\n",
    "## bin index for each delay value, so that we can put the values in the right bin \n",
    "#bin_indices = np.digitize(y_train, bin_edges)\n",
    "#\n",
    "## I want to take all the delay values for each bin\n",
    "#for bin_ind in np.unique(bin_indices):\n",
    "#    # these are the delay values in bin bin_edges[bin_ind]\n",
    "#    train_bin_uldelay_mean[bin_ind-1] = train_bin_uldelay_mean[bin_ind-1] + np.sum(y_train[bin_indices == bin_ind])\n",
    "#    train_bin_count[bin_ind-1] = train_bin_count[bin_ind-1] + len(y_train[bin_indices == bin_ind])\n",
    "#    \n",
    "#    # I want the corresponding err values for these delay values  \n",
    "#    train_bin_err_mean[bin_ind-1] = (train_bin_err_mean[bin_ind-1] + \n",
    "#                                     np.sum(np.abs(y_train[bin_indices == bin_ind] - yhat_train[bin_indices == bin_ind]) ))\n",
    "#    train_bin_baseline_err_mean[bin_ind-1] = (train_bin_baseline_err_mean[bin_ind-1] + \n",
    "#                                             np.sum(np.abs(y_train[bin_indices == bin_ind] - train_baseline_vals[bin_indices == bin_ind]) ))\n",
    "#    train_bin_perc_err_mean[bin_ind-1] = (train_bin_perc_err_mean[bin_ind-1] + \n",
    "#                                          np.sum(np.abs((y_train[bin_indices == bin_ind] - yhat_train[bin_indices == bin_ind]))/(y_train[bin_indices == bin_ind]) ))\n",
    "#    train_bin_baseline_perc_err_mean[bin_ind-1] = (train_bin_baseline_perc_err_mean[bin_ind-1] + \n",
    "#                                                  np.sum(np.abs((y_train[bin_indices == bin_ind] - train_baseline_vals[bin_indices == bin_ind]))/(y_train[bin_indices == bin_ind])) )\n",
    "#\n",
    "## bin index for each delay value\n",
    "#bin_indices = np.digitize(y_test, bin_edges)\n",
    "#\n",
    "## I want to take all the delay values for each bin \n",
    "#for bin_ind in np.unique(bin_indices):\n",
    "#    # these are the delay values in bin bin_edges[bin_ind]\n",
    "#    test_bin_uldelay_mean[bin_ind-1] = test_bin_uldelay_mean[bin_ind-1] + np.sum(y_test[bin_indices == bin_ind])\n",
    "#    test_bin_count[bin_ind-1] = test_bin_count[bin_ind-1] + len(y_test[bin_indices == bin_ind])\n",
    "#    \n",
    "#    # I want the corresponding err values for these delay values\n",
    "#    test_bin_err_mean[bin_ind-1] = (test_bin_err_mean[bin_ind-1] + \n",
    "#                                    np.sum(np.abs(y_test[bin_indices == bin_ind] - yhat_test[bin_indices == bin_ind])) )\n",
    "#    test_bin_baseline_err_mean[bin_ind-1] = (test_bin_baseline_err_mean[bin_ind-1] + \n",
    "#                                             np.sum(np.abs(y_test[bin_indices == bin_ind] - test_baseline_vals[bin_indices == bin_ind])))\n",
    "#    test_bin_perc_err_mean[bin_ind-1] = ( test_bin_perc_err_mean[bin_ind-1] + \n",
    "#                                         np.sum(np.abs((y_test[bin_indices == bin_ind] - yhat_test[bin_indices == bin_ind]))/(y_test[bin_indices == bin_ind])) )\n",
    "#    test_bin_baseline_perc_err_mean[bin_ind-1] = (test_bin_baseline_perc_err_mean[bin_ind-1] + \n",
    "#                                                  np.sum(np.abs((y_test[bin_indices == bin_ind] - test_baseline_vals[bin_indices == bin_ind]))/(y_test[bin_indices == bin_ind])) ) \n",
    "#\n",
    "## Plot\n",
    "#plot_y_yhat(y_train, yhat_train, y_test, yhat_test, model_save_path, IN_PARAM)\n",
    "#\n",
    "## Convert the regression output to class labels and do confusion matrix\n",
    "#cf_matrix = confusion_matrix(value_to_class_label(y_test, delay_class_edges), \n",
    "#                             value_to_class_label(yhat_test, delay_class_edges), normalize='true')\n",
    "#sns.set(rc={'figure.figsize':(8,7)}, font_scale = 1.5)\n",
    "#sns.heatmap(cf_matrix, annot=True, \n",
    "#    fmt='.1%', cmap='Blues')\n",
    "#\n",
    "#\n",
    "#=============================================== plot q-q prediction versus ground truth  ==================================\n",
    "\n",
    "\n",
    "# Binning by Y vale to see if the prediction error in each bin is consistent or not \n",
    "\n",
    "print('-----------------------------------------------------------')\n",
    "print('Top n feature list size: ', len(top_n_features))\n",
    "print(top_n_features)\n",
    "print('-----------------------------------------------------------')\n",
    "\n",
    "print('Loss fun: ', IN_PARAM['loss'])\n",
    "print('Eval err fun: ', IN_PARAM['eval_metric'])\n",
    "# After going over all runs     \n",
    "fig, ax1 = plt.subplots(figsize=(16, 4))\n",
    "ax2 = ax1.twinx()\n",
    "ax1.plot(train_bin_uldelay_mean/train_bin_count, train_bin_err_mean/train_bin_count, 'r.-', label='XGB pred err (truth-pred)')\n",
    "#ax1.plot(train_bin_uldelay_mean/train_bin_count, train_bin_baseline_err_mean/train_bin_count, 'm.-', label='baseline pred err (truth-pred)')\n",
    "ax1.axhline(y=0, color='r', linestyle='--')\n",
    "ax1.set_ylabel('error')\n",
    "#ax1.set_ylim(-5,25)\n",
    "ax1.legend()\n",
    "plt.xlabel('uplink delay (ms)')\n",
    "plt.title('Train samples')\n",
    "plt.grid()\n",
    "ax2.set_ylabel('relative err')\n",
    "ax2.plot(train_bin_uldelay_mean/train_bin_count, train_bin_perc_err_mean/train_bin_count, 'g*-', label='XGB relative err')\n",
    "#ax2.plot(train_bin_uldelay_mean/train_bin_count, train_bin_baseline_perc_err_mean/train_bin_count, 'c*-', label='baseline relative err')\n",
    "ax2.axhline(y=0, color='g', linestyle='--')\n",
    "plt.xscale('log')\n",
    "ax1.legend(loc=6)\n",
    "ax2.legend(loc=1)\n",
    "plt.show() \n",
    "\n",
    "plt.figure(figsize=(16, 2))\n",
    "plt.plot(train_bin_uldelay_mean/train_bin_count, train_bin_count, 'b*-')\n",
    "plt.xlabel('Train samples uplink delay bin (ms)')\n",
    "plt.xscale('log')\n",
    "plt.ylabel('bin count')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "print('Loss fun: ', IN_PARAM['loss'])\n",
    "print('Eval err fun: ', IN_PARAM['eval_metric'])\n",
    "fig, ax1 = plt.subplots(figsize=(16, 4))\n",
    "ax2 = ax1.twinx()\n",
    "ax1.plot(test_bin_uldelay_mean/test_bin_count, test_bin_err_mean/test_bin_count, 'r.-', label='XGB pred err (truth-pred)')\n",
    "#ax1.plot(test_bin_uldelay_mean/test_bin_count, test_bin_baseline_err_mean/test_bin_count, 'm.-', label='baseline pred err (truth-pred)')\n",
    "ax1.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('uplink delay (ms)')\n",
    "ax1.set_ylabel('error')\n",
    "#ax1.set_ylim(-500,250)\n",
    "ax1.legend()\n",
    "plt.title('Test samples')\n",
    "plt.grid()\n",
    "ax2.set_ylabel('relative err')\n",
    "ax2.plot(test_bin_uldelay_mean/test_bin_count, test_bin_perc_err_mean/test_bin_count, 'g*-', label='relative err')\n",
    "#ax2.plot(test_bin_uldelay_mean/test_bin_count, test_bin_baseline_perc_err_mean/test_bin_count, 'c*-', label='baseline relative err')\n",
    "ax2.axhline(y=0, color='g', linestyle='--')\n",
    "plt.xscale('log')\n",
    "ax1.legend(loc=6)\n",
    "ax2.legend(loc=1)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(16, 2))\n",
    "plt.plot(test_bin_uldelay_mean/test_bin_count, test_bin_count, 'b*-')\n",
    "plt.xlabel('Test samples uplink delay bins (ms)')\n",
    "plt.ylabel('bin count')\n",
    "plt.xscale('log')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6b81397-4e4e-4414-b254-36f55d8620d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notebook_save_str = 'no_pretrain_tabnet_notebook'\n",
    "os.system('cp train_and_eval_ML_models.ipynb '+'../saved_notebooks/'+notebook_save_str+'.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8364a61b-1675-46a1-a5c0-8c343eaa43f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_union = ['ul_loadUe_weightBand', 'ul_loadUe_BSRestimate', 'ul_BSRestimateLcg6', 'ul_loadUe_deltaIcc', 'ul_nrOfCsiPart1Bits', 'ul_loadUe_RI', 'dl_pucchFormatType', 'ul_loadUe_BSValueLcg6', 'ul_rv', 'ul_tbSizeInBits', 'dl_loadUe_dciFormat', 'dl_loadUe_pucchResourceIndicator', 'nbm_loadUe_WBeamIndex', 'ul_loadUe_fda', 'dl_NBeamRsrpCurrent', 'dl_loadUe_transmissionAttempt', 'dl_loadUe_bbBearerRef7', 'ul_BSRestimateLcg4', 'dl_numOfPrbs', 'ul_loadUe_isTransformPrecoding', 'ul_loadUe_timingOffset', 'dl_numberOfSymbols', 'ul_BSValueLcg0', 'dl_tbSizeInBits', 'ul_loadUe_VBit', 'ul_loadUe_BSRestimateLcg2', 'ul_csiRequest', 'ul_loadUe_BSValueLcg4', 'dl_loadUe_weightBand', 'wbm_rsrp1', 'dl_loadUe_NBeamRsrpCurrent', 'ul_tpcCommand', 'ul_iccAchievable', 'ul_loadUe_preamblePwr', 'nbm_beam03', 'dl_drb5data', 'dl_pucchSfn', 'ul_loadUe_numOfLayers', 'nbm_rsrp01', 'dl_icc', 'ul_loadUe_iccAchievable', 'ul_fda', 'ul_loadUe_uciDecodedResult', 'ul_loadUe_BSValueLcg0', 'dl_dlCcIndex', 'nbm_loadUe_rsrp11', 'wbm_beam4', 'dl_loadUe_incrementalWeight', 'dl_loadUe_antennaPorts', 'ul_BSValueLcg5', 'ul_RI', 'dl_loadUe_fda', 'dl_loadUe_numBearers', 'ul_loadUe_csiRequest', 'dl_linkAdaptationUeMode', 'wbm_loadUe_cellId', 'ul_loadUe_ACK', 'dl_cellId', 'dl_bbBearerRef2', 'ul_loadUe_BSValueLcg7', 'dl_loadUe_cqi', 'dl_pdschHarqFeedbackTiming', 'dl_loadUe_pucchSfn', 'dl_loadUe_pdschHarqFeedbackTiming', 'ul_loadUe_isClpcSaturated', 'ul_loadUe_startSymbolPusch', 'dl_loadUe_dlCcIndex', 'dl_loadUe_drb7input', 'dl_bbBearerRef7', 'dl_loadUe_bbBearerRef2', 'ul_loadUe_beamIndex', 'dl_rank', 'dl_loadUe_drb4input', 'dl_numBearers', 'wbm_loadUe_beam2', 'nbm_loadUe_beam01', 'ul_loadUe_WBeamRsrpCurrent', 'ul_carrierAggregationUsed', 'nbm_loadUe_rsrp12', 'dl_drb4input', 'dl_mcsIndex', 'dl_sinr', 'dl_drb4data', 'ul_loadUe_harqProcessId', 'dl_transmissionAttempt', 'ul_loadUe_BSRestimateLcg7', 'dl_loadUe_isDrxEnabled', 'dl_drb0data', 'dl_loadUe_numOfPrbs', 'dl_loadUe_drb4data', 'ul_loadUe_numberOfSymbolsPusch', 'nbm_loadUe_ueTraceIdMsw', 'wbm_loadUe_beam3', 'dl_sectorIndex', 'dl_drb1input', 'dl_numberOfSrcBits', 'nbm_beam13', 'ul_clpcCarrierDemand', 'ul_wbUsedNbOverridden', 'ul_loadUe_puschTotalRxPsdAvg', 'dl_srOnPucch', 'dl_loadUe_sectorIndex', 'wbm_beam3', 'dl_drb0input', 'ul_BSRestimate', 'ul_BSRestimateLcg1', 'dl_loadUe_taValue', 'ul_maxRank', 'wbm_beam1', 'dl_loadUe_bbBearerRef3', 'dl_physicalCellId', 'ul_totalCellsReqScheduling', 'wbm_loadUe_beam4', 'nbm_loadUe_rsrp02', 'wbm_loadUe_WBeamIndexNewBest', 'dl_loadUe_WBeamRsrpCurrent', 'nbm_WBeamIndex', 'dl_loadUe_slot', 'dl_bbBearerRef1', 'dl_drb3data', 'nbm_loadUe_rsrp13', 'ul_macSduInBytes', 'dl_loadUe_srOnPucch', 'ul_measNumOfPrb', 'ul_transmissionAttempt', 'ul_loadUe_BSRestimateLcg3', 'dl_loadUe_deltaIcc', 'dl_dciFormat', 'ul_bbCellIndex', 'dl_loadUe_macCtrlElement', 'dl_weightBand', 'ul_loadUe_mcsIndex', 'ul_loadUe_antennaPorts', 'ul_timingOffset', 'dl_redundancyVersion', 'ul_loadUe_BSRestimateLcg1', 'ul_loadUe_wbUsedNbOverridden', 'dl_numberOfActivatedDlCells', 'dl_antennaPorts', 'dl_drb7input', 'dl_harqProcessId', 'nbm_loadUe_beam13', 'ul_loadUe_nrOfCsiPart1Bits', 'ul_dciFormat', 'dl_pucchSlotNo', 'nbm_NBeamIndexChosen', 'ul_precodingInfo', 'ul_loadUe_NBeamRsrpCurrent', 'ul_loadUe_harqFailure', 'dl_bbBearerRef4', 'dl_ndi', 'nbm_beam12', 'dl_loadUe_tda', 'nbm_loadUe_cellId', 'wbm_loadUe_WBeamIndexCurrent', 'ul_ndi', 'dl_loadUe_tbSizeInBits', 'wbm_cellId', 'dl_loadUe_icc', 'nbm_loadUe_beam02', 'dl_pucchResourceIndicator', 'ul_BSValueLcg2', 'ul_loadUe_macSduInBytes', 'ul_measNumOfLayers', 'dl_loadUe_drb1input', 'dl_WBeamRsrpCurrent', 'nbm_loadUe_noOfCriPerCsiReport', 'ul_antennaPorts', 'ul_loadUe_slot', 'wbm_beam2', 'ul_loadUe_startPrb', 'ul_slot', 'nbm_loadUe_rsrp01', 'ul_BSRestimateLcg0', 'ul_loadUe_numOfPrbs', 'ul_isDrxEnabled', 'ul_loadUe_BSRestimateLcg0', 'wbm_rsrp4', 'ul_BSValueLcg4', 'dl_loadUe_pucchSlotNo', 'ul_preamblePwr', 'wbm_loadUe_beam1', 'dl_ACK', 'ul_isPrimaryCell', 'dl_tda', 'ul_loadUe_postEqSinr0', 'dl_loadUe_sinr', 'ul_loadUe_pCmaxCIndex', 'ul_loadUe_cellId', 'dl_loadUe_physicalCellId', 'dl_loadUe_beamIndex', 'dl_loadUe_numberOfSymbols', 'dl_loadUe_drb2data', 'dl_loadUe_drb1data', 'ul_BSRestimateLcg7', 'ul_numOfLayers', 'ul_loadUe_numberOfActivatedUlCells', 'ul_loadUe_BSValueLcg1', 'nbm_beam11', 'ul_loadUe_BSValueLcg5', 'dl_loadUe_newData/reTx(1/0)', 'dl_loadUe_bbBearerRef6', 'ul_loadUe_bbCellIndex', 'nbm_loadUe_NBeamIndexChosen', 'dl_loadUe_numberOfActivatedDlCells', 'ul_isClpcSaturated', 'ul_loadUe_BSRestimateLcg5', 'ul_loadUe_tbSizeInBits', 'ul_BSValueLcg1', 'ul_loadUe_maxRank', 'dl_fda', 'ul_DTX', 'ul_loadUe_clpcCarrierDemand', 'dl_cqi', 'dl_dai', 'wbm_rsrp3', 'ul_numOfPrbs', 'dl_drb3input', 'ul_puschTotalRxPsdAvg', 'nbm_loadUe_NBeamIndexCurrent', 'nbm_cellId', 'ul_loadUe_postEqSinr1', 'dl_loadUe_drb3input', 'ul_pCmaxCIndex', 'ul_numberOfSymbolsPusch', 'dl_loadUe_mcsIndex', 'dl_newData/reTx(1/0)', 'ul_weightBand', 'nbm_rsrp02', 'ul_uciDecodedResult', 'ul_loadUe_BSValueLcg2', 'ul_powerHeadRoomIndex', 'ul_loadUe_rv', 'ul_incrementalWeight', 'dl_loadUe_drb2input', 'dl_loadUe_drb7data', 'dl_drb6input', 'wbm_rsrp2', 'dl_loadUe_bbCellIndex', 'dl_bbBearerRef3', 'dl_loadUe_feedbackIndex', 'ul_startPrb', 'ul_cellId', 'wbm_loadUe_rsrp1', 'dl_loadUe_rank', 'ul_ACK', 'ul_beamIndex', 'dl_isSrBitIncluded', 'ul_numberOfActivatedUlCells', 'dl_bbBearerRef6', 'ul_loadUe_transmissionAttempt', 'dl_loadUe_isPrimaryCell', 'wbm_loadUe_rsrp4', 'ul_BSValueLcg3', 'dl_macCtrlElement', 'dl_drb6data', 'dl_isPrimaryCell', 'ul_harqFailure', 'dl_bbCellIndex', 'ul_loadUe_totalCellsReqScheduling', 'ul_postEqSinr0', 'ul_loadUe_carrierAggregationUsed', 'dl_loadUe_bbBearerRef4', 'ul_loadUe_powerHeadRoomIndex', 'ul_BSValueLcg6', 'nbm_loadUe_beam03', 'dl_drb2data', 'nbm_rsrp13', 'dl_isDrxEnabled', 'ul_loadUe_measNumOfPrb', 'dl_feedbackIndex', 'ul_loadUe_isDrxEnabled', 'dl_loadUe_bbBearerRef1', 'ul_loadUe_ulRequestTypeBitmap', 'nbm_rsrp03', 'dl_slot', 'dl_loadUe_drb5input', 'ul_loadUe_BSRestimateLcg4', 'dl_beamIndex', 'dl_loadUe_isSrBitIncluded', 'ul_BSRestimateLcg3', 'ul_WBeamRsrpCurrent', 'dl_loadUe_DTX', 'nbm_rsrp11', 'ul_BSRestimateLcg5', 'dl_loadUe_redundancyVersion', 'ul_ulRequestTypeBitmap', 'wbm_WBeamIndexNewBest', 'ul_ulschIndicator', 'dl_drb1data', 'dl_drb2input', 'ul_loadUe_BSRestimateLcg6', 'nbm_loadUe_rsrp03', 'dl_loadUe_wbUsedNbOverridden', 'dl_loadUe_drb6input', 'nbm_loadUe_beam11', 'nbm_beam01', 'dl_loadUe_linkAdaptationUeMode', 'ul_loadUe_linkAdaptationUeMode', 'dl_loadUe_bbBearerRef0', 'dl_drb5input', 'dl_loadUe_drb6data', 'ul_loadUe_BSValueLcg3', 'dl_loadUe_drb0data', 'ul_loadUe_sinrAchievable', 'nbm_loadUe_beam12', 'ul_loadUe_incrementalWeight', 'dl_loadUe_dai', 'dl_loadUe_ueTraceIdMsw', 'ul_loadUe_ndi', 'ul_startSymbolPusch', 'dl_loadUe_cellId', 'ul_isTransformPrecoding', 'dl_loadUe_drb5data', 'dl_loadUe_drb3data', 'ul_deltaIcc', 'nbm_noOfCriPerCsiReport', 'dl_loadUe_drb0input', 'ul_mcsIndex', 'ul_chipsetType', 'dl_loadUe_numberOfSrcBits', 'nbm_beam02', 'ul_postEqSinr1', 'ul_loadUe_isPrimaryCell', 'dl_incrementalWeight', 'dl_loadUe_ACK', 'ul_BSRestimateLcg2', 'ul_NBeamRsrpCurrent', 'dl_DTX', 'dl_bbBearerRef5', 'dl_loadUe_bbBearerRef5', 'ul_harqProcessId', 'dl_loadUe_pucchFormatType', 'wbm_loadUe_rsrp2', 'dl_taValue', 'ul_loadUe_precodingInfo', 'ul_BSValueLcg7', 'wbm_loadUe_ueTraceIdMsw', 'dl_loadUe_ndi', 'dl_deltaIcc', 'dl_drb7data', 'ul_loadUe_DTX', 'ul_linkAdaptationUeMode', 'wbm_loadUe_rsrp3', 'nbm_NBeamIndexCurrent', 'dl_loadUe_harqProcessId', 'ul_loadUe_ulschIndicator', 'dl_wbUsedNbOverridden', 'ul_loadUe_measNumOfLayers', 'nbm_rsrp12', 'dl_bbBearerRef0', 'wbm_WBeamIndexCurrent', 'ul_sinrAchievable']\n",
    "print(feat_union)\n",
    "print(len(feat_union))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f8db5a-6547-41cb-9cdc-cf6f0587b854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of featurtes I think will not generalize \n",
    "filtered_cols = [i for i in feat_union if 'eamIndex' in i]\n",
    "print(filtered_cols)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
