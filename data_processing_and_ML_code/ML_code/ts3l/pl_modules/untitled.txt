---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[12], line 282
    274     train_dl = DataLoader(train_ds, s3l_sup_mlp['batch_size'], 
    275                          shuffle=False, sampler = SequentialSampler(train_ds), num_workers=4)
    281 if learning_task_type == 'reg':
--> 282     yhat_test = sup_trainer.predict(model, test_dl)
    283     yhat_test = torch.concat([out.cpu() for out in yhat_test]).squeeze()
    284     yhat_train = sup_trainer.predict(model, train_dl)

File ~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:863, in Trainer.predict(self, model, dataloaders, datamodule, return_predictions, ckpt_path)
    861 self.state.status = TrainerStatus.RUNNING
    862 self.predicting = True
--> 863 return call._call_and_handle_interrupt(
    864     self, self._predict_impl, model, dataloaders, datamodule, return_predictions, ckpt_path
    865 )

File ~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:44, in _call_and_handle_interrupt(trainer, trainer_fn, *args, **kwargs)
     42     if trainer.strategy.launcher is not None:
     43         return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
---> 44     return trainer_fn(*args, **kwargs)
     46 except _TunerExitException:
     47     _call_teardown_hook(trainer)

File ~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:902, in Trainer._predict_impl(self, model, dataloaders, datamodule, return_predictions, ckpt_path)
    898 assert self.state.fn is not None
    899 ckpt_path = self._checkpoint_connector._select_ckpt_path(
    900     self.state.fn, ckpt_path, model_provided=model_provided, model_connected=self.lightning_module is not None
    901 )
--> 902 results = self._run(model, ckpt_path=ckpt_path)
    904 assert self.state.stopped
    905 self.predicting = False

File ~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:986, in Trainer._run(self, model, ckpt_path)
    981 self._signal_connector.register_signal_handlers()
    983 # ----------------------------
    984 # RUN THE TRAINER
    985 # ----------------------------
--> 986 results = self._run_stage()
    988 # ----------------------------
    989 # POST-Training CLEAN UP
    990 # ----------------------------
    991 log.debug(f"{self.__class__.__name__}: trainer tearing down")

File ~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1025, in Trainer._run_stage(self)
   1023     return self._evaluation_loop.run()
   1024 if self.predicting:
-> 1025     return self.predict_loop.run()
   1026 if self.training:
   1027     with isolate_rng():

File ~/.local/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:182, in _no_grad_context.<locals>._decorator(self, *args, **kwargs)
    180     context_manager = torch.no_grad
    181 with context_manager():
--> 182     return loop_run(self, *args, **kwargs)

File ~/.local/lib/python3.8/site-packages/pytorch_lightning/loops/prediction_loop.py:124, in _PredictionLoop.run(self)
    122     self.batch_progress.is_last_batch = data_fetcher.done
    123     # run step hooks
--> 124     self._predict_step(batch, batch_idx, dataloader_idx, dataloader_iter)
    125 except StopIteration:
    126     # this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support
    127     break

File ~/.local/lib/python3.8/site-packages/pytorch_lightning/loops/prediction_loop.py:253, in _PredictionLoop._predict_step(self, batch, batch_idx, dataloader_idx, dataloader_iter)
    247 # configure step_kwargs
    248 step_args = (
    249     self._build_step_args_from_hook_kwargs(hook_kwargs, "predict_step")
    250     if not using_dataloader_iter
    251     else (dataloader_iter,)
    252 )
--> 253 predictions = call._call_strategy_hook(trainer, "predict_step", *step_args)
    254 if predictions is None:
    255     self._warning_cache.warn("predict returned None if it was on purpose, ignore this warning...")

File ~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:311, in _call_strategy_hook(trainer, hook_name, *args, **kwargs)
    308     return None
    310 with trainer.profiler.profile(f"[Strategy]{trainer.strategy.__class__.__name__}.{hook_name}"):
--> 311     output = fn(*args, **kwargs)
    313 # restore current_fx when nested context
    314 pl_module._current_fx_name = prev_fx_name

File ~/.local/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:437, in Strategy.predict_step(self, *args, **kwargs)
    435 if self.model != self.lightning_module:
    436     return self._forward_redirection(self.model, self.lightning_module, "predict_step", *args, **kwargs)
--> 437 return self.lightning_module.predict_step(*args, **kwargs)

File ~/mobile-network-datasets-ns3/data_processing_and_ML_code/ML_code/ts3l/pl_modules/subtab_lightning.py:103, in SubTabLightning.predict_step(self, batch, batch_idx)
     92 def predict_step(self, batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor], batch_idx: int) -> torch.Tensor:
     93     """The perdict step of SubTab
     94 
     95     Args:
   (...)
    100         torch.FloatTensor: The predicted output (logit)
    101     """
--> 103     return F.subtab.second_phase_step(self.model, batch)

File ~/mobile-network-datasets-ns3/data_processing_and_ML_code/ML_code/ts3l/functional/subtab.py:107, in second_phase_step(model, batch)
     95 def second_phase_step(
     96     model: nn.Module, batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]
     97 ) -> torch.Tensor:
     98     """Forward step of SubTab during the second phase.
     99 
    100     Args:
   (...)
    105         torch.Tensor: The predicted label (logit).
    106     """
--> 107     x, _, _ = batch
    108     return model(x).squeeze()

ValueError: not enough values to unpack (expected 3, got 2)