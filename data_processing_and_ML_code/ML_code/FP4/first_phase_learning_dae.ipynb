{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ed58de6-5ac0-4bbb-95e7-c77e64dfaf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload\n",
    "#%reset\n",
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "\n",
    "import warnings\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# DEBUG MODE\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.getLogger('tensorflow').setLevel(logging.FATAL)\n",
    "    \n",
    "from s3l_training import s3l_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26eb320c-c17f-40dd-8264-6dcbd0933f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run with random seed:  561\n",
      "GPU is available.\n",
      "1\n",
      "True\n",
      "12.1\n",
      "Concatenating runs:  range(1, 11)\n",
      "Time to read csv file for run:  5.135329723358154\n",
      "Loaded run 1\n",
      "Time to read csv file for run:  5.545825719833374\n",
      "Loaded run 2\n",
      "Time to read csv file for run:  5.25007963180542\n",
      "Loaded run 3\n",
      "Time to read csv file for run:  5.264433860778809\n",
      "Loaded run 4\n",
      "Time to read csv file for run:  5.014785528182983\n",
      "Loaded run 5\n",
      "Time to read csv file for run:  5.321317195892334\n",
      "Loaded run 6\n",
      "Time to read csv file for run:  4.866106033325195\n",
      "Loaded run 7\n",
      "Time to read csv file for run:  4.572672128677368\n",
      "Loaded run 8\n",
      "Time to read csv file for run:  4.421094179153442\n",
      "Loaded run 9\n",
      "Time to read csv file for run:  4.157354116439819\n",
      "Loaded run 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrain_data, before removing rows that dont have traffic  (1799100, 102)\n",
      "pretrain_data, after removing rows that dont have traffic  (33012, 102)\n",
      "X_pretrain  (32691, 92)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "refresh rate:  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                     | Type    | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | task_loss_fn             | MSELoss | 0      | train\n",
      "1 | mask_loss_fn             | BCELoss | 0      | train\n",
      "2 | categorical_feature_loss | BCELoss | 0      | train\n",
      "3 | continuous_feature_loss  | MSELoss | 0      | train\n",
      "4 | model                    | DAE     | 294 K  | train\n",
      "-------------------------------------------------------------\n",
      "294 K     Trainable params\n",
      "0         Non-trainable params\n",
      "294 K     Total params\n",
      "1.176     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|█████████████████████████████████████████████████████████████████████████████████| 232/232 [00:02<00:00, 80.89it/s, v_num=167]\n",
      "Epoch 1: 100%|███████████████████████████████████████████████| 232/232 [00:02<00:00, 83.19it/s, v_num=167, train_loss=2.370, val_loss=1.750]\u001b[A\n",
      "Epoch 2: 100%|███████████████████████████████████████████████| 232/232 [00:02<00:00, 81.76it/s, v_num=167, train_loss=1.660, val_loss=1.450]\u001b[A\n",
      "Epoch 3: 100%|███████████████████████████████████████████████| 232/232 [00:02<00:00, 79.28it/s, v_num=167, train_loss=1.470, val_loss=1.350]\u001b[A\n",
      "Epoch 4: 100%|███████████████████████████████████████████████| 232/232 [00:02<00:00, 81.36it/s, v_num=167, train_loss=1.390, val_loss=1.280]\u001b[A\n",
      "Epoch 5: 100%|███████████████████████████████████████████████| 232/232 [00:02<00:00, 87.43it/s, v_num=167, train_loss=1.320, val_loss=1.230]\u001b[A\n",
      "Epoch 6: 100%|███████████████████████████████████████████████| 232/232 [00:02<00:00, 86.08it/s, v_num=167, train_loss=1.280, val_loss=1.170]\u001b[A\n",
      "Epoch 7: 100%|███████████████████████████████████████████████| 232/232 [00:02<00:00, 84.59it/s, v_num=167, train_loss=1.230, val_loss=1.130]\u001b[A\n",
      "Epoch 8: 100%|███████████████████████████████████████████████| 232/232 [00:02<00:00, 91.11it/s, v_num=167, train_loss=1.190, val_loss=1.100]\u001b[A\n",
      "Epoch 9: 100%|███████████████████████████████████████████████| 232/232 [00:02<00:00, 88.03it/s, v_num=167, train_loss=1.160, val_loss=1.060]\u001b[A\n",
      "Epoch 10: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 82.41it/s, v_num=167, train_loss=1.130, val_loss=1.040]\u001b[A\n",
      "Epoch 11: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 84.24it/s, v_num=167, train_loss=1.100, val_loss=1.020]\u001b[A\n",
      "Epoch 12: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 84.43it/s, v_num=167, train_loss=1.080, val_loss=1.000]\u001b[A\n",
      "Epoch 13: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 91.18it/s, v_num=167, train_loss=1.070, val_loss=1.000]\u001b[A\n",
      "Epoch 14: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 89.57it/s, v_num=167, train_loss=1.060, val_loss=0.984]\u001b[A\n",
      "Epoch 15: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 88.05it/s, v_num=167, train_loss=1.040, val_loss=0.965]\u001b[A\n",
      "Epoch 16: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 86.46it/s, v_num=167, train_loss=1.030, val_loss=0.960]\u001b[A\n",
      "Epoch 17: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 95.87it/s, v_num=167, train_loss=1.020, val_loss=0.946]\u001b[A\n",
      "Epoch 18: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 88.17it/s, v_num=167, train_loss=1.010, val_loss=0.943]\u001b[A\n",
      "Epoch 19: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 84.04it/s, v_num=167, train_loss=0.996, val_loss=0.930]\u001b[A\n",
      "Epoch 20: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 88.15it/s, v_num=167, train_loss=0.987, val_loss=0.925]\u001b[A\n",
      "Epoch 21: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 81.66it/s, v_num=167, train_loss=0.979, val_loss=0.924]\u001b[A\n",
      "Epoch 22: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 84.53it/s, v_num=167, train_loss=0.973, val_loss=0.913]\u001b[A\n",
      "Epoch 23: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 85.95it/s, v_num=167, train_loss=0.967, val_loss=0.907]\u001b[A\n",
      "Epoch 24: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 88.72it/s, v_num=167, train_loss=0.964, val_loss=0.900]\u001b[A\n",
      "Epoch 25: 100%|██████████████████████████████████████████████| 232/232 [00:03<00:00, 76.43it/s, v_num=167, train_loss=0.954, val_loss=0.907]\u001b[A\n",
      "Epoch 26: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 84.25it/s, v_num=167, train_loss=0.954, val_loss=0.891]\u001b[A\n",
      "Epoch 27: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 85.14it/s, v_num=167, train_loss=0.947, val_loss=0.909]\u001b[A\n",
      "Epoch 28: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 83.89it/s, v_num=167, train_loss=0.940, val_loss=0.878]\u001b[A\n",
      "Epoch 29: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 82.76it/s, v_num=167, train_loss=0.941, val_loss=0.879]\u001b[A\n",
      "Epoch 30: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 82.88it/s, v_num=167, train_loss=0.931, val_loss=0.874]\u001b[A\n",
      "Epoch 31: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 85.86it/s, v_num=167, train_loss=0.927, val_loss=0.884]\u001b[A\n",
      "Epoch 32: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 84.99it/s, v_num=167, train_loss=0.922, val_loss=0.866]\u001b[A\n",
      "Epoch 33: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 83.52it/s, v_num=167, train_loss=0.929, val_loss=0.882]\u001b[A\n",
      "Epoch 34: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 84.80it/s, v_num=167, train_loss=0.920, val_loss=0.867]\u001b[A\n",
      "Epoch 35: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 84.31it/s, v_num=167, train_loss=0.915, val_loss=0.879]\u001b[A\n",
      "Epoch 36: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 84.87it/s, v_num=167, train_loss=0.920, val_loss=0.851]\u001b[A\n",
      "Epoch 37: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 84.61it/s, v_num=167, train_loss=0.910, val_loss=0.853]\u001b[A\n",
      "Epoch 38: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 85.39it/s, v_num=167, train_loss=0.906, val_loss=0.849]\u001b[A\n",
      "Epoch 39: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 82.23it/s, v_num=167, train_loss=0.913, val_loss=0.859]\u001b[A\n",
      "Epoch 40: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 91.29it/s, v_num=167, train_loss=0.902, val_loss=0.848]\u001b[A\n",
      "Epoch 41: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 94.43it/s, v_num=167, train_loss=0.899, val_loss=0.838]\u001b[A\n",
      "Epoch 42: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 91.60it/s, v_num=167, train_loss=0.893, val_loss=0.853]\u001b[A\n",
      "Epoch 43: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 91.91it/s, v_num=167, train_loss=0.888, val_loss=0.842]\u001b[A\n",
      "Epoch 44: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 86.59it/s, v_num=167, train_loss=0.901, val_loss=0.839]\u001b[A\n",
      "Epoch 45: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 91.01it/s, v_num=167, train_loss=0.894, val_loss=0.856]\u001b[A\n",
      "Epoch 46: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 86.07it/s, v_num=167, train_loss=0.887, val_loss=0.832]\u001b[A\n",
      "Epoch 47: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 86.11it/s, v_num=167, train_loss=0.894, val_loss=0.836]\u001b[A\n",
      "Epoch 48: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 81.30it/s, v_num=167, train_loss=0.895, val_loss=0.844]\u001b[A\n",
      "Epoch 49: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 85.61it/s, v_num=167, train_loss=0.888, val_loss=0.833]\u001b[A\n",
      "Epoch 50: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 89.70it/s, v_num=167, train_loss=0.883, val_loss=0.830]\u001b[A\n",
      "Epoch 51: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 88.24it/s, v_num=167, train_loss=0.882, val_loss=0.837]\u001b[A\n",
      "Epoch 52: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 93.64it/s, v_num=167, train_loss=0.882, val_loss=0.846]\u001b[A\n",
      "Epoch 53: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 92.69it/s, v_num=167, train_loss=0.886, val_loss=0.822]\u001b[A\n",
      "Epoch 54: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 86.32it/s, v_num=167, train_loss=0.886, val_loss=0.832]\u001b[A\n",
      "Epoch 55: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 90.12it/s, v_num=167, train_loss=0.877, val_loss=0.837]\u001b[A\n",
      "Epoch 56: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 92.71it/s, v_num=167, train_loss=0.879, val_loss=0.834]\u001b[A\n",
      "Epoch 57: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 88.89it/s, v_num=167, train_loss=0.876, val_loss=0.823]\u001b[A\n",
      "Epoch 58: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 90.41it/s, v_num=167, train_loss=0.879, val_loss=0.821]\u001b[A\n",
      "Epoch 59: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 91.11it/s, v_num=167, train_loss=0.875, val_loss=0.840]\u001b[A\n",
      "Epoch 60: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 90.13it/s, v_num=167, train_loss=0.876, val_loss=0.823]\u001b[A\n",
      "Epoch 61: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 90.08it/s, v_num=167, train_loss=0.871, val_loss=0.842]\u001b[A\n",
      "Epoch 62: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 97.22it/s, v_num=167, train_loss=0.874, val_loss=0.826]\u001b[A\n",
      "Epoch 63: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 87.49it/s, v_num=167, train_loss=0.874, val_loss=0.830]\u001b[A\n",
      "Epoch 64: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 95.30it/s, v_num=167, train_loss=0.872, val_loss=0.833]\u001b[A\n",
      "Epoch 65: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 90.65it/s, v_num=167, train_loss=0.870, val_loss=0.828]\u001b[A\n",
      "Epoch 66: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 95.67it/s, v_num=167, train_loss=0.868, val_loss=0.822]\u001b[A\n",
      "Epoch 67: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 83.82it/s, v_num=167, train_loss=0.864, val_loss=0.803]\u001b[A\n",
      "Epoch 68: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 94.66it/s, v_num=167, train_loss=0.865, val_loss=0.812]\u001b[A\n",
      "Epoch 69: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 91.30it/s, v_num=167, train_loss=0.862, val_loss=0.806]\u001b[A\n",
      "Epoch 70: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 92.14it/s, v_num=167, train_loss=0.860, val_loss=0.816]\u001b[A\n",
      "Epoch 71: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 88.32it/s, v_num=167, train_loss=0.862, val_loss=0.811]\u001b[A\n",
      "Epoch 72: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 90.31it/s, v_num=167, train_loss=0.857, val_loss=0.819]\u001b[A\n",
      "Epoch 73: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 92.62it/s, v_num=167, train_loss=0.859, val_loss=0.818]\u001b[A\n",
      "Epoch 74: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 98.34it/s, v_num=167, train_loss=0.857, val_loss=0.818]\u001b[A\n",
      "Epoch 75: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 86.69it/s, v_num=167, train_loss=0.858, val_loss=0.833]\u001b[A\n",
      "Epoch 76: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 88.61it/s, v_num=167, train_loss=0.858, val_loss=0.813]\u001b[A\n",
      "Epoch 77: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 89.86it/s, v_num=167, train_loss=0.860, val_loss=0.810]\u001b[A\n",
      "Epoch 78: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 87.65it/s, v_num=167, train_loss=0.868, val_loss=0.832]\u001b[A\n",
      "Epoch 79: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 91.91it/s, v_num=167, train_loss=0.863, val_loss=0.806]\u001b[A\n",
      "Epoch 80: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 92.27it/s, v_num=167, train_loss=0.859, val_loss=0.811]\u001b[A\n",
      "Epoch 81: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 86.27it/s, v_num=167, train_loss=0.859, val_loss=0.818]\u001b[A\n",
      "Epoch 82: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 92.21it/s, v_num=167, train_loss=0.854, val_loss=0.812]\u001b[A\n",
      "Epoch 83: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 91.54it/s, v_num=167, train_loss=0.855, val_loss=0.809]\u001b[A\n",
      "Epoch 84: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 92.82it/s, v_num=167, train_loss=0.856, val_loss=0.817]\u001b[A\n",
      "Epoch 85: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 96.38it/s, v_num=167, train_loss=0.854, val_loss=0.815]\u001b[A\n",
      "Epoch 86: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 93.82it/s, v_num=167, train_loss=0.849, val_loss=0.807]\u001b[A\n",
      "Epoch 87: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 87.76it/s, v_num=167, train_loss=0.848, val_loss=0.809]\u001b[A\n",
      "Epoch 88: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 91.07it/s, v_num=167, train_loss=0.853, val_loss=0.833]\u001b[A\n",
      "Epoch 89: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 90.28it/s, v_num=167, train_loss=0.847, val_loss=0.816]\u001b[A\n",
      "Epoch 90: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 91.19it/s, v_num=167, train_loss=0.848, val_loss=0.805]\u001b[A\n",
      "Epoch 91: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 88.03it/s, v_num=167, train_loss=0.855, val_loss=0.815]\u001b[A\n",
      "Epoch 92: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 79.11it/s, v_num=167, train_loss=0.854, val_loss=0.804]\u001b[A\n",
      "Epoch 93: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 78.35it/s, v_num=167, train_loss=0.853, val_loss=0.814]\u001b[A\n",
      "Epoch 94: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 79.58it/s, v_num=167, train_loss=0.846, val_loss=0.814]\u001b[A\n",
      "Epoch 95: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 80.58it/s, v_num=167, train_loss=0.855, val_loss=0.812]\u001b[A\n",
      "Epoch 96: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 77.69it/s, v_num=167, train_loss=0.854, val_loss=0.803]\u001b[A\n",
      "Epoch 97: 100%|██████████████████████████████████████████████| 232/232 [00:02<00:00, 77.75it/s, v_num=167, train_loss=0.851, val_loss=0.803]\u001b[A\n",
      "Epoch 98: 100%|██████████████████████████████████████████████| 232/232 [00:03<00:00, 76.72it/s, v_num=167, train_loss=0.855, val_loss=0.803]\u001b[A\n",
      "Epoch 99: 100%|██████████████████████████████████████████████| 232/232 [00:03<00:00, 71.86it/s, v_num=167, train_loss=0.849, val_loss=0.800]\u001b[A\n",
      "Epoch 99: 100%|██████████████████████████████████████████████| 232/232 [00:03<00:00, 62.23it/s, v_num=167, train_loss=0.853, val_loss=0.808]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████████████████████████████████████████| 232/232 [00:03<00:00, 61.37it/s, v_num=167, train_loss=0.853, val_loss=0.808]\n",
      "DONE SAVING PRETRAINED MODEL\n",
      "   | Name                                             | Type       | Params | Mode \n",
      "-----------------------------------------------------------------------------------------\n",
      "0  | task_loss_fn                                     | MSELoss    | 0      | train\n",
      "1  | mask_loss_fn                                     | BCELoss    | 0      | train\n",
      "2  | categorical_feature_loss                         | BCELoss    | 0      | train\n",
      "3  | continuous_feature_loss                          | MSELoss    | 0      | train\n",
      "4  | model                                            | DAE        | 294 K  | train\n",
      "5  | model._DAE__encoder                              | MLP        | 139 K  | train\n",
      "6  | model._DAE__encoder.linear_0                     | Linear     | 18.6 K | train\n",
      "7  | model._DAE__encoder.relu_0                       | ReLU       | 0      | train\n",
      "8  | model._DAE__encoder.dropout_0                    | Dropout    | 0      | train\n",
      "9  | model._DAE__encoder.linear_1                     | Linear     | 40.2 K | train\n",
      "10 | model._DAE__encoder.relu_1                       | ReLU       | 0      | train\n",
      "11 | model._DAE__encoder.dropout_1                    | Dropout    | 0      | train\n",
      "12 | model._DAE__encoder.linear_2                     | Linear     | 40.2 K | train\n",
      "13 | model._DAE__encoder.relu_2                       | ReLU       | 0      | train\n",
      "14 | model._DAE__encoder.dropout_2                    | Dropout    | 0      | train\n",
      "15 | model._DAE__encoder.linear_n_layers              | Linear     | 40.2 K | train\n",
      "16 | model.mask_predictor_head                        | MLP        | 27.0 K | train\n",
      "17 | model.mask_predictor_head.linear_0               | Linear     | 18.5 K | train\n",
      "18 | model.mask_predictor_head.relu_0                 | ReLU       | 0      | train\n",
      "19 | model.mask_predictor_head.dropout_0              | Dropout    | 0      | train\n",
      "20 | model.mask_predictor_head.linear_n_layers        | Linear     | 8.6 K  | train\n",
      "21 | model.reconstruction_head                        | MLP        | 27.0 K | train\n",
      "22 | model.reconstruction_head.linear_0               | Linear     | 18.5 K | train\n",
      "23 | model.reconstruction_head.relu_0                 | ReLU       | 0      | train\n",
      "24 | model.reconstruction_head.dropout_0              | Dropout    | 0      | train\n",
      "25 | model.reconstruction_head.linear_n_layers        | Linear     | 8.6 K  | train\n",
      "26 | model.one_layer_prediction_head                  | Sequential | 40.4 K | train\n",
      "27 | model.one_layer_prediction_head.head_linear_hid  | Linear     | 40.2 K | train\n",
      "28 | model.one_layer_prediction_head.head_activation  | ReLU       | 0      | train\n",
      "29 | model.one_layer_prediction_head.head_linear_out  | Linear     | 201    | train\n",
      "30 | model.two_layer_prediction_head                  | Sequential | 60.4 K | train\n",
      "31 | model.two_layer_prediction_head.head_linear_hid1 | Linear     | 40.2 K | train\n",
      "32 | model.two_layer_prediction_head.head_activation1 | ReLU       | 0      | train\n",
      "33 | model.two_layer_prediction_head.head_linear_hid2 | Linear     | 20.1 K | train\n",
      "34 | model.two_layer_prediction_head.head_activation2 | ReLU       | 0      | train\n",
      "35 | model.two_layer_prediction_head.head_linear_out  | Linear     | 101    | train\n",
      "-----------------------------------------------------------------------------------------\n",
      "294 K     Trainable params\n",
      "0         Non-trainable params\n",
      "294 K     Total params\n",
      "1.176     Total estimated model params size (MB)\n",
      "DONE PRETRAINING\n"
     ]
    }
   ],
   "source": [
    "s3l_training(pretrain=True, # if True First Phase training\n",
    "             use_pretrained_model=False, # if True Second Phase learning\n",
    "             pt_type='dae', \n",
    "             pt_folder='FP4_only_traffic_rows_no_DO_BN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7160f9-4db4-4688-9556-ba884a04eafb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
