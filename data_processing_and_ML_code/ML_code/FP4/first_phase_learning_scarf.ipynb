{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ed58de6-5ac0-4bbb-95e7-c77e64dfaf11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# reload\n",
    "#%reset\n",
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "\n",
    "import warnings\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# DEBUG MODE\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.getLogger('tensorflow').setLevel(logging.FATAL)\n",
    "    \n",
    "from s3l_training import s3l_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26eb320c-c17f-40dd-8264-6dcbd0933f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run with random seed:  561\n",
      "GPU is available.\n",
      "1\n",
      "True\n",
      "12.1\n",
      "Concatenating runs:  range(1, 11)\n",
      "Time to read csv file for run:  4.425562620162964\n",
      "Loaded run 1\n",
      "Time to read csv file for run:  4.292404651641846\n",
      "Loaded run 2\n",
      "Time to read csv file for run:  4.325503826141357\n",
      "Loaded run 3\n",
      "Time to read csv file for run:  4.3422935009002686\n",
      "Loaded run 4\n",
      "Time to read csv file for run:  4.24535608291626\n",
      "Loaded run 5\n",
      "Time to read csv file for run:  4.285802841186523\n",
      "Loaded run 6\n",
      "Time to read csv file for run:  4.003360986709595\n",
      "Loaded run 7\n",
      "Time to read csv file for run:  4.063351392745972\n",
      "Loaded run 8\n",
      "Time to read csv file for run:  3.993734836578369\n",
      "Loaded run 9\n",
      "Time to read csv file for run:  4.098836183547974\n",
      "Loaded run 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrain_data, before removing rows that dont have traffic  (1799100, 102)\n",
      "pretrain_data, after removing rows that dont have traffic  (33012, 102)\n",
      "X_pretrain  (32691, 92)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "refresh rate:  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type       | Params | Mode \n",
      "--------------------------------------------------------\n",
      "0 | task_loss_fn     | MSELoss    | 0      | train\n",
      "1 | contrastive_loss | NTXentLoss | 0      | train\n",
      "2 | model            | SCARF      | 320 K  | train\n",
      "--------------------------------------------------------\n",
      "320 K     Trainable params\n",
      "0         Non-trainable params\n",
      "320 K     Total params\n",
      "1.282     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|█████████████████████████████████████████████████████████████████████████████████| 232/232 [00:04<00:00, 57.44it/s, v_num=171]\n",
      "Epoch 1: 100%|███████████████████████████████████████████████| 232/232 [00:04<00:00, 56.75it/s, v_num=171, train_loss=2.380, val_loss=1.720]\u001b[A\n",
      "Epoch 2: 100%|███████████████████████████████████████████████| 232/232 [00:04<00:00, 56.77it/s, v_num=171, train_loss=1.580, val_loss=1.410]\u001b[A\n",
      "Epoch 3: 100%|███████████████████████████████████████████████| 232/232 [00:04<00:00, 55.77it/s, v_num=171, train_loss=1.310, val_loss=1.220]\u001b[A\n",
      "Epoch 4: 100%|███████████████████████████████████████████████| 232/232 [00:03<00:00, 59.47it/s, v_num=171, train_loss=1.170, val_loss=1.090]\u001b[A\n",
      "Epoch 5: 100%|███████████████████████████████████████████████| 232/232 [00:04<00:00, 55.50it/s, v_num=171, train_loss=1.070, val_loss=1.010]\u001b[A\n",
      "Epoch 6: 100%|███████████████████████████████████████████████| 232/232 [00:04<00:00, 54.65it/s, v_num=171, train_loss=0.993, val_loss=0.949]\u001b[A\n",
      "Epoch 7: 100%|███████████████████████████████████████████████| 232/232 [00:03<00:00, 58.37it/s, v_num=171, train_loss=0.946, val_loss=0.927]\u001b[A\n",
      "Epoch 8: 100%|███████████████████████████████████████████████| 232/232 [00:04<00:00, 57.53it/s, v_num=171, train_loss=0.895, val_loss=0.872]\u001b[A\n",
      "Epoch 9: 100%|███████████████████████████████████████████████| 232/232 [00:04<00:00, 57.13it/s, v_num=171, train_loss=0.863, val_loss=0.845]\u001b[A\n",
      "Epoch 10: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 56.29it/s, v_num=171, train_loss=0.843, val_loss=0.807]\u001b[A\n",
      "Epoch 11: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 56.13it/s, v_num=171, train_loss=0.823, val_loss=0.817]\u001b[A\n",
      "Epoch 12: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 56.61it/s, v_num=171, train_loss=0.803, val_loss=0.785]\u001b[A\n",
      "Epoch 13: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 56.25it/s, v_num=171, train_loss=0.780, val_loss=0.758]\u001b[A\n",
      "Epoch 14: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 54.97it/s, v_num=171, train_loss=0.766, val_loss=0.742]\u001b[A\n",
      "Epoch 15: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 53.83it/s, v_num=171, train_loss=0.751, val_loss=0.775]\u001b[A\n",
      "Epoch 16: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 53.54it/s, v_num=171, train_loss=0.749, val_loss=0.750]\u001b[A\n",
      "Epoch 17: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 54.69it/s, v_num=171, train_loss=0.729, val_loss=0.708]\u001b[A\n",
      "Epoch 18: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 54.67it/s, v_num=171, train_loss=0.731, val_loss=0.706]\u001b[A\n",
      "Epoch 19: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 52.87it/s, v_num=171, train_loss=0.714, val_loss=0.695]\u001b[A\n",
      "Epoch 20: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 54.20it/s, v_num=171, train_loss=0.707, val_loss=0.707]\u001b[A\n",
      "Epoch 21: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 54.29it/s, v_num=171, train_loss=0.695, val_loss=0.693]\u001b[A\n",
      "Epoch 22: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 55.98it/s, v_num=171, train_loss=0.693, val_loss=0.657]\u001b[A\n",
      "Epoch 23: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 54.83it/s, v_num=171, train_loss=0.688, val_loss=0.678]\u001b[A\n",
      "Epoch 24: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 53.08it/s, v_num=171, train_loss=0.687, val_loss=0.651]\u001b[A\n",
      "Epoch 25: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 51.26it/s, v_num=171, train_loss=0.679, val_loss=0.666]\u001b[A\n",
      "Epoch 26: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 54.62it/s, v_num=171, train_loss=0.664, val_loss=0.653]\u001b[A\n",
      "Epoch 27: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 52.65it/s, v_num=171, train_loss=0.663, val_loss=0.644]\u001b[A\n",
      "Epoch 28: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 55.61it/s, v_num=171, train_loss=0.660, val_loss=0.642]\u001b[A\n",
      "Epoch 29: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 54.39it/s, v_num=171, train_loss=0.650, val_loss=0.653]\u001b[A\n",
      "Epoch 30: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 54.92it/s, v_num=171, train_loss=0.651, val_loss=0.629]\u001b[A\n",
      "Epoch 31: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 55.82it/s, v_num=171, train_loss=0.637, val_loss=0.633]\u001b[A\n",
      "Epoch 32: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 52.71it/s, v_num=171, train_loss=0.629, val_loss=0.628]\u001b[A\n",
      "Epoch 33: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 52.25it/s, v_num=171, train_loss=0.644, val_loss=0.631]\u001b[A\n",
      "Epoch 34: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 53.83it/s, v_num=171, train_loss=0.632, val_loss=0.609]\u001b[A\n",
      "Epoch 35: 100%|██████████████████████████████████████████████| 232/232 [00:03<00:00, 58.04it/s, v_num=171, train_loss=0.629, val_loss=0.604]\u001b[A\n",
      "Epoch 36: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 56.71it/s, v_num=171, train_loss=0.627, val_loss=0.605]\u001b[A\n",
      "Epoch 37: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 52.68it/s, v_num=171, train_loss=0.620, val_loss=0.599]\u001b[A\n",
      "Epoch 38: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 53.96it/s, v_num=171, train_loss=0.617, val_loss=0.612]\u001b[A\n",
      "Epoch 39: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 55.02it/s, v_num=171, train_loss=0.616, val_loss=0.614]\u001b[A\n",
      "Epoch 40: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 54.78it/s, v_num=171, train_loss=0.610, val_loss=0.599]\u001b[A\n",
      "Epoch 41: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 53.34it/s, v_num=171, train_loss=0.614, val_loss=0.588]\u001b[A\n",
      "Epoch 42: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 53.97it/s, v_num=171, train_loss=0.601, val_loss=0.610]\u001b[A\n",
      "Epoch 43: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 55.06it/s, v_num=171, train_loss=0.602, val_loss=0.601]\u001b[A\n",
      "Epoch 44: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 52.96it/s, v_num=171, train_loss=0.596, val_loss=0.586]\u001b[A\n",
      "Epoch 45: 100%|██████████████████████████████████████████████| 232/232 [00:03<00:00, 60.09it/s, v_num=171, train_loss=0.597, val_loss=0.591]\u001b[A\n",
      "Epoch 46: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 52.06it/s, v_num=171, train_loss=0.589, val_loss=0.568]\u001b[A\n",
      "Epoch 47: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 50.18it/s, v_num=171, train_loss=0.588, val_loss=0.571]\u001b[A\n",
      "Epoch 48: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 48.82it/s, v_num=171, train_loss=0.592, val_loss=0.575]\u001b[A\n",
      "Epoch 49: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 47.38it/s, v_num=171, train_loss=0.586, val_loss=0.567]\u001b[A\n",
      "Epoch 50: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 51.74it/s, v_num=171, train_loss=0.580, val_loss=0.569]\u001b[A\n",
      "Epoch 51: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 50.18it/s, v_num=171, train_loss=0.577, val_loss=0.558]\u001b[A\n",
      "Epoch 52: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 49.98it/s, v_num=171, train_loss=0.584, val_loss=0.571]\u001b[A\n",
      "Epoch 53: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 49.81it/s, v_num=171, train_loss=0.565, val_loss=0.570]\u001b[A\n",
      "Epoch 54: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 47.26it/s, v_num=171, train_loss=0.572, val_loss=0.557]\u001b[A\n",
      "Epoch 55: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 47.91it/s, v_num=171, train_loss=0.575, val_loss=0.564]\u001b[A\n",
      "Epoch 56: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 52.19it/s, v_num=171, train_loss=0.574, val_loss=0.558]\u001b[A\n",
      "Epoch 57: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 51.79it/s, v_num=171, train_loss=0.569, val_loss=0.558]\u001b[A\n",
      "Epoch 58: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 50.15it/s, v_num=171, train_loss=0.563, val_loss=0.557]\u001b[A\n",
      "Epoch 59: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 49.41it/s, v_num=171, train_loss=0.564, val_loss=0.553]\u001b[A\n",
      "Epoch 60: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 52.00it/s, v_num=171, train_loss=0.556, val_loss=0.560]\u001b[A\n",
      "Epoch 61: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 49.59it/s, v_num=171, train_loss=0.557, val_loss=0.546]\u001b[A\n",
      "Epoch 62: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 50.50it/s, v_num=171, train_loss=0.555, val_loss=0.555]\u001b[A\n",
      "Epoch 63: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 52.07it/s, v_num=171, train_loss=0.546, val_loss=0.541]\u001b[A\n",
      "Epoch 64: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 49.46it/s, v_num=171, train_loss=0.550, val_loss=0.531]\u001b[A\n",
      "Epoch 65: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 54.89it/s, v_num=171, train_loss=0.548, val_loss=0.552]\u001b[A\n",
      "Epoch 66: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 49.76it/s, v_num=171, train_loss=0.549, val_loss=0.533]\u001b[A\n",
      "Epoch 67: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 54.97it/s, v_num=171, train_loss=0.537, val_loss=0.538]\u001b[A\n",
      "Epoch 68: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 55.35it/s, v_num=171, train_loss=0.535, val_loss=0.532]\u001b[A\n",
      "Epoch 69: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 55.81it/s, v_num=171, train_loss=0.530, val_loss=0.535]\u001b[A\n",
      "Epoch 70: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 54.97it/s, v_num=171, train_loss=0.535, val_loss=0.533]\u001b[A\n",
      "Epoch 71: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 52.02it/s, v_num=171, train_loss=0.531, val_loss=0.513]\u001b[A\n",
      "Epoch 72: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 50.00it/s, v_num=171, train_loss=0.535, val_loss=0.531]\u001b[A\n",
      "Epoch 73: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 51.04it/s, v_num=171, train_loss=0.527, val_loss=0.519]\u001b[A\n",
      "Epoch 74: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 52.16it/s, v_num=171, train_loss=0.532, val_loss=0.516]\u001b[A\n",
      "Epoch 75: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 52.72it/s, v_num=171, train_loss=0.524, val_loss=0.524]\u001b[A\n",
      "Epoch 76: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 51.37it/s, v_num=171, train_loss=0.524, val_loss=0.522]\u001b[A\n",
      "Epoch 77: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 48.33it/s, v_num=171, train_loss=0.521, val_loss=0.515]\u001b[A\n",
      "Epoch 78: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 52.23it/s, v_num=171, train_loss=0.517, val_loss=0.516]\u001b[A\n",
      "Epoch 79: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 49.68it/s, v_num=171, train_loss=0.516, val_loss=0.509]\u001b[A\n",
      "Epoch 80: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 54.36it/s, v_num=171, train_loss=0.511, val_loss=0.500]\u001b[A\n",
      "Epoch 81: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 49.06it/s, v_num=171, train_loss=0.507, val_loss=0.507]\u001b[A\n",
      "Epoch 82: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 50.21it/s, v_num=171, train_loss=0.509, val_loss=0.518]\u001b[A\n",
      "Epoch 83: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 50.05it/s, v_num=171, train_loss=0.508, val_loss=0.509]\u001b[A\n",
      "Epoch 84: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 46.72it/s, v_num=171, train_loss=0.505, val_loss=0.486]\u001b[A\n",
      "Epoch 85: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 46.91it/s, v_num=171, train_loss=0.497, val_loss=0.515]\u001b[A\n",
      "Epoch 86: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 48.83it/s, v_num=171, train_loss=0.498, val_loss=0.485]\u001b[A\n",
      "Epoch 87: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 49.70it/s, v_num=171, train_loss=0.499, val_loss=0.510]\u001b[A\n",
      "Epoch 88: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 48.32it/s, v_num=171, train_loss=0.501, val_loss=0.488]\u001b[A\n",
      "Epoch 89: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 49.41it/s, v_num=171, train_loss=0.496, val_loss=0.491]\u001b[A\n",
      "Epoch 90: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 50.46it/s, v_num=171, train_loss=0.495, val_loss=0.501]\u001b[A\n",
      "Epoch 91: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 48.66it/s, v_num=171, train_loss=0.494, val_loss=0.470]\u001b[A\n",
      "Epoch 92: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 52.00it/s, v_num=171, train_loss=0.496, val_loss=0.484]\u001b[A\n",
      "Epoch 93: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 49.44it/s, v_num=171, train_loss=0.489, val_loss=0.499]\u001b[A\n",
      "Epoch 94: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 47.06it/s, v_num=171, train_loss=0.486, val_loss=0.484]\u001b[A\n",
      "Epoch 95: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 51.20it/s, v_num=171, train_loss=0.484, val_loss=0.477]\u001b[A\n",
      "Epoch 96: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 50.08it/s, v_num=171, train_loss=0.484, val_loss=0.504]\u001b[A\n",
      "Epoch 97: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 50.93it/s, v_num=171, train_loss=0.480, val_loss=0.483]\u001b[A\n",
      "Epoch 98: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 49.36it/s, v_num=171, train_loss=0.481, val_loss=0.464]\u001b[A\n",
      "Epoch 99: 100%|██████████████████████████████████████████████| 232/232 [00:04<00:00, 48.68it/s, v_num=171, train_loss=0.476, val_loss=0.467]\u001b[A\n",
      "Epoch 99: 100%|██████████████████████████████████████████████| 232/232 [00:05<00:00, 42.93it/s, v_num=171, train_loss=0.475, val_loss=0.474]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████████████████████████████████████████| 232/232 [00:05<00:00, 42.62it/s, v_num=171, train_loss=0.475, val_loss=0.474]\n",
      "DONE SAVING PRETRAINED MODEL\n",
      "   | Name                                             | Type       | Params | Mode \n",
      "-----------------------------------------------------------------------------------------\n",
      "0  | task_loss_fn                                     | MSELoss    | 0      | train\n",
      "1  | contrastive_loss                                 | NTXentLoss | 0      | train\n",
      "2  | model                                            | SCARF      | 320 K  | train\n",
      "3  | model._SCARF__encoder                            | MLP        | 139 K  | train\n",
      "4  | model._SCARF__encoder.linear_0                   | Linear     | 18.6 K | train\n",
      "5  | model._SCARF__encoder.relu_0                     | ReLU       | 0      | train\n",
      "6  | model._SCARF__encoder.dropout_0                  | Dropout    | 0      | train\n",
      "7  | model._SCARF__encoder.linear_1                   | Linear     | 40.2 K | train\n",
      "8  | model._SCARF__encoder.relu_1                     | ReLU       | 0      | train\n",
      "9  | model._SCARF__encoder.dropout_1                  | Dropout    | 0      | train\n",
      "10 | model._SCARF__encoder.linear_2                   | Linear     | 40.2 K | train\n",
      "11 | model._SCARF__encoder.relu_2                     | ReLU       | 0      | train\n",
      "12 | model._SCARF__encoder.dropout_2                  | Dropout    | 0      | train\n",
      "13 | model._SCARF__encoder.linear_n_layers            | Linear     | 40.2 K | train\n",
      "14 | model.pretraining_head                           | MLP        | 80.4 K | train\n",
      "15 | model.pretraining_head.linear_0                  | Linear     | 40.2 K | train\n",
      "16 | model.pretraining_head.relu_0                    | ReLU       | 0      | train\n",
      "17 | model.pretraining_head.dropout_0                 | Dropout    | 0      | train\n",
      "18 | model.pretraining_head.linear_n_layers           | Linear     | 40.2 K | train\n",
      "19 | model.one_layer_prediction_head                  | Sequential | 40.4 K | train\n",
      "20 | model.one_layer_prediction_head.head_linear_hid  | Linear     | 40.2 K | train\n",
      "21 | model.one_layer_prediction_head.head_activation  | ReLU       | 0      | train\n",
      "22 | model.one_layer_prediction_head.head_linear_out  | Linear     | 201    | train\n",
      "23 | model.two_layer_prediction_head                  | Sequential | 60.4 K | train\n",
      "24 | model.two_layer_prediction_head.head_linear_hid1 | Linear     | 40.2 K | train\n",
      "25 | model.two_layer_prediction_head.head_activation1 | ReLU       | 0      | train\n",
      "26 | model.two_layer_prediction_head.head_linear_hid2 | Linear     | 20.1 K | train\n",
      "27 | model.two_layer_prediction_head.head_activation2 | ReLU       | 0      | train\n",
      "28 | model.two_layer_prediction_head.head_linear_out  | Linear     | 101    | train\n",
      "-----------------------------------------------------------------------------------------\n",
      "320 K     Trainable params\n",
      "0         Non-trainable params\n",
      "320 K     Total params\n",
      "1.282     Total estimated model params size (MB)\n",
      "DONE PRETRAINING\n"
     ]
    }
   ],
   "source": [
    "s3l_training(pretrain=True, # if True First Phase training\n",
    "             use_pretrained_model=False, # if True Second Phase learning\n",
    "             pt_type='scarf', \n",
    "             pt_folder='FP4_only_traffic_rows_no_DO_BN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7160f9-4db4-4688-9556-ba884a04eafb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
