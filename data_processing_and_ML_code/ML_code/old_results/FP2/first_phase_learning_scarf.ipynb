{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ed58de6-5ac0-4bbb-95e7-c77e64dfaf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload\n",
    "#%reset\n",
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "\n",
    "import warnings\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# DEBUG MODE\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.getLogger('tensorflow').setLevel(logging.FATAL)\n",
    "    \n",
    "from s3l_training import s3l_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26eb320c-c17f-40dd-8264-6dcbd0933f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run with random seed:  561\n",
      "GPU is available.\n",
      "1\n",
      "True\n",
      "12.1\n",
      "Concatenating runs:  range(1, 11)\n",
      "Time to read csv file for run:  4.354745388031006\n",
      "Loaded run 1\n",
      "Time to read csv file for run:  4.433773994445801\n",
      "Loaded run 2\n",
      "Time to read csv file for run:  4.471669435501099\n",
      "Loaded run 3\n",
      "Time to read csv file for run:  4.3256330490112305\n",
      "Loaded run 4\n",
      "Time to read csv file for run:  4.497611045837402\n",
      "Loaded run 5\n",
      "Time to read csv file for run:  4.465775728225708\n",
      "Loaded run 6\n",
      "Time to read csv file for run:  4.32793402671814\n",
      "Loaded run 7\n",
      "Time to read csv file for run:  4.582399606704712\n",
      "Loaded run 8\n",
      "Time to read csv file for run:  4.195291519165039\n",
      "Loaded run 9\n",
      "Time to read csv file for run:  4.275113344192505\n",
      "Loaded run 10\n",
      "cell_conn_type\n",
      "[0 1]\n",
      "categorical_cols:  ['cell_conn_type']\n",
      "(1796746, 92)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1621562, 92)\n",
      "(89838, 92)\n",
      "(85346, 92)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "refresh rate:  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type       | Params | Mode \n",
      "--------------------------------------------------------\n",
      "0 | task_loss_fn     | MSELoss    | 0      | train\n",
      "1 | contrastive_loss | NTXentLoss | 0      | train\n",
      "2 | model            | SCARF      | 322 K  | train\n",
      "--------------------------------------------------------\n",
      "322 K     Trainable params\n",
      "0         Non-trainable params\n",
      "322 K     Total params\n",
      "1.288     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|████████████████████████████████████████████████████████████████████████████| 13371/13371 [05:31<00:00, 40.30it/s, v_num=1273]\n",
      "Epoch 1: 100%|██████████████████████████████████████████| 13371/13371 [05:39<00:00, 39.34it/s, v_num=1273, train_loss=0.677, val_loss=0.897]\u001b[A\n",
      "Epoch 2: 100%|██████████████████████████████████████████| 13371/13371 [05:46<00:00, 38.57it/s, v_num=1273, train_loss=0.461, val_loss=0.717]\u001b[A\n",
      "Epoch 3: 100%|██████████████████████████████████████████| 13371/13371 [05:45<00:00, 38.71it/s, v_num=1273, train_loss=0.403, val_loss=0.573]\u001b[A\n",
      "Epoch 4: 100%|██████████████████████████████████████████| 13371/13371 [05:47<00:00, 38.52it/s, v_num=1273, train_loss=0.354, val_loss=0.474]\u001b[A\n",
      "Epoch 5: 100%|██████████████████████████████████████████| 13371/13371 [05:47<00:00, 38.46it/s, v_num=1273, train_loss=0.312, val_loss=0.405]\u001b[A\n",
      "Epoch 6: 100%|██████████████████████████████████████████| 13371/13371 [05:41<00:00, 39.15it/s, v_num=1273, train_loss=0.276, val_loss=0.361]\u001b[A\n",
      "Epoch 7: 100%|██████████████████████████████████████████| 13371/13371 [05:56<00:00, 37.48it/s, v_num=1273, train_loss=0.246, val_loss=0.311]\u001b[A\n",
      "Epoch 8: 100%|██████████████████████████████████████████| 13371/13371 [05:50<00:00, 38.19it/s, v_num=1273, train_loss=0.220, val_loss=0.281]\u001b[A\n",
      "Epoch 9: 100%|██████████████████████████████████████████| 13371/13371 [05:44<00:00, 38.81it/s, v_num=1273, train_loss=0.201, val_loss=0.256]\u001b[A\n",
      "Epoch 10: 100%|█████████████████████████████████████████| 13371/13371 [05:56<00:00, 37.49it/s, v_num=1273, train_loss=0.186, val_loss=0.236]\u001b[A\n",
      "Epoch 11: 100%|█████████████████████████████████████████| 13371/13371 [05:53<00:00, 37.78it/s, v_num=1273, train_loss=0.173, val_loss=0.220]\u001b[A\n",
      "Epoch 12: 100%|█████████████████████████████████████████| 13371/13371 [05:46<00:00, 38.59it/s, v_num=1273, train_loss=0.162, val_loss=0.208]\u001b[A\n",
      "Epoch 13: 100%|█████████████████████████████████████████| 13371/13371 [05:53<00:00, 37.83it/s, v_num=1273, train_loss=0.152, val_loss=0.194]\u001b[A\n",
      "Epoch 14: 100%|█████████████████████████████████████████| 13371/13371 [05:54<00:00, 37.76it/s, v_num=1273, train_loss=0.144, val_loss=0.185]\u001b[A\n",
      "Epoch 15: 100%|█████████████████████████████████████████| 13371/13371 [05:45<00:00, 38.68it/s, v_num=1273, train_loss=0.137, val_loss=0.178]\u001b[A\n",
      "Epoch 16: 100%|█████████████████████████████████████████| 13371/13371 [05:56<00:00, 37.49it/s, v_num=1273, train_loss=0.131, val_loss=0.169]\u001b[A\n",
      "Epoch 17: 100%|█████████████████████████████████████████| 13371/13371 [05:55<00:00, 37.58it/s, v_num=1273, train_loss=0.126, val_loss=0.163]\u001b[A\n",
      "Epoch 18: 100%|█████████████████████████████████████████| 13371/13371 [05:52<00:00, 37.95it/s, v_num=1273, train_loss=0.121, val_loss=0.159]\u001b[A\n",
      "Epoch 19: 100%|█████████████████████████████████████████| 13371/13371 [05:46<00:00, 38.56it/s, v_num=1273, train_loss=0.117, val_loss=0.151]\u001b[A\n",
      "Epoch 20: 100%|█████████████████████████████████████████| 13371/13371 [05:57<00:00, 37.37it/s, v_num=1273, train_loss=0.113, val_loss=0.145]\u001b[A\n",
      "Epoch 21: 100%|█████████████████████████████████████████| 13371/13371 [05:56<00:00, 37.50it/s, v_num=1273, train_loss=0.109, val_loss=0.142]\u001b[A\n",
      "Epoch 22: 100%|█████████████████████████████████████████| 13371/13371 [05:53<00:00, 37.82it/s, v_num=1273, train_loss=0.106, val_loss=0.138]\u001b[A\n",
      "Epoch 23: 100%|█████████████████████████████████████████| 13371/13371 [05:48<00:00, 38.41it/s, v_num=1273, train_loss=0.104, val_loss=0.132]\u001b[A\n",
      "Epoch 24: 100%|█████████████████████████████████████████| 13371/13371 [05:54<00:00, 37.72it/s, v_num=1273, train_loss=0.101, val_loss=0.129]\u001b[A\n",
      "Epoch 25: 100%|████████████████████████████████████████| 13371/13371 [05:57<00:00, 37.45it/s, v_num=1273, train_loss=0.0986, val_loss=0.128]\u001b[A\n",
      "Epoch 26: 100%|████████████████████████████████████████| 13371/13371 [05:57<00:00, 37.43it/s, v_num=1273, train_loss=0.0968, val_loss=0.123]\u001b[A\n",
      "Epoch 27: 100%|████████████████████████████████████████| 13371/13371 [05:49<00:00, 38.27it/s, v_num=1273, train_loss=0.0949, val_loss=0.124]\u001b[A\n",
      "Epoch 28: 100%|████████████████████████████████████████| 13371/13371 [05:53<00:00, 37.78it/s, v_num=1273, train_loss=0.0933, val_loss=0.119]\u001b[A\n",
      "Epoch 29: 100%|████████████████████████████████████████| 13371/13371 [05:55<00:00, 37.60it/s, v_num=1273, train_loss=0.0921, val_loss=0.116]\u001b[A\n",
      "Epoch 30: 100%|████████████████████████████████████████| 13371/13371 [05:57<00:00, 37.44it/s, v_num=1273, train_loss=0.0904, val_loss=0.117]\u001b[A\n",
      "Epoch 31: 100%|████████████████████████████████████████| 13371/13371 [05:55<00:00, 37.59it/s, v_num=1273, train_loss=0.0893, val_loss=0.113]\u001b[A\n",
      "Epoch 32: 100%|████████████████████████████████████████| 13371/13371 [05:50<00:00, 38.17it/s, v_num=1273, train_loss=0.0881, val_loss=0.114]\u001b[A\n",
      "Epoch 33: 100%|█████████████████████████████████████████| 13371/13371 [05:50<00:00, 38.10it/s, v_num=1273, train_loss=0.087, val_loss=0.111]\u001b[A\n",
      "Epoch 34: 100%|█████████████████████████████████████████| 13371/13371 [05:57<00:00, 37.43it/s, v_num=1273, train_loss=0.086, val_loss=0.110]\u001b[A\n",
      "Epoch 35: 100%|████████████████████████████████████████| 13371/13371 [05:55<00:00, 37.63it/s, v_num=1273, train_loss=0.0852, val_loss=0.109]\u001b[A\n",
      "Epoch 36: 100%|████████████████████████████████████████| 13371/13371 [05:56<00:00, 37.54it/s, v_num=1273, train_loss=0.0842, val_loss=0.109]\u001b[A\n",
      "Epoch 37: 100%|████████████████████████████████████████| 13371/13371 [05:54<00:00, 37.69it/s, v_num=1273, train_loss=0.0835, val_loss=0.107]\u001b[A\n",
      "Epoch 38: 100%|████████████████████████████████████████| 13371/13371 [05:53<00:00, 37.82it/s, v_num=1273, train_loss=0.0825, val_loss=0.107]\u001b[A\n",
      "Epoch 39: 100%|████████████████████████████████████████| 13371/13371 [05:51<00:00, 38.02it/s, v_num=1273, train_loss=0.0819, val_loss=0.104]\u001b[A\n",
      "Epoch 40: 100%|████████████████████████████████████████| 13371/13371 [05:55<00:00, 37.66it/s, v_num=1273, train_loss=0.0813, val_loss=0.104]\u001b[A\n",
      "Epoch 41: 100%|████████████████████████████████████████| 13371/13371 [05:56<00:00, 37.49it/s, v_num=1273, train_loss=0.0807, val_loss=0.106]\u001b[A\n",
      "Epoch 42: 100%|█████████████████████████████████████████| 13371/13371 [05:54<00:00, 37.67it/s, v_num=1273, train_loss=0.080, val_loss=0.103]\u001b[A\n",
      "Epoch 43: 100%|████████████████████████████████████████| 13371/13371 [05:48<00:00, 38.32it/s, v_num=1273, train_loss=0.0793, val_loss=0.101]\u001b[A\n",
      "Epoch 44: 100%|████████████████████████████████████████| 13371/13371 [05:37<00:00, 39.67it/s, v_num=1273, train_loss=0.0787, val_loss=0.103]\u001b[A\n",
      "Epoch 45: 100%|████████████████████████████████████████| 13371/13371 [05:25<00:00, 41.07it/s, v_num=1273, train_loss=0.0781, val_loss=0.102]\u001b[A\n",
      "Epoch 46: 100%|████████████████████████████████████████| 13371/13371 [05:25<00:00, 41.10it/s, v_num=1273, train_loss=0.0775, val_loss=0.103]\u001b[A\n",
      "Epoch 47: 100%|███████████████████████████████████████| 13371/13371 [05:26<00:00, 41.00it/s, v_num=1273, train_loss=0.0766, val_loss=0.0978]\u001b[A\n",
      "Epoch 48: 100%|████████████████████████████████████████| 13371/13371 [05:26<00:00, 40.99it/s, v_num=1273, train_loss=0.076, val_loss=0.0984]\u001b[A\n",
      "Epoch 49: 100%|███████████████████████████████████████| 13371/13371 [05:25<00:00, 41.03it/s, v_num=1273, train_loss=0.0752, val_loss=0.0975]\u001b[A\n",
      "Epoch 50: 100%|███████████████████████████████████████| 13371/13371 [05:25<00:00, 41.04it/s, v_num=1273, train_loss=0.0748, val_loss=0.0959]\u001b[A\n",
      "Epoch 51: 100%|███████████████████████████████████████| 13371/13371 [05:26<00:00, 41.01it/s, v_num=1273, train_loss=0.0742, val_loss=0.0942]\u001b[A\n",
      "Epoch 52: 100%|███████████████████████████████████████| 13371/13371 [05:25<00:00, 41.07it/s, v_num=1273, train_loss=0.0738, val_loss=0.0946]\u001b[A\n",
      "Epoch 53: 100%|███████████████████████████████████████| 13371/13371 [05:26<00:00, 41.00it/s, v_num=1273, train_loss=0.0733, val_loss=0.0946]\u001b[A\n",
      "Epoch 54: 100%|███████████████████████████████████████| 13371/13371 [05:26<00:00, 40.97it/s, v_num=1273, train_loss=0.0729, val_loss=0.0965]\u001b[A\n",
      "Epoch 55: 100%|███████████████████████████████████████| 13371/13371 [05:26<00:00, 41.01it/s, v_num=1273, train_loss=0.0725, val_loss=0.0945]\u001b[A\n",
      "Epoch 56: 100%|████████████████████████████████████████| 13371/13371 [05:26<00:00, 41.00it/s, v_num=1273, train_loss=0.072, val_loss=0.0919]\u001b[A\n",
      "Epoch 57: 100%|███████████████████████████████████████| 13371/13371 [05:25<00:00, 41.09it/s, v_num=1273, train_loss=0.0719, val_loss=0.0929]\u001b[A\n",
      "Epoch 58: 100%|███████████████████████████████████████| 13371/13371 [05:26<00:00, 40.95it/s, v_num=1273, train_loss=0.0714, val_loss=0.0939]\u001b[A\n",
      "Epoch 59: 100%|████████████████████████████████████████| 13371/13371 [05:25<00:00, 41.02it/s, v_num=1273, train_loss=0.0711, val_loss=0.092]\u001b[A\n",
      "Epoch 60: 100%|███████████████████████████████████████| 13371/13371 [05:26<00:00, 41.01it/s, v_num=1273, train_loss=0.0709, val_loss=0.0924]\u001b[A\n",
      "Epoch 61: 100%|███████████████████████████████████████| 13371/13371 [05:26<00:00, 40.95it/s, v_num=1273, train_loss=0.0706, val_loss=0.0943]\u001b[A\n",
      "Epoch 62: 100%|███████████████████████████████████████| 13371/13371 [05:15<00:00, 42.35it/s, v_num=1273, train_loss=0.0703, val_loss=0.0928]\u001b[A\n",
      "Epoch 63: 100%|███████████████████████████████████████| 13371/13371 [05:10<00:00, 43.12it/s, v_num=1273, train_loss=0.0699, val_loss=0.0935]\u001b[A\n",
      "Epoch 64: 100%|███████████████████████████████████████| 13371/13371 [05:12<00:00, 42.86it/s, v_num=1273, train_loss=0.0697, val_loss=0.0916]\u001b[A\n",
      "Epoch 65: 100%|███████████████████████████████████████| 13371/13371 [05:12<00:00, 42.75it/s, v_num=1273, train_loss=0.0694, val_loss=0.0897]\u001b[A\n",
      "Epoch 66: 100%|███████████████████████████████████████| 13371/13371 [05:11<00:00, 42.90it/s, v_num=1273, train_loss=0.0693, val_loss=0.0902]\u001b[A\n",
      "Epoch 67: 100%|████████████████████████████████████████| 13371/13371 [05:12<00:00, 42.85it/s, v_num=1273, train_loss=0.069, val_loss=0.0887]\u001b[A\n",
      "Epoch 68: 100%|███████████████████████████████████████| 13371/13371 [05:11<00:00, 42.86it/s, v_num=1273, train_loss=0.0686, val_loss=0.0886]\u001b[A\n",
      "Epoch 69: 100%|███████████████████████████████████████| 13371/13371 [05:10<00:00, 43.11it/s, v_num=1273, train_loss=0.0683, val_loss=0.0865]\u001b[A\n",
      "Epoch 70: 100%|███████████████████████████████████████| 13371/13371 [05:11<00:00, 42.86it/s, v_num=1273, train_loss=0.0681, val_loss=0.0881]\u001b[A\n",
      "Epoch 71: 100%|█████████████████████████████████████████| 13371/13371 [05:11<00:00, 42.95it/s, v_num=1273, train_loss=0.068, val_loss=0.090]\u001b[A\n",
      "Epoch 72: 100%|███████████████████████████████████████| 13371/13371 [05:11<00:00, 42.86it/s, v_num=1273, train_loss=0.0677, val_loss=0.0865]\u001b[A\n",
      "Epoch 73: 100%|███████████████████████████████████████| 13371/13371 [05:12<00:00, 42.82it/s, v_num=1273, train_loss=0.0676, val_loss=0.0894]\u001b[A\n",
      "Epoch 74: 100%|███████████████████████████████████████| 13371/13371 [05:11<00:00, 42.86it/s, v_num=1273, train_loss=0.0674, val_loss=0.0866]\u001b[A\n",
      "Epoch 75: 100%|███████████████████████████████████████| 13371/13371 [05:10<00:00, 43.01it/s, v_num=1273, train_loss=0.0672, val_loss=0.0875]\u001b[A\n",
      "Epoch 76: 100%|████████████████████████████████████████| 13371/13371 [05:11<00:00, 42.94it/s, v_num=1273, train_loss=0.0669, val_loss=0.087]\u001b[A\n",
      "Epoch 77: 100%|████████████████████████████████████████| 13371/13371 [05:12<00:00, 42.81it/s, v_num=1273, train_loss=0.0665, val_loss=0.087]\u001b[A\n",
      "Epoch 78: 100%|███████████████████████████████████████| 13371/13371 [05:11<00:00, 42.91it/s, v_num=1273, train_loss=0.0664, val_loss=0.0858]\u001b[A\n",
      "Epoch 79: 100%|███████████████████████████████████████| 13371/13371 [05:12<00:00, 42.85it/s, v_num=1273, train_loss=0.0662, val_loss=0.0867]\u001b[A\n",
      "Epoch 80: 100%|████████████████████████████████████████| 13371/13371 [05:11<00:00, 42.87it/s, v_num=1273, train_loss=0.066, val_loss=0.0842]\u001b[A\n",
      "Epoch 81: 100%|███████████████████████████████████████| 13371/13371 [05:09<00:00, 43.14it/s, v_num=1273, train_loss=0.0658, val_loss=0.0854]\u001b[A\n",
      "Epoch 82: 100%|███████████████████████████████████████| 13371/13371 [05:12<00:00, 42.85it/s, v_num=1273, train_loss=0.0658, val_loss=0.0858]\u001b[A\n",
      "Epoch 83: 100%|███████████████████████████████████████| 13371/13371 [05:11<00:00, 42.90it/s, v_num=1273, train_loss=0.0655, val_loss=0.0826]\u001b[A\n",
      "Epoch 84: 100%|███████████████████████████████████████| 13371/13371 [05:12<00:00, 42.75it/s, v_num=1273, train_loss=0.0653, val_loss=0.0849]\u001b[A\n",
      "Epoch 85: 100%|███████████████████████████████████████| 13371/13371 [05:11<00:00, 42.86it/s, v_num=1273, train_loss=0.0651, val_loss=0.0844]\u001b[A\n",
      "Epoch 86: 100%|███████████████████████████████████████| 13371/13371 [05:12<00:00, 42.83it/s, v_num=1273, train_loss=0.0651, val_loss=0.0833]\u001b[A\n",
      "Epoch 87: 100%|███████████████████████████████████████| 13371/13371 [05:11<00:00, 42.96it/s, v_num=1273, train_loss=0.0648, val_loss=0.0852]\u001b[A\n",
      "Epoch 88: 100%|███████████████████████████████████████| 13371/13371 [05:11<00:00, 42.97it/s, v_num=1273, train_loss=0.0648, val_loss=0.0833]\u001b[A\n",
      "Epoch 89: 100%|███████████████████████████████████████| 13371/13371 [05:12<00:00, 42.76it/s, v_num=1273, train_loss=0.0645, val_loss=0.0841]\u001b[A\n",
      "Epoch 90: 100%|███████████████████████████████████████| 13371/13371 [05:12<00:00, 42.76it/s, v_num=1273, train_loss=0.0646, val_loss=0.0867]\u001b[A\n",
      "Epoch 91: 100%|███████████████████████████████████████| 13371/13371 [05:12<00:00, 42.83it/s, v_num=1273, train_loss=0.0643, val_loss=0.0812]\u001b[A\n",
      "Epoch 92: 100%|███████████████████████████████████████| 13371/13371 [05:10<00:00, 43.06it/s, v_num=1273, train_loss=0.0641, val_loss=0.0818]\u001b[A\n",
      "Epoch 93: 100%|███████████████████████████████████████| 13371/13371 [05:11<00:00, 42.91it/s, v_num=1273, train_loss=0.0642, val_loss=0.0821]\u001b[A\n",
      "Epoch 94: 100%|███████████████████████████████████████| 13371/13371 [05:11<00:00, 42.87it/s, v_num=1273, train_loss=0.0639, val_loss=0.0819]\u001b[A\n",
      "Epoch 95: 100%|████████████████████████████████████████| 13371/13371 [05:11<00:00, 42.90it/s, v_num=1273, train_loss=0.064, val_loss=0.0846]\u001b[A\n",
      "Epoch 96: 100%|███████████████████████████████████████| 13371/13371 [05:11<00:00, 42.93it/s, v_num=1273, train_loss=0.0638, val_loss=0.0823]\u001b[A\n",
      "Epoch 97: 100%|███████████████████████████████████████| 13371/13371 [05:11<00:00, 42.86it/s, v_num=1273, train_loss=0.0636, val_loss=0.0845]\u001b[A\n",
      "Epoch 98: 100%|███████████████████████████████████████| 13371/13371 [05:11<00:00, 42.93it/s, v_num=1273, train_loss=0.0635, val_loss=0.0823]\u001b[A\n",
      "Epoch 99: 100%|███████████████████████████████████████| 13371/13371 [05:12<00:00, 42.83it/s, v_num=1273, train_loss=0.0634, val_loss=0.0807]\u001b[A\n",
      "Epoch 99: 100%|███████████████████████████████████████| 13371/13371 [05:21<00:00, 41.56it/s, v_num=1273, train_loss=0.0633, val_loss=0.0812]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|███████████████████████████████████████| 13371/13371 [05:21<00:00, 41.55it/s, v_num=1273, train_loss=0.0633, val_loss=0.0812]\n",
      "DONE SAVING PRETRAINED MODEL\n",
      "   | Name                                             | Type        | Params | Mode \n",
      "------------------------------------------------------------------------------------------\n",
      "0  | task_loss_fn                                     | MSELoss     | 0      | train\n",
      "1  | contrastive_loss                                 | NTXentLoss  | 0      | train\n",
      "2  | model                                            | SCARF       | 322 K  | train\n",
      "3  | model._SCARF__encoder                            | MLP         | 140 K  | train\n",
      "4  | model._SCARF__encoder.linear_0                   | Linear      | 18.6 K | train\n",
      "5  | model._SCARF__encoder.batchnorm_0                | BatchNorm1d | 400    | train\n",
      "6  | model._SCARF__encoder.relu_0                     | ReLU        | 0      | train\n",
      "7  | model._SCARF__encoder.dropout_0                  | Dropout     | 0      | train\n",
      "8  | model._SCARF__encoder.linear_1                   | Linear      | 40.2 K | train\n",
      "9  | model._SCARF__encoder.batchnorm_1                | BatchNorm1d | 400    | train\n",
      "10 | model._SCARF__encoder.relu_1                     | ReLU        | 0      | train\n",
      "11 | model._SCARF__encoder.dropout_1                  | Dropout     | 0      | train\n",
      "12 | model._SCARF__encoder.linear_2                   | Linear      | 40.2 K | train\n",
      "13 | model._SCARF__encoder.batchnorm_2                | BatchNorm1d | 400    | train\n",
      "14 | model._SCARF__encoder.relu_2                     | ReLU        | 0      | train\n",
      "15 | model._SCARF__encoder.dropout_2                  | Dropout     | 0      | train\n",
      "16 | model._SCARF__encoder.linear_n_layers            | Linear      | 40.2 K | train\n",
      "17 | model.pretraining_head                           | MLP         | 80.8 K | train\n",
      "18 | model.pretraining_head.linear_0                  | Linear      | 40.2 K | train\n",
      "19 | model.pretraining_head.batchnorm_0               | BatchNorm1d | 400    | train\n",
      "20 | model.pretraining_head.relu_0                    | ReLU        | 0      | train\n",
      "21 | model.pretraining_head.dropout_0                 | Dropout     | 0      | train\n",
      "22 | model.pretraining_head.linear_n_layers           | Linear      | 40.2 K | train\n",
      "23 | model.one_layer_prediction_head                  | Sequential  | 40.4 K | train\n",
      "24 | model.one_layer_prediction_head.head_linear_hid  | Linear      | 40.2 K | train\n",
      "25 | model.one_layer_prediction_head.head_activation  | ReLU        | 0      | train\n",
      "26 | model.one_layer_prediction_head.head_linear_out  | Linear      | 201    | train\n",
      "27 | model.two_layer_prediction_head                  | Sequential  | 60.4 K | train\n",
      "28 | model.two_layer_prediction_head.head_linear_hid1 | Linear      | 40.2 K | train\n",
      "29 | model.two_layer_prediction_head.head_activation1 | ReLU        | 0      | train\n",
      "30 | model.two_layer_prediction_head.head_linear_hid2 | Linear      | 20.1 K | train\n",
      "31 | model.two_layer_prediction_head.head_activation2 | ReLU        | 0      | train\n",
      "32 | model.two_layer_prediction_head.head_linear_out  | Linear      | 101    | train\n",
      "------------------------------------------------------------------------------------------\n",
      "322 K     Trainable params\n",
      "0         Non-trainable params\n",
      "322 K     Total params\n",
      "1.288     Total estimated model params size (MB)\n",
      "DONE PRETRAINING\n"
     ]
    }
   ],
   "source": [
    "s3l_training(pretrain=True, # if True First Phase training\n",
    "             use_pretrained_model=False, # if True Second Phase learning\n",
    "             pt_type='scarf', \n",
    "             pt_folder='FP2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7160f9-4db4-4688-9556-ba884a04eafb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
