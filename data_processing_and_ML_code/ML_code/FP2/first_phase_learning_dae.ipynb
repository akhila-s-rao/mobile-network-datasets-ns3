{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ed58de6-5ac0-4bbb-95e7-c77e64dfaf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload\n",
    "#%reset\n",
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "\n",
    "import warnings\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# DEBUG MODE\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.getLogger('tensorflow').setLevel(logging.FATAL)\n",
    "    \n",
    "from s3l_training import s3l_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26eb320c-c17f-40dd-8264-6dcbd0933f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run with random seed:  561\n",
      "GPU is available.\n",
      "1\n",
      "True\n",
      "12.1\n",
      "Concatenating runs:  range(1, 11)\n",
      "Time to read csv file for run:  3.1239678859710693\n",
      "Loaded run 1\n",
      "Time to read csv file for run:  3.0049524307250977\n",
      "Loaded run 2\n",
      "Time to read csv file for run:  3.026463270187378\n",
      "Loaded run 3\n",
      "Time to read csv file for run:  3.018110990524292\n",
      "Loaded run 4\n",
      "Time to read csv file for run:  3.0178911685943604\n",
      "Loaded run 5\n",
      "Time to read csv file for run:  2.7987160682678223\n",
      "Loaded run 6\n",
      "Time to read csv file for run:  3.006895065307617\n",
      "Loaded run 7\n",
      "Time to read csv file for run:  2.935994863510132\n",
      "Loaded run 8\n",
      "Time to read csv file for run:  2.884636402130127\n",
      "Loaded run 9\n",
      "Time to read csv file for run:  2.904096841812134\n",
      "Loaded run 10\n",
      "cell_conn_type\n",
      "[0 1]\n",
      "categorical_cols:  ['cell_conn_type']\n",
      "(1796746, 92)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1621562, 92)\n",
      "(89838, 92)\n",
      "(85346, 92)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "refresh rate:  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                     | Type    | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | task_loss_fn             | MSELoss | 0      | train\n",
      "1 | mask_loss_fn             | BCELoss | 0      | train\n",
      "2 | categorical_feature_loss | BCELoss | 0      | train\n",
      "3 | continuous_feature_loss  | MSELoss | 0      | train\n",
      "4 | model                    | DAE     | 295 K  | train\n",
      "-------------------------------------------------------------\n",
      "295 K     Trainable params\n",
      "0         Non-trainable params\n",
      "295 K     Total params\n",
      "1.183     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|████████████████████████████████████████████████████████████████████████████| 13371/13371 [02:10<00:00, 102.14it/s, v_num=974]\n",
      "Epoch 1: 100%|███████████████████████████████████████████| 13371/13371 [02:20<00:00, 95.02it/s, v_num=974, train_loss=1.030, val_loss=0.851]\u001b[A\n",
      "Epoch 2: 100%|███████████████████████████████████████████| 13371/13371 [02:27<00:00, 90.70it/s, v_num=974, train_loss=0.902, val_loss=0.835]\u001b[A\n",
      "Epoch 3: 100%|███████████████████████████████████████████| 13371/13371 [02:26<00:00, 91.41it/s, v_num=974, train_loss=0.880, val_loss=0.816]\u001b[A\n",
      "Epoch 4: 100%|███████████████████████████████████████████| 13371/13371 [02:27<00:00, 90.83it/s, v_num=974, train_loss=0.866, val_loss=0.820]\u001b[A\n",
      "Epoch 5: 100%|███████████████████████████████████████████| 13371/13371 [02:26<00:00, 91.19it/s, v_num=974, train_loss=0.857, val_loss=0.800]\u001b[A\n",
      "Epoch 6: 100%|███████████████████████████████████████████| 13371/13371 [02:25<00:00, 91.96it/s, v_num=974, train_loss=0.848, val_loss=0.789]\u001b[A\n",
      "Epoch 7: 100%|███████████████████████████████████████████| 13371/13371 [02:26<00:00, 91.04it/s, v_num=974, train_loss=0.839, val_loss=0.776]\u001b[A\n",
      "Epoch 8: 100%|███████████████████████████████████████████| 13371/13371 [02:25<00:00, 92.11it/s, v_num=974, train_loss=0.832, val_loss=0.770]\u001b[A\n",
      "Epoch 9: 100%|███████████████████████████████████████████| 13371/13371 [02:27<00:00, 90.54it/s, v_num=974, train_loss=0.827, val_loss=0.771]\u001b[A\n",
      "Epoch 10: 100%|██████████████████████████████████████████| 13371/13371 [02:27<00:00, 90.70it/s, v_num=974, train_loss=0.821, val_loss=0.762]\u001b[A\n",
      "Epoch 11: 100%|██████████████████████████████████████████| 13371/13371 [02:27<00:00, 90.64it/s, v_num=974, train_loss=0.816, val_loss=0.756]\u001b[A\n",
      "Epoch 12: 100%|██████████████████████████████████████████| 13371/13371 [02:26<00:00, 91.18it/s, v_num=974, train_loss=0.811, val_loss=0.756]\u001b[A\n",
      "Epoch 13: 100%|██████████████████████████████████████████| 13371/13371 [02:25<00:00, 91.72it/s, v_num=974, train_loss=0.806, val_loss=0.751]\u001b[A\n",
      "Epoch 14: 100%|██████████████████████████████████████████| 13371/13371 [02:24<00:00, 92.24it/s, v_num=974, train_loss=0.802, val_loss=0.737]\u001b[A\n",
      "Epoch 15: 100%|██████████████████████████████████████████| 13371/13371 [02:26<00:00, 91.43it/s, v_num=974, train_loss=0.797, val_loss=0.733]\u001b[A\n",
      "Epoch 16: 100%|██████████████████████████████████████████| 13371/13371 [02:27<00:00, 90.42it/s, v_num=974, train_loss=0.793, val_loss=0.734]\u001b[A\n",
      "Epoch 17: 100%|██████████████████████████████████████████| 13371/13371 [02:26<00:00, 91.25it/s, v_num=974, train_loss=0.789, val_loss=0.727]\u001b[A\n",
      "Epoch 18: 100%|██████████████████████████████████████████| 13371/13371 [02:27<00:00, 90.36it/s, v_num=974, train_loss=0.786, val_loss=0.727]\u001b[A\n",
      "Epoch 19: 100%|██████████████████████████████████████████| 13371/13371 [02:27<00:00, 90.88it/s, v_num=974, train_loss=0.781, val_loss=0.720]\u001b[A\n",
      "Epoch 20: 100%|██████████████████████████████████████████| 13371/13371 [02:27<00:00, 90.71it/s, v_num=974, train_loss=0.776, val_loss=0.721]\u001b[A\n",
      "Epoch 21: 100%|██████████████████████████████████████████| 13371/13371 [02:27<00:00, 90.54it/s, v_num=974, train_loss=0.773, val_loss=0.719]\u001b[A\n",
      "Epoch 22: 100%|██████████████████████████████████████████| 13371/13371 [02:26<00:00, 91.03it/s, v_num=974, train_loss=0.771, val_loss=0.708]\u001b[A\n",
      "Epoch 23: 100%|██████████████████████████████████████████| 13371/13371 [02:27<00:00, 90.51it/s, v_num=974, train_loss=0.766, val_loss=0.712]\u001b[A\n",
      "Epoch 24: 100%|██████████████████████████████████████████| 13371/13371 [02:26<00:00, 91.34it/s, v_num=974, train_loss=0.764, val_loss=0.704]\u001b[A\n",
      "Epoch 25: 100%|██████████████████████████████████████████| 13371/13371 [02:26<00:00, 91.26it/s, v_num=974, train_loss=0.762, val_loss=0.713]\u001b[A\n",
      "Epoch 26: 100%|██████████████████████████████████████████| 13371/13371 [02:27<00:00, 90.86it/s, v_num=974, train_loss=0.758, val_loss=0.703]\u001b[A\n",
      "Epoch 27: 100%|██████████████████████████████████████████| 13371/13371 [02:27<00:00, 90.65it/s, v_num=974, train_loss=0.756, val_loss=0.702]\u001b[A\n",
      "Epoch 28: 100%|██████████████████████████████████████████| 13371/13371 [02:29<00:00, 89.41it/s, v_num=974, train_loss=0.754, val_loss=0.697]\u001b[A\n",
      "Epoch 29: 100%|██████████████████████████████████████████| 13371/13371 [02:28<00:00, 90.24it/s, v_num=974, train_loss=0.752, val_loss=0.695]\u001b[A\n",
      "Epoch 30: 100%|██████████████████████████████████████████| 13371/13371 [02:28<00:00, 90.04it/s, v_num=974, train_loss=0.747, val_loss=0.687]\u001b[A\n",
      "Epoch 31: 100%|██████████████████████████████████████████| 13371/13371 [02:28<00:00, 90.32it/s, v_num=974, train_loss=0.746, val_loss=0.690]\u001b[A\n",
      "Epoch 32: 100%|██████████████████████████████████████████| 13371/13371 [02:26<00:00, 91.31it/s, v_num=974, train_loss=0.744, val_loss=0.699]\u001b[A\n",
      "Epoch 33: 100%|██████████████████████████████████████████| 13371/13371 [02:26<00:00, 91.24it/s, v_num=974, train_loss=0.740, val_loss=0.690]\u001b[A\n",
      "Epoch 34: 100%|██████████████████████████████████████████| 13371/13371 [02:27<00:00, 90.86it/s, v_num=974, train_loss=0.739, val_loss=0.683]\u001b[A\n",
      "Epoch 35: 100%|██████████████████████████████████████████| 13371/13371 [02:27<00:00, 90.60it/s, v_num=974, train_loss=0.738, val_loss=0.679]\u001b[A\n",
      "Epoch 36: 100%|██████████████████████████████████████████| 13371/13371 [02:26<00:00, 91.34it/s, v_num=974, train_loss=0.736, val_loss=0.683]\u001b[A\n",
      "Epoch 37: 100%|██████████████████████████████████████████| 13371/13371 [02:26<00:00, 90.96it/s, v_num=974, train_loss=0.734, val_loss=0.674]\u001b[A\n",
      "Epoch 38: 100%|██████████████████████████████████████████| 13371/13371 [02:27<00:00, 90.59it/s, v_num=974, train_loss=0.734, val_loss=0.678]\u001b[A\n",
      "Epoch 39: 100%|██████████████████████████████████████████| 13371/13371 [02:24<00:00, 92.36it/s, v_num=974, train_loss=0.730, val_loss=0.672]\u001b[A\n",
      "Epoch 40: 100%|██████████████████████████████████████████| 13371/13371 [02:28<00:00, 90.16it/s, v_num=974, train_loss=0.730, val_loss=0.673]\u001b[A\n",
      "Epoch 41: 100%|██████████████████████████████████████████| 13371/13371 [02:25<00:00, 91.88it/s, v_num=974, train_loss=0.729, val_loss=0.667]\u001b[A\n",
      "Epoch 42: 100%|██████████████████████████████████████████| 13371/13371 [02:27<00:00, 90.56it/s, v_num=974, train_loss=0.727, val_loss=0.670]\u001b[A\n",
      "Epoch 43: 100%|██████████████████████████████████████████| 13371/13371 [02:26<00:00, 91.48it/s, v_num=974, train_loss=0.725, val_loss=0.667]\u001b[A\n",
      "Epoch 44: 100%|██████████████████████████████████████████| 13371/13371 [02:28<00:00, 89.99it/s, v_num=974, train_loss=0.725, val_loss=0.662]\u001b[A\n",
      "Epoch 45: 100%|██████████████████████████████████████████| 13371/13371 [02:28<00:00, 90.05it/s, v_num=974, train_loss=0.724, val_loss=0.660]\u001b[A\n",
      "Epoch 46: 100%|██████████████████████████████████████████| 13371/13371 [02:27<00:00, 90.62it/s, v_num=974, train_loss=0.725, val_loss=0.661]\u001b[A\n",
      "Epoch 47: 100%|██████████████████████████████████████████| 13371/13371 [02:27<00:00, 90.91it/s, v_num=974, train_loss=0.720, val_loss=0.660]\u001b[A\n",
      "Epoch 48: 100%|██████████████████████████████████████████| 13371/13371 [02:29<00:00, 89.71it/s, v_num=974, train_loss=0.718, val_loss=0.670]\u001b[A\n",
      "Epoch 49: 100%|██████████████████████████████████████████| 13371/13371 [02:25<00:00, 91.77it/s, v_num=974, train_loss=0.719, val_loss=0.665]\u001b[A\n",
      "Epoch 50: 100%|██████████████████████████████████████████| 13371/13371 [02:28<00:00, 90.05it/s, v_num=974, train_loss=0.717, val_loss=0.653]\u001b[A\n",
      "Epoch 51: 100%|██████████████████████████████████████████| 13371/13371 [02:31<00:00, 88.01it/s, v_num=974, train_loss=0.717, val_loss=0.659]\u001b[A\n",
      "Epoch 52: 100%|██████████████████████████████████████████| 13371/13371 [02:29<00:00, 89.33it/s, v_num=974, train_loss=0.715, val_loss=0.657]\u001b[A\n",
      "Epoch 53: 100%|██████████████████████████████████████████| 13371/13371 [02:26<00:00, 91.06it/s, v_num=974, train_loss=0.713, val_loss=0.653]\u001b[A\n",
      "Epoch 54: 100%|██████████████████████████████████████████| 13371/13371 [02:26<00:00, 91.42it/s, v_num=974, train_loss=0.712, val_loss=0.653]\u001b[A\n",
      "Epoch 55: 100%|██████████████████████████████████████████| 13371/13371 [02:28<00:00, 90.31it/s, v_num=974, train_loss=0.712, val_loss=0.652]\u001b[A\n",
      "Epoch 56: 100%|██████████████████████████████████████████| 13371/13371 [02:25<00:00, 91.71it/s, v_num=974, train_loss=0.709, val_loss=0.652]\u001b[A\n",
      "Epoch 57: 100%|██████████████████████████████████████████| 13371/13371 [02:26<00:00, 91.16it/s, v_num=974, train_loss=0.710, val_loss=0.653]\u001b[A\n",
      "Epoch 58: 100%|██████████████████████████████████████████| 13371/13371 [02:28<00:00, 90.16it/s, v_num=974, train_loss=0.708, val_loss=0.649]\u001b[A\n",
      "Epoch 59: 100%|██████████████████████████████████████████| 13371/13371 [02:26<00:00, 90.97it/s, v_num=974, train_loss=0.710, val_loss=0.640]\u001b[A\n",
      "Epoch 60: 100%|██████████████████████████████████████████| 13371/13371 [02:28<00:00, 89.96it/s, v_num=974, train_loss=0.707, val_loss=0.649]\u001b[A\n",
      "Epoch 61: 100%|██████████████████████████████████████████| 13371/13371 [02:28<00:00, 89.81it/s, v_num=974, train_loss=0.706, val_loss=0.643]\u001b[A\n",
      "Epoch 62: 100%|██████████████████████████████████████████| 13371/13371 [02:26<00:00, 91.01it/s, v_num=974, train_loss=0.705, val_loss=0.645]\u001b[A\n",
      "Epoch 63: 100%|██████████████████████████████████████████| 13371/13371 [02:27<00:00, 90.92it/s, v_num=974, train_loss=0.703, val_loss=0.644]\u001b[A\n",
      "Epoch 64: 100%|██████████████████████████████████████████| 13371/13371 [02:27<00:00, 90.54it/s, v_num=974, train_loss=0.703, val_loss=0.648]\u001b[A\n",
      "Epoch 65: 100%|██████████████████████████████████████████| 13371/13371 [02:23<00:00, 92.93it/s, v_num=974, train_loss=0.702, val_loss=0.645]\u001b[A\n",
      "Epoch 66: 100%|██████████████████████████████████████████| 13371/13371 [02:27<00:00, 90.58it/s, v_num=974, train_loss=0.703, val_loss=0.647]\u001b[A\n",
      "Epoch 67: 100%|██████████████████████████████████████████| 13371/13371 [02:26<00:00, 91.25it/s, v_num=974, train_loss=0.701, val_loss=0.634]\u001b[A\n",
      "Epoch 68: 100%|██████████████████████████████████████████| 13371/13371 [02:25<00:00, 91.65it/s, v_num=974, train_loss=0.700, val_loss=0.636]\u001b[A\n",
      "Epoch 69: 100%|██████████████████████████████████████████| 13371/13371 [02:27<00:00, 90.49it/s, v_num=974, train_loss=0.698, val_loss=0.635]\u001b[A\n",
      "Epoch 70: 100%|██████████████████████████████████████████| 13371/13371 [02:26<00:00, 91.37it/s, v_num=974, train_loss=0.698, val_loss=0.636]\u001b[A\n",
      "Epoch 71: 100%|██████████████████████████████████████████| 13371/13371 [02:27<00:00, 90.81it/s, v_num=974, train_loss=0.699, val_loss=0.637]\u001b[A\n",
      "Epoch 72: 100%|██████████████████████████████████████████| 13371/13371 [02:26<00:00, 91.42it/s, v_num=974, train_loss=0.700, val_loss=0.634]\u001b[A\n",
      "Epoch 73: 100%|██████████████████████████████████████████| 13371/13371 [02:26<00:00, 91.51it/s, v_num=974, train_loss=0.697, val_loss=0.634]\u001b[A\n",
      "Epoch 74: 100%|██████████████████████████████████████████| 13371/13371 [02:27<00:00, 90.55it/s, v_num=974, train_loss=0.695, val_loss=0.629]\u001b[A\n",
      "Epoch 75: 100%|██████████████████████████████████████████| 13371/13371 [02:27<00:00, 90.84it/s, v_num=974, train_loss=0.695, val_loss=0.629]\u001b[A\n",
      "Epoch 76: 100%|██████████████████████████████████████████| 13371/13371 [02:28<00:00, 90.33it/s, v_num=974, train_loss=0.695, val_loss=0.629]\u001b[A\n",
      "Epoch 77: 100%|██████████████████████████████████████████| 13371/13371 [02:26<00:00, 91.35it/s, v_num=974, train_loss=0.694, val_loss=0.640]\u001b[A\n",
      "Epoch 78: 100%|██████████████████████████████████████████| 13371/13371 [02:28<00:00, 90.03it/s, v_num=974, train_loss=0.693, val_loss=0.632]\u001b[A\n",
      "Epoch 79: 100%|██████████████████████████████████████████| 13371/13371 [02:27<00:00, 90.59it/s, v_num=974, train_loss=0.692, val_loss=0.630]\u001b[A\n",
      "Epoch 80: 100%|██████████████████████████████████████████| 13371/13371 [02:27<00:00, 90.56it/s, v_num=974, train_loss=0.696, val_loss=0.632]\u001b[A\n",
      "Epoch 81: 100%|██████████████████████████████████████████| 13371/13371 [02:27<00:00, 90.58it/s, v_num=974, train_loss=0.692, val_loss=0.631]\u001b[A\n",
      "Epoch 82: 100%|██████████████████████████████████████████| 13371/13371 [02:26<00:00, 91.04it/s, v_num=974, train_loss=0.693, val_loss=0.631]\u001b[A\n",
      "Epoch 83: 100%|██████████████████████████████████████████| 13371/13371 [02:25<00:00, 91.61it/s, v_num=974, train_loss=0.692, val_loss=0.629]\u001b[A\n",
      "Epoch 84: 100%|██████████████████████████████████████████| 13371/13371 [02:27<00:00, 90.95it/s, v_num=974, train_loss=0.691, val_loss=0.631]\u001b[A\n",
      "Epoch 85: 100%|██████████████████████████████████████████| 13371/13371 [02:27<00:00, 90.58it/s, v_num=974, train_loss=0.691, val_loss=0.632]\u001b[A\n",
      "Epoch 86: 100%|██████████████████████████████████████████| 13371/13371 [02:27<00:00, 90.73it/s, v_num=974, train_loss=0.689, val_loss=0.620]\u001b[A\n",
      "Epoch 87: 100%|██████████████████████████████████████████| 13371/13371 [02:28<00:00, 89.99it/s, v_num=974, train_loss=0.690, val_loss=0.624]\u001b[A\n",
      "Epoch 88: 100%|██████████████████████████████████████████| 13371/13371 [02:29<00:00, 89.60it/s, v_num=974, train_loss=0.689, val_loss=0.631]\u001b[A\n",
      "Epoch 89: 100%|██████████████████████████████████████████| 13371/13371 [02:27<00:00, 90.51it/s, v_num=974, train_loss=0.689, val_loss=0.632]\u001b[A\n",
      "Epoch 90: 100%|██████████████████████████████████████████| 13371/13371 [02:26<00:00, 91.43it/s, v_num=974, train_loss=0.688, val_loss=0.622]\u001b[A\n",
      "Epoch 91: 100%|██████████████████████████████████████████| 13371/13371 [02:27<00:00, 90.56it/s, v_num=974, train_loss=0.687, val_loss=0.622]\u001b[A\n",
      "Epoch 92: 100%|██████████████████████████████████████████| 13371/13371 [02:25<00:00, 91.84it/s, v_num=974, train_loss=0.688, val_loss=0.634]\u001b[A\n",
      "Epoch 93: 100%|██████████████████████████████████████████| 13371/13371 [02:27<00:00, 90.93it/s, v_num=974, train_loss=0.684, val_loss=0.623]\u001b[A\n",
      "Epoch 94: 100%|██████████████████████████████████████████| 13371/13371 [02:29<00:00, 89.23it/s, v_num=974, train_loss=0.685, val_loss=0.621]\u001b[A\n",
      "Epoch 95: 100%|██████████████████████████████████████████| 13371/13371 [02:26<00:00, 90.99it/s, v_num=974, train_loss=0.686, val_loss=0.624]\u001b[A\n",
      "Epoch 96: 100%|██████████████████████████████████████████| 13371/13371 [02:28<00:00, 90.34it/s, v_num=974, train_loss=0.685, val_loss=0.619]\u001b[A\n",
      "Epoch 97: 100%|██████████████████████████████████████████| 13371/13371 [02:26<00:00, 91.34it/s, v_num=974, train_loss=0.684, val_loss=0.624]\u001b[A\n",
      "Epoch 98: 100%|██████████████████████████████████████████| 13371/13371 [02:29<00:00, 89.42it/s, v_num=974, train_loss=0.682, val_loss=0.620]\u001b[A\n",
      "Epoch 99: 100%|██████████████████████████████████████████| 13371/13371 [02:27<00:00, 90.55it/s, v_num=974, train_loss=0.684, val_loss=0.618]\u001b[A\n",
      "Epoch 99: 100%|██████████████████████████████████████████| 13371/13371 [02:32<00:00, 87.55it/s, v_num=974, train_loss=0.683, val_loss=0.636]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████████████████████████████████████| 13371/13371 [02:32<00:00, 87.54it/s, v_num=974, train_loss=0.683, val_loss=0.636]\n",
      "DONE SAVING PRETRAINED MODEL\n",
      "   | Name                                             | Type        | Params | Mode \n",
      "------------------------------------------------------------------------------------------\n",
      "0  | task_loss_fn                                     | MSELoss     | 0      | train\n",
      "1  | mask_loss_fn                                     | BCELoss     | 0      | train\n",
      "2  | categorical_feature_loss                         | BCELoss     | 0      | train\n",
      "3  | continuous_feature_loss                          | MSELoss     | 0      | train\n",
      "4  | model                                            | DAE         | 295 K  | train\n",
      "5  | model._DAE__encoder                              | MLP         | 140 K  | train\n",
      "6  | model._DAE__encoder.linear_0                     | Linear      | 18.6 K | train\n",
      "7  | model._DAE__encoder.batchnorm_0                  | BatchNorm1d | 400    | train\n",
      "8  | model._DAE__encoder.relu_0                       | ReLU        | 0      | train\n",
      "9  | model._DAE__encoder.dropout_0                    | Dropout     | 0      | train\n",
      "10 | model._DAE__encoder.linear_1                     | Linear      | 40.2 K | train\n",
      "11 | model._DAE__encoder.batchnorm_1                  | BatchNorm1d | 400    | train\n",
      "12 | model._DAE__encoder.relu_1                       | ReLU        | 0      | train\n",
      "13 | model._DAE__encoder.dropout_1                    | Dropout     | 0      | train\n",
      "14 | model._DAE__encoder.linear_2                     | Linear      | 40.2 K | train\n",
      "15 | model._DAE__encoder.batchnorm_2                  | BatchNorm1d | 400    | train\n",
      "16 | model._DAE__encoder.relu_2                       | ReLU        | 0      | train\n",
      "17 | model._DAE__encoder.dropout_2                    | Dropout     | 0      | train\n",
      "18 | model._DAE__encoder.linear_n_layers              | Linear      | 40.2 K | train\n",
      "19 | model.mask_predictor_head                        | MLP         | 27.2 K | train\n",
      "20 | model.mask_predictor_head.linear_0               | Linear      | 18.5 K | train\n",
      "21 | model.mask_predictor_head.batchnorm_0            | BatchNorm1d | 184    | train\n",
      "22 | model.mask_predictor_head.relu_0                 | ReLU        | 0      | train\n",
      "23 | model.mask_predictor_head.dropout_0              | Dropout     | 0      | train\n",
      "24 | model.mask_predictor_head.linear_n_layers        | Linear      | 8.6 K  | train\n",
      "25 | model.reconstruction_head                        | MLP         | 27.2 K | train\n",
      "26 | model.reconstruction_head.linear_0               | Linear      | 18.5 K | train\n",
      "27 | model.reconstruction_head.batchnorm_0            | BatchNorm1d | 184    | train\n",
      "28 | model.reconstruction_head.relu_0                 | ReLU        | 0      | train\n",
      "29 | model.reconstruction_head.dropout_0              | Dropout     | 0      | train\n",
      "30 | model.reconstruction_head.linear_n_layers        | Linear      | 8.6 K  | train\n",
      "31 | model.one_layer_prediction_head                  | Sequential  | 40.4 K | train\n",
      "32 | model.one_layer_prediction_head.head_linear_hid  | Linear      | 40.2 K | train\n",
      "33 | model.one_layer_prediction_head.head_activation  | ReLU        | 0      | train\n",
      "34 | model.one_layer_prediction_head.head_linear_out  | Linear      | 201    | train\n",
      "35 | model.two_layer_prediction_head                  | Sequential  | 60.4 K | train\n",
      "36 | model.two_layer_prediction_head.head_linear_hid1 | Linear      | 40.2 K | train\n",
      "37 | model.two_layer_prediction_head.head_activation1 | ReLU        | 0      | train\n",
      "38 | model.two_layer_prediction_head.head_linear_hid2 | Linear      | 20.1 K | train\n",
      "39 | model.two_layer_prediction_head.head_activation2 | ReLU        | 0      | train\n",
      "40 | model.two_layer_prediction_head.head_linear_out  | Linear      | 101    | train\n",
      "------------------------------------------------------------------------------------------\n",
      "295 K     Trainable params\n",
      "0         Non-trainable params\n",
      "295 K     Total params\n",
      "1.183     Total estimated model params size (MB)\n",
      "DONE PRETRAINING\n"
     ]
    }
   ],
   "source": [
    "s3l_training(pretrain=True, # if True First Phase training\n",
    "             use_pretrained_model=False, # if True Second Phase learning\n",
    "             pt_type='dae', \n",
    "             pt_folder='FP2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7160f9-4db4-4688-9556-ba884a04eafb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
