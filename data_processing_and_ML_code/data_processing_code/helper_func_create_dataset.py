import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import glob
import os
import time
import sys
import math
import scipy.stats as stats
import datetime as dt
from multiprocessing import Pool
import subprocess

# constant multipliers
M = (10**6)
K = (10**3)

# list of all the file names and categories 
# All the log files from the ns-3 inbuilt RAN logs enabled by EnableTraces() 
# I have taken the Interference file off this list because it was Goddamn huge and I did not plan on using it anyway
ran_files = ['UlSinrStats.txt', # observed at basestation 
             'DlRsrpSinrStats.txt', # observed at UEs
             'UlTxPhyStats.txt', # observed at UEs 
             'UlRxPhyStats.txt', # observed at basestation 
             'DlTxPhyStats.txt', # observed at basestation 
             'DlRxPhyStats.txt', # observed at UEs
             'UlMacStats.txt', # observed at basestation 
             'DlMacStats.txt', # observed at basestation 
             'UlRlcStats.txt', # observed at ?
             'DlRlcStats.txt', # observed at ?
             'UlPdcpStats.txt', # observed at ?
             'DlPdcpStats.txt' # observed at ?
             ]

# All application related log files
app_files = ['vrBurst_trace.txt', 'vrFragment_trace.txt', # VR
             'dashClient_trace.txt', 'mpegPlayer_trace.txt', # video streaming
             'httpServerDelay_trace.txt', 'httpClientDelay_trace.txt', 'httpClientRtt_trace.txt', # web browsing
             'dlThroughput_trace.txt', 'ulThroughput_trace.txt', # throughput measurement
             'delay_trace.txt', 'rtt_trace.txt', # delay measurement
             'flow_trace.txt'] # UDP flow 

# Other files that have connectivity and location info
other_files = ['handover_trace.txt', 'mobility_trace.txt']

# These are files that keep track of the bytes flying through the different layer.
# I am interested in this specific group to make sure that the bytes flowing through 
# the layers is roughly the same after accounting for headers, and to also make sure
# that what we expect to see for each UL and DL are actually seen. 
files_with_bytes = ['DlTxPhyStats.txt', 'DlRxPhyStats.txt',
                    'UlTxPhyStats.txt', 'UlRxPhyStats.txt',
                    'DlMacStats.txt', 'UlMacStats.txt',
                    'DlRlcStats.txt', 'UlRlcStats.txt',  
                    'DlPdcpStats.txt', 'UlPdcpStats.txt']

# Some internally generated files have a trailing tab at the end of each row, 
# but not at the end of the first header row
# This causes the read_csv function to read the data weirdly. 
# So these files have been grouped to fix that
files_with_trailing_tab=['DlRlcStats.txt', 'UlRlcStats.txt', 
                         'DlPdcpStats.txt', 'UlPdcpStats.txt']

# The timestamp unit in different RAN log files is different, even though they are all generated by the same 
# internal ns-3 logging system (eye roll)
file_name_to_tstamp_unit = {'DlTxPhyStats.txt':'ms', 'UlRxPhyStats.txt':'ms', 
                            'UlSinrStats.txt':'s', 'UlPdcpStats.txt':'s', 'DlRsrpSinrStats.txt':'s', 
                            'UlRlcStats.txt':'s', 'UlMacStats.txt':'s', 'UlTxPhyStats.txt':'ms', 
                            'DlPdcpStats.txt':'s', 'DlRlcStats.txt':'s', 'DlRxPhyStats.txt':'ms', 
                            'DlMacStats.txt':'s'}

# These are files that will go through the windowing process where the time slice is made uniform and all IMSIs are included 
# Don't include files which do not have all IMSIs
a_vs_b_files = ran_files + ['mobility_trace.txt', 'dlThroughput_trace.txt', 'ulThroughput_trace.txt', 
                            'delay_trace.txt', 'rtt_trace.txt', 'vrFragment_trace.txt', 'dashClient_trace.txt'] 

# The internal log files do not follow a consistent naming convention for some metrics
# and hence I need to isolate them and fix it
files_that_use_upper_case_cellid=['UlPdcpStats.txt', 'UlRlcStats.txt', 'DlPdcpStats.txt', 'DlRlcStats.txt']

# this is not in files so no need to remove
not_to_be_used_now=['UlInterferenceStats.txt']








# columns that are not features and hence can be removed if counting number of features
#nonfeature_columns = ['tstamp_us', 'ueId', 'IMSI', 'cellId', 'dir', 'pktUid']

# These are to be used to work with or dropped before the columsn fo multiple logs are combined 
# since these are col names before adding any suffix or prefix to col names to combine them over multiple diff logs  
sum_feats = ['sizeTb1', 'sizeTb2', 'nTxPDUs', 'TxBytes', 'nRxPDUs', 'RxBytes', 'size']

drop_cols_before_sep = ['ueId', 'RNTI', 'IMSI', 'tstamp_us', 'componentCarrierId', 'componentCarrierId.1', 'ComponentCarrierId', 'ccId',
                        'min', 'min.1', 'max', 'max.1', 'stdDev', 'stdDev.1']
drop_cols_after_sep = ['cellId','cell_cellId']



def clean_up_logs(df, file, sim_start_time, sim_end_time):
    ## Do some file specific preprocessing
    ## Make uniform the timestamp units convert them all to micro seconds 
    if '% time' in df.columns:
        df.rename(columns = {'% time':'tstamp_us'}, inplace = True)
        if file_name_to_tstamp_unit[file] == 'ms':
            df['tstamp_us'] = df['tstamp_us']*K
        elif file_name_to_tstamp_unit[file] == 's':
            df['tstamp_us'] = df['tstamp_us']*M
    
    ## Make uniform the timestamp units        
    if '% start' in df.columns:
        ## TO DO: check if this is actually micro seconds. I think it is seconds   
        df.rename(columns = {'% start':'tstamp_us'}, inplace = True)
        df.rename(columns = {'end':'end_timeslot_us'}, inplace = True)
        df['tstamp_us'] = df['tstamp_us']*M
        df['end_timeslot_us'] = df['end_timeslot_us']*M
    
    ## Some internally generated logs use the naming 'CellId' replace that with 'cellId'
    if ('CellId' in df.columns):
        df.rename(columns = {'CellId':'cellId'}, inplace = True)
    if ('currentCellId' in df.columns):
        df.rename(columns = {'currentCellId':'cellId'}, inplace = True)    
    
    # Warning: cellId and IMSI here are IP addresses, but it should not matter since 
    # there will only be 1 UE doing these ul and dl throughput scans   
    if file == 'dlThroughput_trace.txt':
        df.rename(columns = {'toAddr':'IMSI', 'fromAddr': 'cellId'}, inplace = True)
        assert (df['IMSI'].nunique() == 1), "More than one throughput measurement UE is in the logs" 
    if file == 'ulThroughput_trace.txt':
        df.rename(columns = {'toAddr':'cellId', 'fromAddr': 'IMSI'}, inplace = True)
        assert (df['IMSI'].nunique() == 1), "More than one throughput measurement UE is in the logs" 
        
    ## Just for plotting change the timestamp_us to seconds and delay values to milli seconds 
    ## since I am mostly plotting directly from pandas and don't know how to add a multiplicative factor to a column    
    if 'tstamp_us' in df.columns:
        df['tstamp_us'] = df['tstamp_us']/M
        # Set datetime index for all files so that we can do series operations 
        datatime_timestamps = pd.to_datetime(df['tstamp_us'], unit='s', origin='unix')
        df = df.set_index(datatime_timestamps, inplace=False)
        # add a sample at the beginning and ending of every timeseries at sim_start_time and sim_end_time. 
        # This way the timeseries after resampling are all of the same length 
        if file in a_vs_b_files: 
            alignment_sample = np.empty(df.shape[1])
            alignment_sample[:] = np.nan
            start_sample = pd.DataFrame([alignment_sample], columns=df.columns, index=[sim_start_time])
            end_sample = pd.DataFrame([alignment_sample], columns=df.columns, index=[sim_end_time])
            if 'dir' in df.columns:
                imsis_in_file = sorted(df['IMSI'].unique())
                # all_imsis = range(1,total_num_ues+1)
                for imsi in imsis_in_file:
                    for di in ['UL', 'DL']:
                        start_sample['IMSI'] = imsi
                        start_sample['dir'] = di
                        end_sample['IMSI'] = imsi
                        end_sample['dir'] = di
                        df = pd.concat([start_sample, df, end_sample])
            elif 'cellId' in df.columns:
                #print('The issue is here. I am only initializing for cell Id 0 and not the others which is why those start from 0.5 and cellSI 0 starts from 0')
                # Is there a better way to do this ? Like just slice them afterwards to something smaller? 
                imsis_in_file = sorted(df['IMSI'].unique())
                # all_imsis = range(1,total_num_ues+1)
                for imsi in imsis_in_file:
                    start_sample['IMSI'] = imsi
                    start_sample['cellId'] = 0
                    end_sample['IMSI'] = imsi
                    end_sample['cellId'] = 0
                    df = pd.concat([start_sample, df, end_sample])
            else:
                imsis_in_file = sorted(df['IMSI'].unique())
                # all_imsis = range(1,total_num_ues+1)
                for imsi in imsis_in_file:
                    start_sample['IMSI'] = imsi
                    end_sample['IMSI'] = imsi
                    df = pd.concat([start_sample, df, end_sample])
    
    if file == 'dlThroughput_trace.txt' or file == 'ulThroughput_trace.txt':
        df['IMSI'] = 1
        thput_meas_imsi = 1
    
    # converting all delay values to ms instead of us
    if 'delay' in df.columns:
        df['delay'] = df['delay']/K
        
    return df

def run_ran_to_completion(run):
    # Don't need this anymore since we now have fixed the bug that would make it crash
    with open(run+'/simulation_info.txt', "r") as sim_info_file:
        last_line = sim_info_file.readlines()[-1]
        print(last_line)
        if 'Elapsed wall clock' not in last_line:
            print('This run did not finish to completion, skipping it')
            return False
        else:
            return True


# taking num of steps as input create step indexed column names for a set of columns
# This is currently used to expand the drop_cols set depending of whether one needs 
# to drop just one or multiple steps of this column 
def expand_cols_to_step_size(cols, steps):
    if steps == 1:
        return cols
    #else
    expanded_cols = []
    for col in cols:
        expanded_cols.extend([col+'_'+str(i) for i in range(steps)])   
    return expanded_cols


def filtered_imsis(filter_str, imsis):
    if filter_str == 'macro':
        return list(set(imsis).intersection(macro_imsis))
    elif filter_str == 'micro':
        return list(set(imsis).intersection(micro_imsis))
    elif filter_str == 'fast':
        return list(set(imsis).intersection(fast_imsis))
    elif filter_str == 'slow':
        return list(set(imsis).intersection(slow_imsis))
    else:# keep them all 
        return imsis

def plot_histogram(val, colour, xlabel, plot_name, plot_dir):
    plt.figure(figsize=(6,4))
    xlimit=np.nanquantile(val,0.97)
    plt.hist(val[val <= xlimit], bins=50, color=colour, edgecolor='black')
    plt.yticks([])
    plt.xlabel(xlabel); 
    fname=plot_dir+plot_name
    plt.savefig(fname)
    plt.show()
    
def plot_histogram_xlim(val, colour, xlabel, plot_name, plot_dir, xlimit):
    plt.figure(figsize=(6,4))
    val = val[(val > xlimit[0])]
    val = val[(val <= xlimit[1])]
    plt.hist(val, bins=50, color=colour, edgecolor='black')
    plt.yticks([])
    plt.xlabel(xlabel); 
    #plt.xlim(0, xlimit);
    fname=plot_dir+plot_name
    plt.savefig(fname)
    plt.show()    
    
    
def plot_metric_vs_distance_to_cell(dist, y, colour, ylabel, plot_name, plot_dir):
    fig = plt.figure(figsize=(8,6))
    ylimit=np.nanquantile(y,0.99)
    plt.plot(dist[y <= ylimit], y[y <= ylimit], '.', color=colour)
    plt.ylim()
    plt.xlabel('UE - BS distance (meters)'); plt.ylabel(ylabel); 
    fname=plot_dir+plot_name
    plt.savefig(fname)
    plt.show()
    
def plot_metric_vs_sinr(sinr, y, colour, ylabel, plot_name, plot_dir):
    fig = plt.figure(figsize=(8,6))
    ylimit=np.nanquantile(y,0.99)
    plt.plot(sinr[y <= ylimit], y[y <= ylimit], '.', color=colour)
    plt.ylim()
    plt.xlabel('SINR (dB)'); plt.ylabel(ylabel); 
    fname=plot_dir+plot_name
    plt.savefig(fname)
    plt.show()   