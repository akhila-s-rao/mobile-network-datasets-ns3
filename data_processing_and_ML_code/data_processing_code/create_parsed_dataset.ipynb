{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2e3e665-f8c9-4dc0-8ad9-94ec774bbc43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reload\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dff9e1d9-f635-4ec2-abe5-f4e8c87e9d27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import  \n",
    "\n",
    "#%matplotlib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import sys\n",
    "import math\n",
    "import scipy.stats as stats\n",
    "#from tqdm import tqdm\n",
    "import datetime as dt\n",
    "from functions_dicts_to_parse_data import *\n",
    "# Increase plot font size from default for all plots instead of setting it in each plot \n",
    "plt.rcParams.update({'font.size': 22})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8aa75b9b-2ebf-400d-9897-81fae82ec05d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# redirect all print statements to file instead of console\n",
    "\n",
    "#file_path = 'logs_from_notebook.txt'\n",
    "#sys.stdout = open(file_path, \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec37f892-4511-4f01-b102-a9babff74722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data folder: \n",
      "../../dataset_ver1/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "create_dataset = True\n",
    "print_logfile_info = True\n",
    "# When using time aggregation to smooth out plots and make them more visually interpretable what window size to use \n",
    "time_wind_str = '500ms'\n",
    "time_wind_val = 500\n",
    "data_slice = 'all' #'macro' 'micro' 'fast' 'slow' 'all'\n",
    "\n",
    "#===================================\n",
    "# Data source\n",
    "#=================================== \n",
    "data_dir = '../../dataset_ver1/'\n",
    "\n",
    "# This is a subset of log files that we would like to parse and plot at the moment depending on what is in the data \n",
    "#files = ['mobility_trace.txt']+ran_files+[\n",
    "#                   #'dlThroughput_trace.txt',\n",
    "#                   #'rtt_trace.txt', \n",
    "#                   'delay_trace.txt', \n",
    "#                   'handover_trace.txt',     \n",
    "#                   'vrBurst_trace.txt', 'vrFragment_trace.txt',\n",
    "#                   'dashClient_trace.txt', 'mpegPlayer_trace.txt', # video streaming\n",
    "#                   'httpServerDelay_trace.txt', 'httpClientDelay_trace.txt', 'httpClientRtt_trace.txt'\n",
    "#                    ]\n",
    "# reduced version to test df buildup \n",
    "\n",
    "files= ['mobility_trace.txt']+['dashClient_trace.txt', 'delay_trace.txt', 'vrFragment_trace.txt']+ran_files\n",
    "print('Raw data folder: \\n'+data_dir+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2399008f-1ebd-407b-a5f0-3fa7d641a914",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "../../dataset_ver1/run16\n",
      "============================================================\n",
      "run16\n",
      "{'macro_rings': {'value': '0'}, 'macro_num_bs': {'value': '3'}, 'macro_layer_ues': {'value': '30'}, 'simulation_time_seconds': {'value': '1000'}, 'rand_seed': {'value': '15'}, 'create_micro_layer': {'value': '1'}, 'scheduler': {'value': 'PF'}, 'handover_algo': {'value': 'A2A4Rsrq'}, 'micro_num_bs': {'value': '3'}, 'micro_layer_ues': {'value': '60'}, 'delay_app_installed': {'value': '1'}, 'delay_pkt_interval_seconds': {'value': '+0.1s'}, 'rtt_app_installed': {'value': '0'}, 'http_app_installed': {'value': '1'}, 'dash_app_installed': {'value': '1'}, 'vr_app_installed': {'value': '1'}}\n",
      "separate_macro_micro  True\n",
      "total_num_cells 6\n",
      "total_num_ues 90\n",
      "sim_time 1000\n",
      "Macro UE IMSIs:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
      "Micro UE IMSIs:  [31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90]\n",
      "Macro fast UE IMSIs:  [1, 2, 3, 4, 5, 6]\n",
      "Macro Slow UE IMSIs:  [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
      "Only delay IMSIs:  [3, 5, 7, 9, 13, 15, 17, 19, 23, 25, 27, 29, 33, 35, 37, 39, 43, 45, 47, 49, 53, 55, 57, 59, 63, 65, 67, 69, 73, 75, 77, 79, 83, 85, 87]\n",
      "Macro CellIds:  [1, 2, 3]\n",
      "Micro CellIds:  [4, 5, 6]\n",
      "--------------------------------------------\n",
      "mobility_trace.txt\n",
      "../../dataset_ver1/run16/mobility_trace.txt\n",
      "time to read file:  0.28087449073791504\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.5 ,  1001.0 )\n",
      "log runtime: 1000.5  seconds\n",
      "--------------------------------------------\n",
      "dashClient_trace.txt\n",
      "../../dataset_ver1/run16/dashClient_trace.txt\n",
      "time to read file:  0.015657663345336914\n",
      "ueIds: min: 1 max: 84 count: 28\n",
      "log time (start, end): ( 0.576999 ,  1000.771999 )\n",
      "log runtime: 1000.195  seconds\n",
      "video streaming IMSIs:  [1, 6, 11, 16, 21, 26, 32, 34, 38, 40, 42, 44, 48, 50, 52, 54, 58, 60, 62, 64, 68, 70, 72, 74, 78, 80, 82, 84]\n",
      "                              dashClient_trace.txt_newBitRate_bps  \\\n",
      "IMSI wind_tstamp                                                    \n",
      "1    1970-01-01 00:00:01.000                             334000.0   \n",
      "     1970-01-01 00:00:01.500                                  NaN   \n",
      "\n",
      "                              dashClient_trace.txt_oldBitRate_bps  \n",
      "IMSI wind_tstamp                                                   \n",
      "1    1970-01-01 00:00:01.000                              45000.0  \n",
      "     1970-01-01 00:00:01.500                                  NaN  \n",
      "time to parse file:  0.28198885917663574\n",
      "--------------------------------------------\n",
      "delay_trace.txt\n",
      "../../dataset_ver1/run16/delay_trace.txt\n",
      "time to read file:  1.1202919483184814\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.512999 ,  1000.986999 )\n",
      "log runtime: 1000.4739999999999  seconds\n",
      "time to parse file:  12.367131471633911\n",
      "--------------------------------------------\n",
      "vrFragment_trace.txt\n",
      "../../dataset_ver1/run16/vrFragment_trace.txt\n",
      "time to read file:  0.6228373050689697\n",
      "ueIds: min: 30 max: 90 count: 4\n",
      "log time (start, end): ( 0.503999 ,  561.698999 )\n",
      "log runtime: 561.1949999999999  seconds\n",
      "VR IMSIs:  [30, 88, 89, 90]\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "---------------------------------------------------------\n",
      "time to parse file:  16.455414295196533\n",
      "--------------------------------------------\n",
      "UlSinrStats.txt\n",
      "../../dataset_ver1/run16/UlSinrStats.txt\n",
      "time to read file:  0.38157105445861816\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.321 ,  1001.05 )\n",
      "log runtime: 1000.7289999999999  seconds\n",
      "time to parse file:  156.3439929485321\n",
      "--------------------------------------------\n",
      "DlRsrpSinrStats.txt\n",
      "../../dataset_ver1/run16/DlRsrpSinrStats.txt\n",
      "time to read file:  1.0125627517700195\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.239214 ,  1001.04 )\n",
      "log runtime: 1000.800786  seconds\n",
      "time to parse file:  84.80371975898743\n",
      "--------------------------------------------\n",
      "UlTxPhyStats.txt\n",
      "../../dataset_ver1/run16/UlTxPhyStats.txt\n",
      "time to read file:  4.761678218841553\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.514 ,  1001.053 )\n",
      "log runtime: 1000.539  seconds\n",
      "time to parse file:  145.95404171943665\n",
      "--------------------------------------------\n",
      "UlRxPhyStats.txt\n",
      "../../dataset_ver1/run16/UlRxPhyStats.txt\n",
      "time to read file:  4.922013759613037\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.514 ,  1001.049 )\n",
      "log runtime: 1000.535  seconds\n",
      "time to parse file:  146.91825318336487\n",
      "--------------------------------------------\n",
      "DlTxPhyStats.txt\n",
      "../../dataset_ver1/run16/DlTxPhyStats.txt\n",
      "time to read file:  3.3034913539886475\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.503 ,  1001.049 )\n",
      "log runtime: 1000.5459999999999  seconds\n",
      "time to parse file:  123.93448066711426\n",
      "--------------------------------------------\n",
      "DlRxPhyStats.txt\n",
      "../../dataset_ver1/run16/DlRxPhyStats.txt\n",
      "time to read file:  3.412294626235962\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.503 ,  1001.049 )\n",
      "log runtime: 1000.5459999999999  seconds\n",
      "time to parse file:  129.68304347991943\n",
      "--------------------------------------------\n",
      "UlMacStats.txt\n",
      "../../dataset_ver1/run16/UlMacStats.txt\n",
      "time to read file:  4.383254766464233\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.508 ,  1001.05 )\n",
      "log runtime: 1000.5419999999999  seconds\n",
      "time to parse file:  141.99896001815796\n",
      "--------------------------------------------\n",
      "DlMacStats.txt\n",
      "../../dataset_ver1/run16/DlMacStats.txt\n",
      "time to read file:  3.363603115081787\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.501 ,  1001.05 )\n",
      "log runtime: 1000.549  seconds\n",
      "time to parse file:  126.31186056137085\n",
      "--------------------------------------------\n",
      "UlRlcStats.txt\n",
      "../../dataset_ver1/run16/UlRlcStats.txt\n",
      "time to read file:  0.4044015407562256\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.5 ,  1001.0 )\n",
      "log runtime: 1000.5  seconds\n",
      "time to parse file:  77.65763926506042\n",
      "--------------------------------------------\n",
      "DlRlcStats.txt\n",
      "../../dataset_ver1/run16/DlRlcStats.txt\n",
      "time to read file:  0.368333101272583\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.5 ,  1001.0 )\n",
      "log runtime: 1000.5  seconds\n",
      "time to parse file:  79.55189299583435\n",
      "--------------------------------------------\n",
      "UlPdcpStats.txt\n",
      "../../dataset_ver1/run16/UlPdcpStats.txt\n",
      "time to read file:  0.40634751319885254\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.5 ,  1001.0 )\n",
      "log runtime: 1000.5  seconds\n",
      "time to parse file:  79.60844707489014\n",
      "--------------------------------------------\n",
      "DlPdcpStats.txt\n",
      "../../dataset_ver1/run16/DlPdcpStats.txt\n",
      "time to read file:  0.4207160472869873\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.5 ,  1001.0 )\n",
      "log runtime: 1000.5  seconds\n",
      "time to parse file:  80.02780842781067\n",
      "============================================================\n",
      "======================= RUN DONE =====================================\n",
      "============================================================\n",
      "../../dataset_ver1/run17\n",
      "============================================================\n",
      "run17\n",
      "{'macro_rings': {'value': '0'}, 'macro_num_bs': {'value': '3'}, 'macro_layer_ues': {'value': '30'}, 'simulation_time_seconds': {'value': '1000'}, 'rand_seed': {'value': '16'}, 'create_micro_layer': {'value': '1'}, 'scheduler': {'value': 'PF'}, 'handover_algo': {'value': 'A2A4Rsrq'}, 'micro_num_bs': {'value': '3'}, 'micro_layer_ues': {'value': '60'}, 'delay_app_installed': {'value': '1'}, 'delay_pkt_interval_seconds': {'value': '+0.1s'}, 'rtt_app_installed': {'value': '0'}, 'http_app_installed': {'value': '1'}, 'dash_app_installed': {'value': '1'}, 'vr_app_installed': {'value': '1'}}\n",
      "separate_macro_micro  True\n",
      "total_num_cells 6\n",
      "total_num_ues 90\n",
      "sim_time 1000\n",
      "Macro UE IMSIs:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
      "Micro UE IMSIs:  [31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90]\n",
      "Macro fast UE IMSIs:  [1, 2, 3, 4, 5, 6]\n",
      "Macro Slow UE IMSIs:  [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
      "Only delay IMSIs:  [3, 5, 7, 9, 13, 15, 17, 19, 23, 25, 27, 29, 33, 35, 37, 39, 43, 45, 47, 49, 53, 55, 57, 59, 63, 65, 67, 69, 73, 75, 77, 79, 83, 85, 87]\n",
      "Macro CellIds:  [1, 2, 3]\n",
      "Micro CellIds:  [4, 5, 6]\n",
      "--------------------------------------------\n",
      "mobility_trace.txt\n",
      "../../dataset_ver1/run17/mobility_trace.txt\n",
      "time to read file:  0.15288782119750977\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.5 ,  1001.0 )\n",
      "log runtime: 1000.5  seconds\n",
      "--------------------------------------------\n",
      "dashClient_trace.txt\n",
      "../../dataset_ver1/run17/dashClient_trace.txt\n",
      "time to read file:  0.008933544158935547\n",
      "ueIds: min: 1 max: 84 count: 28\n",
      "log time (start, end): ( 0.563999 ,  1000.574999 )\n",
      "log runtime: 1000.0110000000001  seconds\n",
      "video streaming IMSIs:  [1, 6, 11, 16, 21, 26, 32, 34, 38, 40, 42, 44, 48, 50, 52, 54, 58, 60, 62, 64, 68, 70, 72, 74, 78, 80, 82, 84]\n",
      "                              dashClient_trace.txt_newBitRate_bps  \\\n",
      "IMSI wind_tstamp                                                    \n",
      "1    1970-01-01 00:00:01.000                                  NaN   \n",
      "     1970-01-01 00:00:01.500                                  NaN   \n",
      "\n",
      "                              dashClient_trace.txt_oldBitRate_bps  \n",
      "IMSI wind_tstamp                                                   \n",
      "1    1970-01-01 00:00:01.000                                  NaN  \n",
      "     1970-01-01 00:00:01.500                                  NaN  \n",
      "time to parse file:  0.28380918502807617\n",
      "--------------------------------------------\n",
      "delay_trace.txt\n",
      "../../dataset_ver1/run17/delay_trace.txt\n",
      "time to read file:  1.141143560409546\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.510999 ,  1000.915931 )\n",
      "log runtime: 1000.404932  seconds\n",
      "time to parse file:  9.7252357006073\n",
      "--------------------------------------------\n",
      "vrFragment_trace.txt\n",
      "../../dataset_ver1/run17/vrFragment_trace.txt\n",
      "time to read file:  0.526958703994751\n",
      "ueIds: min: 30 max: 90 count: 4\n",
      "log time (start, end): ( 0.503999 ,  563.478999 )\n",
      "log runtime: 562.975  seconds\n",
      "VR IMSIs:  [30, 88, 89, 90]\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "---------------------------------------------------------\n",
      "time to parse file:  16.33970022201538\n",
      "--------------------------------------------\n",
      "UlSinrStats.txt\n",
      "../../dataset_ver1/run17/UlSinrStats.txt\n",
      "time to read file:  0.38761234283447266\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.321 ,  1001.05 )\n",
      "log runtime: 1000.7289999999999  seconds\n",
      "time to parse file:  164.66257119178772\n",
      "--------------------------------------------\n",
      "DlRsrpSinrStats.txt\n",
      "../../dataset_ver1/run17/DlRsrpSinrStats.txt\n",
      "time to read file:  0.9499101638793945\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.239214 ,  1001.04 )\n",
      "log runtime: 1000.800786  seconds\n",
      "time to parse file:  88.31286334991455\n",
      "--------------------------------------------\n",
      "UlTxPhyStats.txt\n",
      "../../dataset_ver1/run17/UlTxPhyStats.txt\n",
      "time to read file:  3.867464065551758\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.513 ,  1001.053 )\n",
      "log runtime: 1000.54  seconds\n",
      "time to parse file:  142.16088199615479\n",
      "--------------------------------------------\n",
      "UlRxPhyStats.txt\n",
      "../../dataset_ver1/run17/UlRxPhyStats.txt\n",
      "time to read file:  4.077048063278198\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.513 ,  1001.049 )\n",
      "log runtime: 1000.536  seconds\n",
      "time to parse file:  142.78530859947205\n",
      "--------------------------------------------\n",
      "DlTxPhyStats.txt\n",
      "../../dataset_ver1/run17/DlTxPhyStats.txt\n",
      "time to read file:  2.686282157897949\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.503 ,  1001.049 )\n",
      "log runtime: 1000.5459999999999  seconds\n",
      "time to parse file:  124.786794424057\n",
      "--------------------------------------------\n",
      "DlRxPhyStats.txt\n",
      "../../dataset_ver1/run17/DlRxPhyStats.txt\n",
      "time to read file:  3.1526577472686768\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.503 ,  1001.049 )\n",
      "log runtime: 1000.5459999999999  seconds\n",
      "time to parse file:  130.14581513404846\n",
      "--------------------------------------------\n",
      "UlMacStats.txt\n",
      "../../dataset_ver1/run17/UlMacStats.txt\n",
      "time to read file:  3.9559834003448486\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.507 ,  1001.05 )\n",
      "log runtime: 1000.543  seconds\n",
      "time to parse file:  139.92716479301453\n",
      "--------------------------------------------\n",
      "DlMacStats.txt\n",
      "../../dataset_ver1/run17/DlMacStats.txt\n",
      "time to read file:  3.2128918170928955\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.501 ,  1001.05 )\n",
      "log runtime: 1000.549  seconds\n",
      "time to parse file:  128.4065363407135\n",
      "--------------------------------------------\n",
      "UlRlcStats.txt\n",
      "../../dataset_ver1/run17/UlRlcStats.txt\n",
      "time to read file:  0.4140162467956543\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.5 ,  1001.0 )\n",
      "log runtime: 1000.5  seconds\n",
      "time to parse file:  82.80953121185303\n",
      "--------------------------------------------\n",
      "DlRlcStats.txt\n",
      "../../dataset_ver1/run17/DlRlcStats.txt\n",
      "time to read file:  0.41565465927124023\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.5 ,  1001.0 )\n",
      "log runtime: 1000.5  seconds\n",
      "time to parse file:  84.17589831352234\n",
      "--------------------------------------------\n",
      "UlPdcpStats.txt\n",
      "../../dataset_ver1/run17/UlPdcpStats.txt\n",
      "time to read file:  0.406113862991333\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.5 ,  1001.0 )\n",
      "log runtime: 1000.5  seconds\n",
      "time to parse file:  84.0495445728302\n",
      "--------------------------------------------\n",
      "DlPdcpStats.txt\n",
      "../../dataset_ver1/run17/DlPdcpStats.txt\n",
      "time to read file:  0.4189934730529785\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.5 ,  1001.0 )\n",
      "log runtime: 1000.5  seconds\n",
      "time to parse file:  83.96992564201355\n",
      "============================================================\n",
      "======================= RUN DONE =====================================\n",
      "============================================================\n",
      "../../dataset_ver1/run18\n",
      "============================================================\n",
      "run18\n",
      "{'macro_rings': {'value': '0'}, 'macro_num_bs': {'value': '3'}, 'macro_layer_ues': {'value': '30'}, 'simulation_time_seconds': {'value': '1000'}, 'rand_seed': {'value': '17'}, 'create_micro_layer': {'value': '1'}, 'scheduler': {'value': 'PF'}, 'handover_algo': {'value': 'A2A4Rsrq'}, 'micro_num_bs': {'value': '3'}, 'micro_layer_ues': {'value': '60'}, 'delay_app_installed': {'value': '1'}, 'delay_pkt_interval_seconds': {'value': '+0.1s'}, 'rtt_app_installed': {'value': '0'}, 'http_app_installed': {'value': '1'}, 'dash_app_installed': {'value': '1'}, 'vr_app_installed': {'value': '1'}}\n",
      "separate_macro_micro  True\n",
      "total_num_cells 6\n",
      "total_num_ues 90\n",
      "sim_time 1000\n",
      "Macro UE IMSIs:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
      "Micro UE IMSIs:  [31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90]\n",
      "Macro fast UE IMSIs:  [1, 2, 3, 4, 5, 6]\n",
      "Macro Slow UE IMSIs:  [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
      "Only delay IMSIs:  [3, 5, 7, 9, 13, 15, 17, 19, 23, 25, 27, 29, 33, 35, 37, 39, 43, 45, 47, 49, 53, 55, 57, 59, 63, 65, 67, 69, 73, 75, 77, 79, 83, 85, 87]\n",
      "Macro CellIds:  [1, 2, 3]\n",
      "Micro CellIds:  [4, 5, 6]\n",
      "--------------------------------------------\n",
      "mobility_trace.txt\n",
      "../../dataset_ver1/run18/mobility_trace.txt\n",
      "time to read file:  0.13889145851135254\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.5 ,  1001.0 )\n",
      "log runtime: 1000.5  seconds\n",
      "--------------------------------------------\n",
      "dashClient_trace.txt\n",
      "../../dataset_ver1/run18/dashClient_trace.txt\n",
      "time to read file:  0.008675575256347656\n",
      "ueIds: min: 1 max: 84 count: 28\n",
      "log time (start, end): ( 0.704999 ,  1000.842999 )\n",
      "log runtime: 1000.1379999999999  seconds\n",
      "video streaming IMSIs:  [1, 6, 11, 16, 21, 26, 32, 34, 38, 40, 42, 44, 48, 50, 52, 54, 58, 60, 62, 64, 68, 70, 72, 74, 78, 80, 82, 84]\n",
      "                              dashClient_trace.txt_newBitRate_bps  \\\n",
      "IMSI wind_tstamp                                                    \n",
      "1    1970-01-01 00:00:01.000                                  NaN   \n",
      "     1970-01-01 00:00:01.500                            3079000.0   \n",
      "\n",
      "                              dashClient_trace.txt_oldBitRate_bps  \n",
      "IMSI wind_tstamp                                                   \n",
      "1    1970-01-01 00:00:01.000                                  NaN  \n",
      "     1970-01-01 00:00:01.500                            1547000.0  \n",
      "time to parse file:  0.27752089500427246\n",
      "--------------------------------------------\n",
      "delay_trace.txt\n",
      "../../dataset_ver1/run18/delay_trace.txt\n",
      "time to read file:  1.1542394161224365\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.513931 ,  1000.992931 )\n",
      "log runtime: 1000.479  seconds\n",
      "time to parse file:  9.479855298995972\n",
      "--------------------------------------------\n",
      "vrFragment_trace.txt\n",
      "../../dataset_ver1/run18/vrFragment_trace.txt\n",
      "time to read file:  0.6362342834472656\n",
      "ueIds: min: 30 max: 90 count: 4\n",
      "log time (start, end): ( 0.503999 ,  561.976999 )\n",
      "log runtime: 561.473  seconds\n",
      "VR IMSIs:  [30, 88, 89, 90]\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "---------------------------------------------------------\n",
      "time to parse file:  16.9008629322052\n",
      "--------------------------------------------\n",
      "UlSinrStats.txt\n",
      "../../dataset_ver1/run18/UlSinrStats.txt\n",
      "time to read file:  0.35002636909484863\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.321 ,  1001.05 )\n",
      "log runtime: 1000.7289999999999  seconds\n",
      "time to parse file:  164.68727922439575\n",
      "--------------------------------------------\n",
      "DlRsrpSinrStats.txt\n",
      "../../dataset_ver1/run18/DlRsrpSinrStats.txt\n",
      "time to read file:  0.9716479778289795\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.239214 ,  1001.04 )\n",
      "log runtime: 1000.800786  seconds\n",
      "time to parse file:  88.57584023475647\n",
      "--------------------------------------------\n",
      "UlTxPhyStats.txt\n",
      "../../dataset_ver1/run18/UlTxPhyStats.txt\n",
      "time to read file:  4.360623598098755\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.513 ,  1001.053 )\n",
      "log runtime: 1000.54  seconds\n",
      "time to parse file:  150.79832530021667\n",
      "--------------------------------------------\n",
      "UlRxPhyStats.txt\n",
      "../../dataset_ver1/run18/UlRxPhyStats.txt\n",
      "time to read file:  4.603007793426514\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.513 ,  1001.049 )\n",
      "log runtime: 1000.536  seconds\n",
      "time to parse file:  152.40231895446777\n",
      "--------------------------------------------\n",
      "DlTxPhyStats.txt\n",
      "../../dataset_ver1/run18/DlTxPhyStats.txt\n",
      "time to read file:  2.6603739261627197\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.503 ,  1001.049 )\n",
      "log runtime: 1000.5459999999999  seconds\n",
      "time to parse file:  125.69145464897156\n",
      "--------------------------------------------\n",
      "DlRxPhyStats.txt\n",
      "../../dataset_ver1/run18/DlRxPhyStats.txt\n",
      "time to read file:  3.192711114883423\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.503 ,  1001.049 )\n",
      "log runtime: 1000.5459999999999  seconds\n",
      "time to parse file:  132.27604913711548\n",
      "--------------------------------------------\n",
      "UlMacStats.txt\n",
      "../../dataset_ver1/run18/UlMacStats.txt\n",
      "time to read file:  4.385511159896851\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.507 ,  1001.05 )\n",
      "log runtime: 1000.543  seconds\n",
      "time to parse file:  146.85696935653687\n",
      "--------------------------------------------\n",
      "DlMacStats.txt\n",
      "../../dataset_ver1/run18/DlMacStats.txt\n",
      "time to read file:  3.153164863586426\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.501 ,  1001.05 )\n",
      "log runtime: 1000.549  seconds\n",
      "time to parse file:  129.11523699760437\n",
      "--------------------------------------------\n",
      "UlRlcStats.txt\n",
      "../../dataset_ver1/run18/UlRlcStats.txt\n",
      "time to read file:  0.39944911003112793\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.5 ,  1001.0 )\n",
      "log runtime: 1000.5  seconds\n",
      "time to parse file:  82.11196637153625\n",
      "--------------------------------------------\n",
      "DlRlcStats.txt\n",
      "../../dataset_ver1/run18/DlRlcStats.txt\n",
      "time to read file:  0.37506556510925293\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.5 ,  1001.0 )\n",
      "log runtime: 1000.5  seconds\n",
      "time to parse file:  84.07676148414612\n",
      "--------------------------------------------\n",
      "UlPdcpStats.txt\n",
      "../../dataset_ver1/run18/UlPdcpStats.txt\n",
      "time to read file:  0.4224700927734375\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.5 ,  1001.0 )\n",
      "log runtime: 1000.5  seconds\n",
      "time to parse file:  83.7325587272644\n",
      "--------------------------------------------\n",
      "DlPdcpStats.txt\n",
      "../../dataset_ver1/run18/DlPdcpStats.txt\n",
      "time to read file:  0.4234950542449951\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.5 ,  1001.0 )\n",
      "log runtime: 1000.5  seconds\n",
      "time to parse file:  80.31644678115845\n",
      "============================================================\n",
      "======================= RUN DONE =====================================\n",
      "============================================================\n",
      "../../dataset_ver1/run19\n",
      "============================================================\n",
      "run19\n",
      "{'macro_rings': {'value': '0'}, 'macro_num_bs': {'value': '3'}, 'macro_layer_ues': {'value': '30'}, 'simulation_time_seconds': {'value': '1000'}, 'rand_seed': {'value': '18'}, 'create_micro_layer': {'value': '1'}, 'scheduler': {'value': 'PF'}, 'handover_algo': {'value': 'A2A4Rsrq'}, 'micro_num_bs': {'value': '3'}, 'micro_layer_ues': {'value': '60'}, 'delay_app_installed': {'value': '1'}, 'delay_pkt_interval_seconds': {'value': '+0.1s'}, 'rtt_app_installed': {'value': '0'}, 'http_app_installed': {'value': '1'}, 'dash_app_installed': {'value': '1'}, 'vr_app_installed': {'value': '1'}}\n",
      "separate_macro_micro  True\n",
      "total_num_cells 6\n",
      "total_num_ues 90\n",
      "sim_time 1000\n",
      "Macro UE IMSIs:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
      "Micro UE IMSIs:  [31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90]\n",
      "Macro fast UE IMSIs:  [1, 2, 3, 4, 5, 6]\n",
      "Macro Slow UE IMSIs:  [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
      "Only delay IMSIs:  [3, 5, 7, 9, 13, 15, 17, 19, 23, 25, 27, 29, 33, 35, 37, 39, 43, 45, 47, 49, 53, 55, 57, 59, 63, 65, 67, 69, 73, 75, 77, 79, 83, 85, 87]\n",
      "Macro CellIds:  [1, 2, 3]\n",
      "Micro CellIds:  [4, 5, 6]\n",
      "--------------------------------------------\n",
      "mobility_trace.txt\n",
      "../../dataset_ver1/run19/mobility_trace.txt\n",
      "time to read file:  0.13203740119934082\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.5 ,  1001.0 )\n",
      "log runtime: 1000.5  seconds\n",
      "--------------------------------------------\n",
      "dashClient_trace.txt\n",
      "../../dataset_ver1/run19/dashClient_trace.txt\n",
      "time to read file:  0.008840560913085938\n",
      "ueIds: min: 1 max: 84 count: 28\n",
      "log time (start, end): ( 0.576999 ,  1000.868999 )\n",
      "log runtime: 1000.292  seconds\n",
      "video streaming IMSIs:  [1, 6, 11, 16, 21, 26, 32, 34, 38, 40, 42, 44, 48, 50, 52, 54, 58, 60, 62, 64, 68, 70, 72, 74, 78, 80, 82, 84]\n",
      "                              dashClient_trace.txt_newBitRate_bps  \\\n",
      "IMSI wind_tstamp                                                    \n",
      "1    1970-01-01 00:00:01.000                                  NaN   \n",
      "     1970-01-01 00:00:01.500                                  NaN   \n",
      "\n",
      "                              dashClient_trace.txt_oldBitRate_bps  \n",
      "IMSI wind_tstamp                                                   \n",
      "1    1970-01-01 00:00:01.000                                  NaN  \n",
      "     1970-01-01 00:00:01.500                                  NaN  \n",
      "time to parse file:  0.2787504196166992\n",
      "--------------------------------------------\n",
      "delay_trace.txt\n",
      "../../dataset_ver1/run19/delay_trace.txt\n",
      "time to read file:  1.0323855876922607\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.511999 ,  1000.912931 )\n",
      "log runtime: 1000.400932  seconds\n",
      "time to parse file:  11.176689624786377\n",
      "--------------------------------------------\n",
      "vrFragment_trace.txt\n",
      "../../dataset_ver1/run19/vrFragment_trace.txt\n",
      "time to read file:  2.4105112552642822\n",
      "ueIds: min: 30 max: 90 count: 4\n",
      "log time (start, end): ( 0.503999 ,  560.860999 )\n",
      "log runtime: 560.357  seconds\n",
      "VR IMSIs:  [30, 88, 89, 90]\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "---------------------------------------------------------\n",
      "time to parse file:  18.535383224487305\n",
      "--------------------------------------------\n",
      "UlSinrStats.txt\n",
      "../../dataset_ver1/run19/UlSinrStats.txt\n",
      "time to read file:  0.35312843322753906\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.321 ,  1001.05 )\n",
      "log runtime: 1000.7289999999999  seconds\n",
      "time to parse file:  156.10898423194885\n",
      "--------------------------------------------\n",
      "DlRsrpSinrStats.txt\n",
      "../../dataset_ver1/run19/DlRsrpSinrStats.txt\n",
      "time to read file:  0.9681210517883301\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.239214 ,  1001.04 )\n",
      "log runtime: 1000.800786  seconds\n",
      "time to parse file:  84.29846143722534\n",
      "--------------------------------------------\n",
      "UlTxPhyStats.txt\n",
      "../../dataset_ver1/run19/UlTxPhyStats.txt\n",
      "time to read file:  4.123436450958252\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.514 ,  1001.014 )\n",
      "log runtime: 1000.5  seconds\n",
      "time to parse file:  141.94554162025452\n",
      "--------------------------------------------\n",
      "UlRxPhyStats.txt\n",
      "../../dataset_ver1/run19/UlRxPhyStats.txt\n",
      "time to read file:  4.696033477783203\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.514 ,  1001.014 )\n",
      "log runtime: 1000.5  seconds\n",
      "time to parse file:  143.9644136428833\n",
      "--------------------------------------------\n",
      "DlTxPhyStats.txt\n",
      "../../dataset_ver1/run19/DlTxPhyStats.txt\n",
      "time to read file:  2.5466012954711914\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.503 ,  1001.017 )\n",
      "log runtime: 1000.514  seconds\n",
      "time to parse file:  117.83094096183777\n",
      "--------------------------------------------\n",
      "DlRxPhyStats.txt\n",
      "../../dataset_ver1/run19/DlRxPhyStats.txt\n",
      "time to read file:  3.0168099403381348\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.503 ,  1001.017 )\n",
      "log runtime: 1000.514  seconds\n",
      "time to parse file:  123.90153861045837\n",
      "--------------------------------------------\n",
      "UlMacStats.txt\n",
      "../../dataset_ver1/run19/UlMacStats.txt\n",
      "time to read file:  4.187591791152954\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.508 ,  1001.01 )\n",
      "log runtime: 1000.502  seconds\n",
      "time to parse file:  138.574049949646\n",
      "--------------------------------------------\n",
      "DlMacStats.txt\n",
      "../../dataset_ver1/run19/DlMacStats.txt\n",
      "time to read file:  2.9614174365997314\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.501 ,  1001.01 )\n",
      "log runtime: 1000.509  seconds\n",
      "time to parse file:  121.1230719089508\n",
      "--------------------------------------------\n",
      "UlRlcStats.txt\n",
      "../../dataset_ver1/run19/UlRlcStats.txt\n",
      "time to read file:  0.3907146453857422\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.5 ,  1001.0 )\n",
      "log runtime: 1000.5  seconds\n",
      "time to parse file:  78.02457666397095\n",
      "--------------------------------------------\n",
      "DlRlcStats.txt\n",
      "../../dataset_ver1/run19/DlRlcStats.txt\n",
      "time to read file:  0.38538312911987305\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.5 ,  1001.0 )\n",
      "log runtime: 1000.5  seconds\n",
      "time to parse file:  79.72102475166321\n",
      "--------------------------------------------\n",
      "UlPdcpStats.txt\n",
      "../../dataset_ver1/run19/UlPdcpStats.txt\n",
      "time to read file:  0.4035451412200928\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.5 ,  1001.0 )\n",
      "log runtime: 1000.5  seconds\n",
      "time to parse file:  79.40949869155884\n",
      "--------------------------------------------\n",
      "DlPdcpStats.txt\n",
      "../../dataset_ver1/run19/DlPdcpStats.txt\n",
      "time to read file:  0.38660550117492676\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.5 ,  1001.0 )\n",
      "log runtime: 1000.5  seconds\n",
      "time to parse file:  80.06833100318909\n",
      "============================================================\n",
      "======================= RUN DONE =====================================\n",
      "============================================================\n",
      "../../dataset_ver1/run20\n",
      "============================================================\n",
      "run20\n",
      "{'macro_rings': {'value': '0'}, 'macro_num_bs': {'value': '3'}, 'macro_layer_ues': {'value': '30'}, 'simulation_time_seconds': {'value': '1000'}, 'rand_seed': {'value': '19'}, 'create_micro_layer': {'value': '1'}, 'scheduler': {'value': 'PF'}, 'handover_algo': {'value': 'A2A4Rsrq'}, 'micro_num_bs': {'value': '3'}, 'micro_layer_ues': {'value': '60'}, 'delay_app_installed': {'value': '1'}, 'delay_pkt_interval_seconds': {'value': '+0.1s'}, 'rtt_app_installed': {'value': '0'}, 'http_app_installed': {'value': '1'}, 'dash_app_installed': {'value': '1'}, 'vr_app_installed': {'value': '1'}}\n",
      "separate_macro_micro  True\n",
      "total_num_cells 6\n",
      "total_num_ues 90\n",
      "sim_time 1000\n",
      "Macro UE IMSIs:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
      "Micro UE IMSIs:  [31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90]\n",
      "Macro fast UE IMSIs:  [1, 2, 3, 4, 5, 6]\n",
      "Macro Slow UE IMSIs:  [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
      "Only delay IMSIs:  [3, 5, 7, 9, 13, 15, 17, 19, 23, 25, 27, 29, 33, 35, 37, 39, 43, 45, 47, 49, 53, 55, 57, 59, 63, 65, 67, 69, 73, 75, 77, 79, 83, 85, 87]\n",
      "Macro CellIds:  [1, 2, 3]\n",
      "Micro CellIds:  [4, 5, 6]\n",
      "--------------------------------------------\n",
      "mobility_trace.txt\n",
      "../../dataset_ver1/run20/mobility_trace.txt\n",
      "time to read file:  0.14441967010498047\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.5 ,  1001.0 )\n",
      "log runtime: 1000.5  seconds\n",
      "--------------------------------------------\n",
      "dashClient_trace.txt\n",
      "../../dataset_ver1/run20/dashClient_trace.txt\n",
      "time to read file:  0.017343997955322266\n",
      "ueIds: min: 1 max: 84 count: 28\n",
      "log time (start, end): ( 0.566999 ,  1000.752999 )\n",
      "log runtime: 1000.186  seconds\n",
      "video streaming IMSIs:  [1, 6, 11, 16, 21, 26, 32, 34, 38, 40, 42, 44, 48, 50, 52, 54, 58, 60, 62, 64, 68, 70, 72, 74, 78, 80, 82, 84]\n",
      "                              dashClient_trace.txt_newBitRate_bps  \\\n",
      "IMSI wind_tstamp                                                    \n",
      "1    1970-01-01 00:00:01.000                                  NaN   \n",
      "     1970-01-01 00:00:01.500                            3840000.0   \n",
      "\n",
      "                              dashClient_trace.txt_oldBitRate_bps  \n",
      "IMSI wind_tstamp                                                   \n",
      "1    1970-01-01 00:00:01.000                                  NaN  \n",
      "     1970-01-01 00:00:01.500                            2484000.0  \n",
      "time to parse file:  0.28292369842529297\n",
      "--------------------------------------------\n",
      "delay_trace.txt\n",
      "../../dataset_ver1/run20/delay_trace.txt\n",
      "time to read file:  1.092597484588623\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.508999 ,  1001.031931 )\n",
      "log runtime: 1000.522932  seconds\n",
      "time to parse file:  9.941975831985474\n",
      "--------------------------------------------\n",
      "vrFragment_trace.txt\n",
      "../../dataset_ver1/run20/vrFragment_trace.txt\n",
      "time to read file:  0.7399768829345703\n",
      "ueIds: min: 30 max: 90 count: 4\n",
      "log time (start, end): ( 0.503999 ,  560.971999 )\n",
      "log runtime: 560.468  seconds\n",
      "VR IMSIs:  [30, 88, 89, 90]\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "---------------------------------------------------------\n",
      "time to parse file:  16.119831562042236\n",
      "--------------------------------------------\n",
      "UlSinrStats.txt\n",
      "../../dataset_ver1/run20/UlSinrStats.txt\n",
      "time to read file:  0.3475346565246582\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.321 ,  1001.05 )\n",
      "log runtime: 1000.7289999999999  seconds\n",
      "time to parse file:  156.39145731925964\n",
      "--------------------------------------------\n",
      "DlRsrpSinrStats.txt\n",
      "../../dataset_ver1/run20/DlRsrpSinrStats.txt\n",
      "time to read file:  0.9490406513214111\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.239214 ,  1001.04 )\n",
      "log runtime: 1000.800786  seconds\n",
      "time to parse file:  84.00249886512756\n",
      "--------------------------------------------\n",
      "UlTxPhyStats.txt\n",
      "../../dataset_ver1/run20/UlTxPhyStats.txt\n",
      "time to read file:  4.129205226898193\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.513 ,  1001.053 )\n",
      "log runtime: 1000.54  seconds\n",
      "time to parse file:  142.97781324386597\n",
      "--------------------------------------------\n",
      "UlRxPhyStats.txt\n",
      "../../dataset_ver1/run20/UlRxPhyStats.txt\n",
      "time to read file:  4.646538496017456\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.513 ,  1001.049 )\n",
      "log runtime: 1000.536  seconds\n",
      "time to parse file:  143.6586446762085\n",
      "--------------------------------------------\n",
      "DlTxPhyStats.txt\n",
      "../../dataset_ver1/run20/DlTxPhyStats.txt\n",
      "time to read file:  2.584118127822876\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.503 ,  1001.049 )\n",
      "log runtime: 1000.5459999999999  seconds\n",
      "time to parse file:  118.71092295646667\n",
      "--------------------------------------------\n",
      "DlRxPhyStats.txt\n",
      "../../dataset_ver1/run20/DlRxPhyStats.txt\n",
      "time to read file:  3.2981410026550293\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.503 ,  1001.049 )\n",
      "log runtime: 1000.5459999999999  seconds\n",
      "time to parse file:  124.63008618354797\n",
      "--------------------------------------------\n",
      "UlMacStats.txt\n",
      "../../dataset_ver1/run20/UlMacStats.txt\n",
      "time to read file:  4.115903377532959\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.507 ,  1001.05 )\n",
      "log runtime: 1000.543  seconds\n",
      "time to parse file:  139.78383469581604\n",
      "--------------------------------------------\n",
      "DlMacStats.txt\n",
      "../../dataset_ver1/run20/DlMacStats.txt\n",
      "time to read file:  3.330258369445801\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.501 ,  1001.05 )\n",
      "log runtime: 1000.549  seconds\n",
      "time to parse file:  122.15180230140686\n",
      "--------------------------------------------\n",
      "UlRlcStats.txt\n",
      "../../dataset_ver1/run20/UlRlcStats.txt\n",
      "time to read file:  0.3882441520690918\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.5 ,  1001.0 )\n",
      "log runtime: 1000.5  seconds\n",
      "time to parse file:  78.24761581420898\n",
      "--------------------------------------------\n",
      "DlRlcStats.txt\n",
      "../../dataset_ver1/run20/DlRlcStats.txt\n",
      "time to read file:  0.37233972549438477\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.5 ,  1001.0 )\n",
      "log runtime: 1000.5  seconds\n",
      "time to parse file:  79.80309796333313\n",
      "--------------------------------------------\n",
      "UlPdcpStats.txt\n",
      "../../dataset_ver1/run20/UlPdcpStats.txt\n",
      "time to read file:  0.3935546875\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.5 ,  1001.0 )\n",
      "log runtime: 1000.5  seconds\n",
      "time to parse file:  79.84138345718384\n",
      "--------------------------------------------\n",
      "DlPdcpStats.txt\n",
      "../../dataset_ver1/run20/DlPdcpStats.txt\n",
      "time to read file:  0.42658138275146484\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.5 ,  1001.0 )\n",
      "log runtime: 1000.5  seconds\n",
      "time to parse file:  79.82670783996582\n",
      "============================================================\n",
      "======================= RUN DONE =====================================\n",
      "============================================================\n",
      "======================= ALL DONE =====================================\n"
     ]
    }
   ],
   "source": [
    "plt.rcParams[\"figure.autolayout\"] = False\n",
    "#=======================================\n",
    "# Initilizing empty lists \n",
    "#=======================================\n",
    "# USe 15 runs for pretraining and fine tuning and 5 runs for testing \n",
    "runs = range(16, 20 + 1)\n",
    "skip_runs = []\n",
    "\n",
    "# constant multipliers\n",
    "M = (10**6)\n",
    "K = (10**3)\n",
    "\n",
    "\n",
    "# This dataframe shall aggregate the logs over all the different files and horzcat it over files and then vertcat it over runs\n",
    "master_df = pd.DataFrame()\n",
    "\n",
    "#=======================================\n",
    "# Iterate over runs and files \n",
    "#=======================================\n",
    "\n",
    "#=================================================================\n",
    "#use_runs = [13]\n",
    "#use_runs = range(1,5)\n",
    "#use_runs_paths = [(data_dir+'run'+str(r)) for r in use_runs]\n",
    "\n",
    "\n",
    "#use_runs_paths = glob.glob(data_dir+'run*')\n",
    "#for run in tqdm(use_runs_paths):\n",
    "\n",
    "#for run in [data_dir+'run'+str(3)]:\n",
    "for run in [data_dir+'run'+str(r) for r in runs]:\n",
    "    \n",
    "#=================================================================\n",
    "    print('============================================================')\n",
    "    print (run)\n",
    "    print('============================================================')\n",
    "    only_run = run.split('/')[-1]\n",
    "    print(only_run)\n",
    "    # check if run ran to completion\n",
    "    # Don't need this anymore since we now have fixed the bug that would make it crash\n",
    "    #with open(run+'/simulation_info.txt', \"r\") as sim_info_file:\n",
    "    #    last_line = sim_info_file.readlines()[-1]\n",
    "    #    print(last_line)\n",
    "    #    if 'Elapsed wall clock' not in last_line:\n",
    "    #        print('This run did not finish to completion, skipping it')\n",
    "    #        skip_runs.append(only_run)\n",
    "    #        print(skip_runs)\n",
    "    #        continue\n",
    "    \n",
    "    sim_info = pd.read_csv(run+'/sim_info.txt', sep=',').set_index('parameter').to_dict('index')\n",
    "    # Specify here topology details from the dataset we are using \n",
    "    # This can be infered from the files, but this is easier\n",
    "    print(sim_info)\n",
    "    separate_macro_micro = bool(int(sim_info['create_micro_layer']['value'])) \n",
    "    print('separate_macro_micro ', separate_macro_micro)\n",
    "    total_num_cells=(int(sim_info['macro_num_bs']['value'])+int(sim_info['micro_num_bs']['value'])) if separate_macro_micro else int(sim_info['macro_num_bs']['value']) \n",
    "    print('total_num_cells', total_num_cells)\n",
    "    total_num_ues=int(sim_info['macro_layer_ues']['value'])+int(sim_info['micro_layer_ues']['value']) if separate_macro_micro else int(sim_info['macro_layer_ues']['value'])\n",
    "    print('total_num_ues', total_num_ues)\n",
    "    sim_time = int(sim_info['simulation_time_seconds']['value']) # seconds\n",
    "    print('sim_time', sim_time)\n",
    "    \n",
    "    sim_start_time = pd.to_datetime(1.0, unit='s', origin='unix')\n",
    "    sim_end_time = pd.to_datetime(sim_time, unit='s', origin='unix')\n",
    "\n",
    "    dict_gnb_pos = pd.read_csv(run+'/gnb_locations.txt', sep=',').set_index('cellId').to_dict('index')\n",
    "    dict_gnb_pos[0] = {'gnbpos_x': np.nan, 'gnbpos_y': np.nan, 'gnbpos_z': np.nan}\n",
    "    #print(dict_gnb_pos)\n",
    "    # read UE group list\n",
    "    ue_groups = open(run+'/ue_gnb_groups.txt').readlines()\n",
    "    macro_imsis = [int(i) for i in ue_groups[0].split(',')[1:-1]]\n",
    "    print(\"Macro UE IMSIs: \", macro_imsis)\n",
    "    micro_imsis = list(set(range(1, total_num_ues+1)) - set(macro_imsis))\n",
    "    print(\"Micro UE IMSIs: \", micro_imsis)\n",
    "    fast_imsis = [int(i) for i in ue_groups[1].split(',')[1:-1]]\n",
    "    print(\"Macro fast UE IMSIs: \", fast_imsis)\n",
    "    slow_imsis = list(set(range(1, total_num_ues+1)) - set(fast_imsis) - set(micro_imsis))\n",
    "    print(\"Macro Slow UE IMSIs: \", slow_imsis)\n",
    "    #only_delay_imsis=[3,5,7,9,13,15,17,19,23,55,57]\n",
    "    only_delay_imsis=[int(i) for i in ue_groups[2].split(',')[1:-1]]\n",
    "    print(\"Only delay IMSIs: \", only_delay_imsis)\n",
    "\n",
    "    \n",
    "    # Read these from the ue_groups file later and also rename ue_groups to something else \n",
    "    macro_cells = [1,2,3]\n",
    "    print('Macro CellIds: ', macro_cells)\n",
    "    micro_cells = [4,5,6]\n",
    "    print('Micro CellIds: ', micro_cells)\n",
    "    \n",
    "    vr_imsi=np.empty(0)\n",
    "    dash_imsi=np.empty(0)\n",
    "    http_imsi=np.empty(0)\n",
    "    \n",
    "    df_mi_cellId = None\n",
    "    df_cellId = None\n",
    "    \n",
    "    # This dataframe shall aggregate the parsed features from all the different files \n",
    "    # before vertically concatenating to master_df \n",
    "    per_run_df = pd.DataFrame()\n",
    "    # stored from the first RAN file processed \n",
    "    basic_info = pd.DataFrame(columns=['IMSI', 'cellId', 'conn_imsi_count'])\n",
    "    first_ran_file = True \n",
    "    \n",
    "    #chosen_imsis = None\n",
    "#=================================================================\n",
    "    for file in files:\n",
    "#=================================================================\n",
    "        print('--------------------------------------------')\n",
    "        print(file)\n",
    "        start_time = time.time()\n",
    "        \n",
    "        #=======================================\n",
    "        # Preprocess logs \n",
    "        #=======================================\n",
    "        \n",
    "        ## Read file while fixing the tab issue\n",
    "        if file in files_with_trailing_tab:\n",
    "            #df = pd.read_csv(data_dir+run+'/'+file, sep='\\t', usecols=range(0,18))\n",
    "            df = pd.read_csv(run+'/'+file, sep='\\t', usecols=range(0,18))\n",
    "            print(run+'/'+file)\n",
    "        else:\n",
    "            #df = pd.read_csv(data_dir+run+'/'+file, sep='\\t')\n",
    "            df = pd.read_csv(run+'/'+file, sep='\\t')\n",
    "            print(run+'/'+file)\n",
    "        \n",
    "        print('time to read file: ', (time.time() - start_time))\n",
    "        \n",
    "        \n",
    "        ## Do some file specific preprocessing\n",
    "        ## Make uniform the timestamp units convert them all to micro seconds \n",
    "        if '% time' in df.columns:\n",
    "            df.rename(columns = {'% time':'tstamp_us'}, inplace = True)\n",
    "            if file_name_to_tstamp_unit[file] == 'ms':\n",
    "                df['tstamp_us'] = df['tstamp_us']*K\n",
    "            elif file_name_to_tstamp_unit[file] == 's':\n",
    "                df['tstamp_us'] = df['tstamp_us']*M\n",
    "        \n",
    "        ## Make uniform the timestamp units        \n",
    "        if '% start' in df.columns:\n",
    "            ## TO DO: check if this is actually micro seconds. I think it is seconds   \n",
    "            df.rename(columns = {'% start':'tstamp_us'}, inplace = True)\n",
    "            df.rename(columns = {'end':'end_timeslot_us'}, inplace = True)\n",
    "            df['tstamp_us'] = df['tstamp_us']*M\n",
    "            df['end_timeslot_us'] = df['end_timeslot_us']*M\n",
    "        \n",
    "        ## Some internally generated logs use the naming 'CellId' replace that with 'cellId'\n",
    "        if ('CellId' in df.columns):\n",
    "            df.rename(columns = {'CellId':'cellId'}, inplace = True)\n",
    "        if ('currentCellId' in df.columns):\n",
    "            df.rename(columns = {'currentCellId':'cellId'}, inplace = True)    \n",
    "        \n",
    "        # Warning: cellId and IMSI here are IP addresses, but it should not matter since \n",
    "        # there will only be 1 UE doing these ul and dl throughput scans   \n",
    "        if file == 'dlThroughput_trace.txt':\n",
    "            df.rename(columns = {'toAddr':'IMSI', 'fromAddr': 'cellId'}, inplace = True)\n",
    "            assert (df['IMSI'].nunique() == 1), \"More than one throughput measurement UE is in the logs\" \n",
    "        if file == 'ulThroughput_trace.txt':\n",
    "            df.rename(columns = {'toAddr':'cellId', 'fromAddr': 'IMSI'}, inplace = True)\n",
    "            assert (df['IMSI'].nunique() == 1), \"More than one throughput measurement UE is in the logs\" \n",
    "            \n",
    "        ## Just for plotting change the timestamp_us to seconds and delay values to milli seconds \n",
    "        ## since I am mostly plotting directly from pandas and don't know how to add a multiplicative factor to a column    \n",
    "        if 'tstamp_us' in df.columns:\n",
    "            df['tstamp_us'] = df['tstamp_us']/M\n",
    "            # Set datetime index for all files so that we can do series operations \n",
    "            datatime_timestamps = pd.to_datetime(df['tstamp_us'], unit='s', origin='unix')\n",
    "            df = df.set_index(datatime_timestamps, inplace=False)\n",
    "            \n",
    "            # add a sample at the beginning and ending of every timeseries at sim_start_time and sim_end_time. \n",
    "            # This way the timeseries after resampling are all of the same length \n",
    "            if file in a_vs_b_files: \n",
    "                alignment_sample = np.empty(df.shape[1])\n",
    "                alignment_sample[:] = np.nan\n",
    "                start_sample = pd.DataFrame([alignment_sample], columns=df.columns, index=[sim_start_time])\n",
    "                end_sample = pd.DataFrame([alignment_sample], columns=df.columns, index=[sim_end_time])\n",
    "                if 'dir' in df.columns:\n",
    "                    imsis_in_file = sorted(df['IMSI'].unique())\n",
    "                    # all_imsis = range(1,total_num_ues+1)\n",
    "                    for imsi in imsis_in_file:\n",
    "                        for di in ['UL', 'DL']:\n",
    "                            start_sample['IMSI'] = imsi\n",
    "                            start_sample['dir'] = di\n",
    "                            end_sample['IMSI'] = imsi\n",
    "                            end_sample['dir'] = di\n",
    "                            df = pd.concat([start_sample, df, end_sample])\n",
    "                elif 'cellId' in df.columns:\n",
    "                    #print('The issue is here. I am only initializing for cell Id 0 and not the others which is why those start from 0.5 and cellSI 0 starts from 0')\n",
    "                    # Is there a better way to do this ? Like just slice them afterwards to something smaller? \n",
    "                    imsis_in_file = sorted(df['IMSI'].unique())\n",
    "                    # all_imsis = range(1,total_num_ues+1)\n",
    "                    for imsi in imsis_in_file:\n",
    "                        start_sample['IMSI'] = imsi\n",
    "                        start_sample['cellId'] = 0\n",
    "                        end_sample['IMSI'] = imsi\n",
    "                        end_sample['cellId'] = 0\n",
    "                        df = pd.concat([start_sample, df, end_sample])\n",
    "                else:\n",
    "                    imsis_in_file = sorted(df['IMSI'].unique())\n",
    "                    # all_imsis = range(1,total_num_ues+1)\n",
    "                    for imsi in imsis_in_file:\n",
    "                        start_sample['IMSI'] = imsi\n",
    "                        end_sample['IMSI'] = imsi\n",
    "                        df = pd.concat([start_sample, df, end_sample])\n",
    "        \n",
    "        if file == 'dlThroughput_trace.txt' or file == 'ulThroughput_trace.txt':\n",
    "            df['IMSI'] = 1\n",
    "            thput_meas_imsi = 1\n",
    "        \n",
    "        # converting all delay values to ms instead of us\n",
    "        if 'delay' in df.columns:\n",
    "            df['delay'] = df['delay']/K\n",
    "        \n",
    "        #=======================================\n",
    "        # Print log file info  \n",
    "        #=======================================\n",
    "        if print_logfile_info:\n",
    "            ## Display info about the UEs who have made entries in this file to make sure that all the UEs who should be here are here. \n",
    "            print('ueIds: min:', min(df['IMSI'].value_counts().index), 'max:', max(df['IMSI'].value_counts().index), \n",
    "                  'count:', len(df['IMSI'].value_counts().index))\n",
    "            if (file in ran_files) and (len(df['IMSI'].value_counts().index) < total_num_ues):\n",
    "                print('WARNING: Fewer UEs in this file than the total number in the simulation')\n",
    "                print(df['IMSI'].value_counts())\n",
    "\n",
    "            ## Display info about the Cells who have made entries in this file\n",
    "            #print('cellIds: min:', min(df['cellId'].value_counts().index), 'max:', max(df['cellId'].value_counts().index),\n",
    "            #     'count:', len(df['cellId'].value_counts().index))\n",
    "\n",
    "            ## Total runtime of log\n",
    "            print('log time (start, end): (', np.nanmin(df['tstamp_us']), ', ' ,np.nanmax(df['tstamp_us']), ')')\n",
    "            print('log runtime:', (np.nanmax(df['tstamp_us']) - np.nanmin(df['tstamp_us'])), ' seconds')\n",
    "           \n",
    "        #==========================================================================\n",
    "        # File specific extraction and aggregation of metrics for dataset creation\n",
    "        #==========================================================================\n",
    "        \n",
    "        if create_dataset:\n",
    "            \n",
    "            # this must come first, so make sure it is the first file being read \n",
    "            # by putting it first in the files list\n",
    "            \n",
    "            if file == 'mobility_trace.txt':\n",
    "                \n",
    "                # Need to take unique for each IMSI list\n",
    "                df_mi_cellId_imsi_conn = df[['IMSI', 'cellId']].sort_values(by=['cellId']).groupby(by=['cellId'])\n",
    "                df_mi_cellId_imsi_conn = df_mi_cellId_imsi_conn.resample(time_wind_str).agg(list)\n",
    "                # This slices away any extras. It does not take care if there is a window missing i.e. the sequence is shorter \n",
    "                # lets just slice off a second since I goofed up in the beginning and dont want to include extra samples for each cell \n",
    "                df_mi_cellId_imsi_conn = df_mi_cellId_imsi_conn.drop(['cellId'], axis=1).loc[pd.IndexSlice[:, sim_start_time : sim_end_time], :]  \n",
    "                df_mi_cellId_imsi_conn.index = df_mi_cellId_imsi_conn.index.set_names(['cellId', 'wind_tstamp'])\n",
    "             \n",
    "            elif file == 'dashClient_trace.txt':\n",
    "                \n",
    "                chosen_imsis = filtered_imsis(data_slice, sorted(df['IMSI'].unique()))                    \n",
    "                print('video streaming IMSIs: ', chosen_imsis)\n",
    "                \n",
    "                # IMSI perspective \n",
    "                \n",
    "                # Create the UE and cell perspective for the dataset \n",
    "                # This has 2 levels of indexes, one is IMSI and the other is windowed timestep  \n",
    "                df_mi_imsi_log = df.sort_values(by=['IMSI']).groupby(by=['IMSI'])\n",
    "                df_mi_imsi_log = df_mi_imsi_log.resample(time_wind_str).mean()\n",
    "        \n",
    "                # This slices away any extras. It does not take care if there is a window missing i.e. the sequence is shorter \n",
    "                df_mi_imsi_log = df_mi_imsi_log.loc[pd.IndexSlice[:, sim_start_time : sim_end_time], :]\n",
    "                # give a name to the resampled time index \n",
    "                df_mi_imsi_log.index = df_mi_imsi_log.index.set_names(['IMSI', 'wind_tstamp'])\n",
    "                \n",
    "                this_log_df = pd.DataFrame(columns=['newBitRate_bps', 'oldBitRate_bps'])\n",
    "                # I want to drop all the rows that correspond to IMSIs that are not in chosen_imsis\n",
    "                this_log_df['newBitRate_bps'] = df_mi_imsi_log[df_mi_imsi_log.index.get_level_values('IMSI').isin(chosen_imsis)]['newBitRate_bps']\n",
    "                this_log_df['oldBitRate_bps'] = df_mi_imsi_log[df_mi_imsi_log.index.get_level_values('IMSI').isin(chosen_imsis)]['oldBitRate_bps']\n",
    "                \n",
    "                # add a prefix to the column names\n",
    "                this_log_df = this_log_df.add_prefix(file+'_')\n",
    "                                                \n",
    "                # Make sure that the order of IMSIs is the same as when we do the RAN logs\n",
    "                #print(this_log_df.shape)\n",
    "                #print(this_log_df.columns)\n",
    "                #print(this_log_df.head(n=2))\n",
    "                \n",
    "                # Concatenating along axis=1 and setting 'IMSI' and 'wind_tstamp' as the index again\n",
    "                per_run_df = pd.concat([per_run_df, this_log_df], axis=1, sort=True)\n",
    "                print(per_run_df.head(n=2))\n",
    "                \n",
    "                print('time to parse file: ', (time.time() - start_time)) \n",
    "                \n",
    "            elif file == 'vrFragment_trace.txt':\n",
    "\n",
    "                chosen_imsis = filtered_imsis(data_slice, sorted(df['IMSI'].unique()))\n",
    "                print('VR IMSIs: ', chosen_imsis)\n",
    "                \n",
    "                this_log_df = pd.DataFrame()\n",
    "                tmp_this_log_df = pd.DataFrame()\n",
    "\n",
    "                for imsi, group1 in df.sort_values(by=['IMSI']).groupby(by=['IMSI']):\n",
    "                    imsi = imsi[0]\n",
    "                    if imsi in chosen_imsis:\n",
    "                        #print(imsi)        \n",
    "                        vr_frag_time_df = (group1['delay']).resample(time_wind_str).mean().loc[sim_start_time : sim_end_time]\n",
    "                        #print('vr_frag_time_df')\n",
    "                        #print(vr_frag_time_df.index)\n",
    "                        #print(vr_frag_time_df.head(n=2))\n",
    "                        vr_frag_thput_df = (group1['burstSize']*8/K/group1['numFragsInBurst']/group1['delay']).resample(time_wind_str).mean().loc[sim_start_time : sim_end_time] #Mbps\n",
    "                        #print('vr_frag_thput_df')\n",
    "                        #print(vr_frag_thput_df.index)\n",
    "                        #print(vr_frag_thput_df.head(n=2))\n",
    "                        \n",
    "                        vr_burst_time_df = list()\n",
    "                        vr_burst_thput_df = list()\n",
    "                        vr_index = list()\n",
    "                        \n",
    "                        for burstSeqNum, group2 in group1.groupby(by=['burstSeqNum']):\n",
    "                            # this takes the receive timestamp\n",
    "                            vr_index = vr_index + [group2.index[-1]]\n",
    "                            vr_burst_time_df = vr_burst_time_df + [group2['delay'].iloc[-1]]\n",
    "                            vr_burst_thput_df = vr_burst_thput_df + [(group2['burstSize'].iloc[-1]*8/K)/group2['delay'].iloc[-1] ]# Mbps\n",
    "                        \n",
    "                        vr_burst_time_df = pd.DataFrame(index=pd.to_datetime(vr_index), columns=['vr_burst_time'], data=vr_burst_time_df)\n",
    "                        vr_burst_time_df = vr_burst_time_df.resample(time_wind_str).mean().loc[sim_start_time : sim_end_time]\n",
    "                        #print('vr_burst_time_df')\n",
    "                        #print(vr_burst_time_df.index)\n",
    "                        #print(vr_burst_time_df.head(n=2))\n",
    "                        \n",
    "                        vr_burst_thput_df = pd.DataFrame(index=vr_index, columns=['vr_burst_thput_mbps'], data=vr_burst_thput_df)\n",
    "                        vr_burst_thput_df = vr_burst_thput_df.resample(time_wind_str).mean().loc[sim_start_time : sim_end_time] # Mbps\n",
    "                        #print('vr_burst_thput_df')\n",
    "                        #print(vr_burst_thput_df.index)\n",
    "                        #print(vr_burst_thput_df.head(n=2))\n",
    "                        \n",
    "                        print('----------------------------------')\n",
    "                        # concatenate\n",
    "                        this_log_df_per_imsi = pd.concat([vr_frag_time_df, vr_frag_thput_df, vr_burst_time_df, vr_burst_thput_df], axis=1)\n",
    "                        this_log_df_per_imsi.columns = ['vr_frag_time', 'vr_frag_thput_mbps', 'vr_burst_time', 'vr_burst_thput_mbps']\n",
    "                        # Add IMSI as a column to use later for indexing\n",
    "                        this_log_df_per_imsi['IMSI'] = [imsi] * this_log_df_per_imsi.shape[0]\n",
    "                        #print('this_log_df_per_imsi')\n",
    "                        #print(this_log_df_per_imsi.index)\n",
    "                        #print(this_log_df_per_imsi.head(n=2))\n",
    "                        \n",
    "                        tmp_this_log_df = pd.concat([tmp_this_log_df, this_log_df_per_imsi], axis=0)  \n",
    "                        #print('tmp_this_log_df')\n",
    "                        #print(tmp_this_log_df.index)\n",
    "                        #print(tmp_this_log_df.head(n=2))\n",
    "                        \n",
    "                # Make sure that the order of IMSIs is the same as when we do the RAN logs \n",
    "                # set index name for the resampled time stamp\n",
    "                print('---------------------------------------------------------')\n",
    "                tmp_this_log_df.index = tmp_this_log_df.index.set_names('wind_tstamp')\n",
    "                #print('tmp_this_log_df')\n",
    "                #print(tmp_this_log_df.head(n=2))\n",
    "                \n",
    "                # Convert the IMSI column to an index\n",
    "                #tmp_this_log_df = tmp_this_log_df.sort_values(by=['IMSI']).groupby(by=['IMSI'])\n",
    "                tmp_this_log_df = tmp_this_log_df.set_index('IMSI', append=True)\n",
    "                this_log_df = tmp_this_log_df.reorder_levels(['IMSI', 'wind_tstamp'])\n",
    "                #print('this_log_df')\n",
    "                #print(this_log_df.head(n=2))\n",
    "                #print(this_log_df.columns)\n",
    "                \n",
    "                #this_log_df.columns = ['vr_frag_time', 'vr_frag_thput', 'vr_burst_time', 'vr_burst_thput']\n",
    "                # add a prefix to the column names\n",
    "                this_log_df = this_log_df.add_prefix(file+'_')\n",
    "                \n",
    "                #print(this_log_df.shape)\n",
    "                #print(this_log_df.columns)\n",
    "                #print(this_log_df.head(n=2))\n",
    "                \n",
    "                # horz concatenate it to the per_run_df\n",
    "                #per_run_df = pd.concat([per_run_df, this_log_df], axis=1)\n",
    "                # Concatenating along axis=1 and setting 'IMSI' and 'wind_tstamp' as the index again\n",
    "                per_run_df = pd.concat([per_run_df, this_log_df], axis=1, sort=True)\n",
    "                #print(per_run_df.head(n=2))\n",
    "                \n",
    "                print('time to parse file: ', (time.time() - start_time))\n",
    "                \n",
    "            elif file == 'delay_trace.txt': \n",
    "                # process the my_metrics part and skip the cell_metrics part \n",
    "                # just take the ul and dl delay cols and discard the rest \n",
    "                \n",
    "                # IMSI perspective \n",
    "                \n",
    "                # Create the UE and cell perspective for the dataset \n",
    "                # This has 2 levels of indexes, one is IMSI and the other is windowed timestep  \n",
    "                # Also drop the UL and DL markers so that only numerica features are left\n",
    "                df_ul = df[df['dir'] == 'UL'].drop(['dir'], axis=1)\n",
    "                df_dl = df[df['dir'] == 'DL'].drop(['dir'], axis=1)\n",
    "                #print(df_ul.columns)\n",
    "                #print(df_dl.columns)\n",
    "                df_mi_imsi_log_ul = df_ul.sort_values(by=['IMSI']).groupby(by=['IMSI'])\n",
    "                \n",
    "                df_mi_imsi_log_ul = df_mi_imsi_log_ul.resample(time_wind_str).mean()\n",
    "                \n",
    "                df_mi_imsi_log_dl = df_dl.sort_values(by=['IMSI']).groupby(by=['IMSI'])\n",
    "                df_mi_imsi_log_dl = df_mi_imsi_log_dl.resample(time_wind_str).mean()\n",
    "                \n",
    "                # This slices away any extras. It does not take care if there is a window missing i.e. the sequence is shorter \n",
    "                df_mi_imsi_log_ul = df_mi_imsi_log_ul.loc[pd.IndexSlice[:, sim_start_time : sim_end_time], :]\n",
    "                df_mi_imsi_log_dl = df_mi_imsi_log_dl.loc[pd.IndexSlice[:, sim_start_time : sim_end_time], :]\n",
    "                # give a name to the resampled time index \n",
    "                df_mi_imsi_log_ul.index = df_mi_imsi_log_ul.index.set_names(['IMSI', 'wind_tstamp'])\n",
    "                df_mi_imsi_log_dl.index = df_mi_imsi_log_dl.index.set_names(['IMSI', 'wind_tstamp'])\n",
    "                \n",
    "                this_log_df = pd.DataFrame(columns=['ul_delay', 'dl_delay'])\n",
    "                \n",
    "                chosen_imsis = filtered_imsis(data_slice, sorted(df['IMSI'].unique()))\n",
    "\n",
    "                this_log_df['ul_delay'] = df_mi_imsi_log_ul[df_mi_imsi_log_ul.index.get_level_values('IMSI').isin(chosen_imsis)]['delay']\n",
    "                this_log_df['dl_delay'] = df_mi_imsi_log_dl[df_mi_imsi_log_dl.index.get_level_values('IMSI').isin(chosen_imsis)]['delay']\n",
    "            \n",
    "                # Make sure that the order of IMSIs is the same as when we do the RAN logs\n",
    "                # add a prefix to the column names\n",
    "                this_log_df = this_log_df.add_prefix(file+'_')\n",
    "                \n",
    "                #print(this_log_df.shape)\n",
    "                #print(this_log_df.columns)\n",
    "                \n",
    "                # horz concatenate it to the per_run_df\n",
    "                #print(per_run_df.head(2))\n",
    "                #print(this_log_df.head(2))\n",
    "                \n",
    "                per_run_df = pd.concat([per_run_df, this_log_df], axis=1, sort=True)\n",
    "                #print(per_run_df.head(n=2))\n",
    "                \n",
    "                print('time to parse file: ', (time.time() - start_time)) \n",
    "            \n",
    "            # ends up being all the RAN files \n",
    "            else:\n",
    "                # IMSI perspective \n",
    "                \n",
    "                # Create the UE and cell perspective for the dataset \n",
    "                # This has 2 levels of indexes, one is IMSI and the other is windowed timestep  \n",
    "                df_mi_imsi_log = df.sort_values(by=['IMSI']).groupby(by=['IMSI'])\n",
    "                # identify the sum and mean columns from the set of columns in this log file\n",
    "                sum_cols_feats = list(set(ran_sum_feats) & set(df.columns))\n",
    "                mean_cols_feats = list(set(df.columns) - set(sum_cols_feats))\n",
    "                #resample into windows and either mean or sum based on which group the column belongs to\n",
    "                sum_df_mi_imsi_log = df_mi_imsi_log[sum_cols_feats].resample(time_wind_str).sum()\n",
    "                mean_df_mi_imsi_log = df_mi_imsi_log[mean_cols_feats].resample(time_wind_str).mean()\n",
    "                df_mi_imsi_log = pd.concat([mean_df_mi_imsi_log, sum_df_mi_imsi_log], axis=1)\n",
    "                # This slices away any extras. It does not take care if there is a window missing i.e. the sequence is shorter \n",
    "                df_mi_imsi_log = df_mi_imsi_log.loc[pd.IndexSlice[:, sim_start_time : sim_end_time], :]\n",
    "                # give a name to the resampled time index \n",
    "                df_mi_imsi_log.index = df_mi_imsi_log.index.set_names(['IMSI', 'wind_tstamp'])\n",
    "                df_mi_imsi_log = df_mi_imsi_log.drop(drop_cols_before_sep, axis=1, errors='ignore')\n",
    "\n",
    "                # Cell perspective\n",
    "                \n",
    "                # Create the UE abnd cell perspective for the dataset \n",
    "                # This has 2 levels of indexes, one is IMSI and the other is windowed timestep  \n",
    "                df_mi_cell_log = df.sort_values(by=['cellId']).groupby(by=['cellId'])\n",
    "                #resample into windows and either mean or sum based on which group the column belongs to \n",
    "                sum_df_mi_cell_log = df_mi_cell_log[sum_cols_feats].resample(time_wind_str).sum()\n",
    "                mean_df_mi_cell_log = df_mi_cell_log[mean_cols_feats].resample(time_wind_str).mean()\n",
    "                df_mi_cell_log = pd.concat([mean_df_mi_cell_log, sum_df_mi_cell_log], axis=1)\n",
    "                # This slices away any extras. It does not take care if there is a window missing i.e. the sequence is shorter \n",
    "                df_mi_cell_log = df_mi_cell_log.loc[pd.IndexSlice[:, sim_start_time : sim_end_time], :]\n",
    "                # give a name to the resampled time index \n",
    "                df_mi_cell_log.index = df_mi_cell_log.index.set_names(['cellId', 'wind_tstamp'])\n",
    "                df_mi_cell_log = df_mi_cell_log.drop(drop_cols_before_sep, axis=1, errors='ignore')\n",
    "                \n",
    "                # Align the cell perspective to the IMSI perspective\n",
    "                \n",
    "                this_log_df = pd.DataFrame()\n",
    "                # the imsis from whose application's perspective we are constructing the dataset \n",
    "                # make sure we are reading from a fixed list and not something like for imsi in all_imsis_in_this_file\n",
    "                # we need the same order of IMSIs to be iterated thru so that the data frame for each file \n",
    "                # will have the same imsi order \n",
    "\n",
    "                # THIS WAS ADDED 27 Dec\n",
    "                chosen_imsis = filtered_imsis(data_slice, sorted(df['IMSI'].unique()))\n",
    "                \n",
    "                for i_imsi in chosen_imsis:\n",
    "                    \n",
    "                    num_rows = df_mi_imsi_log.loc[i_imsi].shape[0]\n",
    "                    num_cols = df_mi_imsi_log.loc[i_imsi].shape[1]\n",
    "                    basic_info_per_imsi = pd.DataFrame(index=df_mi_imsi_log.loc[i_imsi].index,\n",
    "                                                   columns=['IMSI', 'cellId', 'conn_imsi_count'])\n",
    "                    if first_ran_file:\n",
    "                        # set the index for this as well \n",
    "                        basic_info_per_imsi['IMSI'] = [i_imsi] * num_rows\n",
    "                        basic_info_per_imsi['cellId'] = [0] * num_rows #an array of the corresponding cellId\n",
    "                        basic_info_per_imsi['conn_imsi_count'] = [0] * num_rows #as given by the imsis connected cell \n",
    "                        \n",
    "                    # Get the metrics for this IMSI\n",
    "                    my_metrics = df_mi_imsi_log.loc[i_imsi]\n",
    "\n",
    "                    # Initialize a DataFrame to store metrics for other IMSIs\n",
    "                    cell_metrics = pd.DataFrame(index=df_mi_imsi_log.loc[i_imsi].index,\n",
    "                                                  columns=['cell_' + col for col in df_mi_imsi_log.loc[i_imsi].columns],\n",
    "                                                  data=np.zeros((num_rows, num_cols)))\n",
    "                    \n",
    "                    # Iterate over each window for this IMSI\n",
    "                    for wind in df_mi_imsi_log.loc[i_imsi].index:\n",
    "                        # the cell that the UE was connected to\n",
    "                        con_cell = np.round(df_mi_imsi_log.loc[i_imsi, wind]['cellId'])\n",
    "                        if first_ran_file:\n",
    "                            basic_info_per_imsi.loc[wind, 'cellId'] = con_cell\n",
    "                        # Handle NaN case\n",
    "                        if np.isnan(con_cell):\n",
    "                            cell_metrics.loc[wind] = np.nan\n",
    "                            continue\n",
    "\n",
    "                        if first_ran_file:\n",
    "                            basic_info_per_imsi.loc[wind, 'conn_imsi_count'] = len(np.unique(df_mi_cellId_imsi_conn.loc[con_cell, wind][0]))\n",
    "                        cell_metrics.loc[wind] = df_mi_cell_log.loc[con_cell, wind].values\n",
    "                        \n",
    "                    # Combine metrics for the current IMSI and other IMSIs\n",
    "                    this_imsi_df = pd.concat([my_metrics, cell_metrics], axis=1)\n",
    "\n",
    "                    # Drop the columns that need to be removed\n",
    "                    this_imsi_df = this_imsi_df.drop(drop_cols_after_sep, axis=1, errors='ignore')\n",
    "\n",
    "                    # Append the metrics for this IMSI to the final DataFrame and include IMSI as a column to use as index later\n",
    "                    this_imsi_df['IMSI'] = [i_imsi] * this_imsi_df.shape[0]\n",
    "                    this_log_df = pd.concat([this_log_df, this_imsi_df], axis=0)\n",
    "                    \n",
    "                    if first_ran_file:\n",
    "                        # combine the basic info \n",
    "                        basic_info = pd.concat([basic_info, basic_info_per_imsi], axis=0)\n",
    "     \n",
    "                # end of for over imsis\n",
    "\n",
    "                # convert the IMSI column of this_log_df into an index\n",
    "                this_log_df = this_log_df.set_index('IMSI', append=True)\n",
    "                this_log_df = this_log_df.reorder_levels(['IMSI', 'wind_tstamp'])\n",
    "                \n",
    "                # add a prefix to the column names\n",
    "                this_log_df = this_log_df.add_prefix(file+'_')\n",
    "                #print(this_log_df.shape)\n",
    "                #print(this_log_df.columns)\n",
    "                \n",
    "                # horz concatenate it to the per_run_df\n",
    "                if first_ran_file:\n",
    "                    basic_info.index = basic_info.index.set_names('wind_tstamp')                    \n",
    "                    # Convert the IMSI column to an index\n",
    "                    basic_info = basic_info.set_index('IMSI', append=True)\n",
    "                    basic_info = basic_info.reorder_levels(['IMSI', 'wind_tstamp'])                 \n",
    "                    #print(basic_info.head)\n",
    "                    #print(per_run_df.head)\n",
    "                    #print(this_log_df.head)\n",
    "                    per_run_df = pd.concat([basic_info, per_run_df, this_log_df], axis=1)\n",
    "                \n",
    "                per_run_df = pd.concat([per_run_df, this_log_df], axis=1)\n",
    "                first_ran_file = False\n",
    "                print('time to parse file: ', (time.time() - start_time))   \n",
    "            # end of elif file in ran_files:\n",
    "        # end of if create_dataset:\n",
    "    # end of for over files\n",
    "    # each run is saved separately in a different file \n",
    "    # Save the dataset\n",
    "    save_dir=data_dir+'parsed_data/'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    per_run_df = per_run_df.reset_index(level=('IMSI',))    \n",
    "    per_run_df.to_csv(save_dir+only_run+'_dataslice_'+data_slice+'_video_delay_vr_'+time_wind_str+'.csv', index=True)\n",
    "    print('============================================================')   \n",
    "    print('======================= RUN DONE =====================================')\n",
    "    #master_df = pd.concat([master_df, per_run_df], axis=0)   \n",
    "    #num_runs=num_runs+1\n",
    "# end of for over runs\n",
    "\n",
    "#if create_dataset:\n",
    "#    # Save the dataset\n",
    "#    save_dir=data_dir+'parsed_data/'\n",
    "#    if not os.path.exists(save_dir):\n",
    "#        os.makedirs(save_dir)\n",
    "#    master_df.to_csv(save_dir+'dataset_'+data_slice+'_video_delay_vr_'+time_wind_str+'.csv', index=True)\n",
    "\n",
    "#print(master_df.shape)\n",
    "#print(list(master_df.columns))\n",
    "print('============================================================')   \n",
    "print('======================= ALL DONE =====================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed69b69c-db84-4f17-a30f-a08a75930250",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Do not know what this code does dfferently\n",
    "# Seems like it tries to take each IMSI versus the rest instead of taking the whole cell as I have done in the previous cell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea1144b-1c07-47da-9978-8cf2d1593d71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "        if create_dataset:\n",
    "            # this must come first, so make sure it is the first file being read \n",
    "            # by putting it first in the files list\n",
    "            if file == 'mobility_trace.txt':\n",
    "                # Need to take unique for each IMSI list\n",
    "                df_mi_cellId = df[['IMSI', 'cellId']].sort_values(by=['cellId']).groupby(by=['cellId'])\n",
    "                df_mi_cellId = df_mi_cellId.resample(time_wind_str).agg(list)\n",
    "                # This slices away any extras. It does not take care if there is a window missing i.e. the sequence is shorter \n",
    "                # lets just slice off a second since I goofed up in the beginning and dont want to include extra samples for each cell \n",
    "                df_mi_cellId = df_mi_cellId.drop(['cellId'], axis=1).loc[pd.IndexSlice[:, sim_start_time : sim_end_time], :]\n",
    "                df_cellId = df_mi_cellId.reset_index(level='cellId')\n",
    "                #for cell in (df_mi_cellId.index.get_level_values('cellId').unique()):\n",
    "                #    print('cell ', cell)\n",
    "                #    print(df_mi_cellId.loc[cell])\n",
    "             \n",
    "            elif file in ran_files:\n",
    "            #elif False:    \n",
    "                # Do the one versus rest thingy \n",
    "                # This has 2 levels of indexes, one is IMSI and the other is windowed timestep \n",
    "                df_mi_imsi_log = df.sort_values(by=['IMSI']).groupby(by=['IMSI'])\n",
    "                df_mi_imsi_log = df_mi_imsi_log.resample(time_wind_str).mean()\n",
    "                # This slices away any extras. It does not take care if there is a window missing i.e. the sequence is shorter \n",
    "                df_mi_imsi_log = df_mi_imsi_log.loc[pd.IndexSlice[:, sim_start_time : sim_end_time], :]\n",
    "                #df_imsi_log = df_mi_imsi_log.droplevel('IMSI')\n",
    "                # give a name to the resampled time index \n",
    "                df_mi_imsi_log.index = df_mi_imsi_log.index.set_names(['IMSI', 'wind_tstamp'])\n",
    "\n",
    "\n",
    "                full_df = pd.DataFrame()\n",
    "                # the imsis from whose application's perspective we are constructing the dataset \n",
    "                for i_imsi in macro_imsis:\n",
    "                    # Get the metrics for this IMSI\n",
    "                    my_metrics = df_mi_imsi_log.loc[i_imsi]\n",
    "\n",
    "                    # the time over which the simulation ran \n",
    "                    num_rows = df_mi_imsi_log.loc[i_imsi].shape[0]\n",
    "                    num_cols = df_mi_imsi_log.loc[i_imsi].shape[1]\n",
    "\n",
    "                    # Initialize a DataFrame to store metrics for other IMSIs\n",
    "                    others_metrics = pd.DataFrame(index=df_mi_imsi_log.loc[i_imsi].index,\n",
    "                                                  columns=['others_' + col for col in df_mi_imsi_log.loc[i_imsi].columns],\n",
    "                                                  data=np.zeros((num_rows, num_cols)))\n",
    "\n",
    "                    # Iterate over each window for this IMSI\n",
    "                    for wind in df_mi_imsi_log.loc[i_imsi].index:\n",
    "                        # the cell that the UE was connected to\n",
    "                        con_cell = np.round(df_mi_imsi_log.loc[i_imsi, wind]['cellId'])\n",
    "\n",
    "                        # Handle NaN case\n",
    "                        if np.isnan(con_cell):\n",
    "                            others_metrics.loc[wind] = np.nan\n",
    "                            continue\n",
    "\n",
    "                        # Find other IMSIs for this window\n",
    "                        other_imsis = np.unique(df_mi_cellId.loc[con_cell, wind][0])\n",
    "                        other_imsis = other_imsis[other_imsis != i_imsi]  # Exclude the current IMSI\n",
    "                        #others_windows = df_mi_imsi_log.index[level='wind_tstamp']\n",
    "\n",
    "                        # Calculate the mean of other IMSIs' metrics for each window\n",
    "                        if len(other_imsis) > 0:\n",
    "                            others_metrics.loc[wind] = df_mi_imsi_log.loc[pd.IndexSlice[other_imsis, wind], :].mean().values\n",
    "                        \n",
    "                        print(others_metrics.loc[wind])\n",
    "                        ggg\n",
    "                    \n",
    "                    print(others_metrics)\n",
    "                    # Combine metrics for the current IMSI and other IMSIs\n",
    "                    this_imsi_df = pd.concat([my_metrics, others_metrics], axis=1)\n",
    "\n",
    "                    # Drop the columns that need to be removed\n",
    "                    this_imsi_df = this_imsi_df.drop(drop_cols_after_sep, axis=1, errors='ignore')\n",
    "                    #print(this_imsi_df)\n",
    "\n",
    "                    # Append the metrics for this IMSI to the final DataFrame\n",
    "                    full_df = pd.concat([full_df, this_imsi_df], axis=0)\n",
    "\n",
    "                print(full_df.shape)\n",
    "                print(full_df.columns)\n",
    "                print('time to parse file: ', (time.time() - start_time))\n",
    "                ggg\n",
    "\n",
    "            \n",
    "            #elif file in ran_files: \n",
    "            elif False:   # the one I wrote, unoptimised  \n",
    "                # Do the one versus rest thingy \n",
    "                # This has 2 levels of indexes, one is IMSI and the other is windowed timestep \n",
    "                df_mi_imsi_log = df.sort_values(by=['IMSI']).groupby(by=['IMSI'])\n",
    "                df_mi_imsi_log = df_mi_imsi_log.resample(time_wind_str).mean()\n",
    "                # This slices away any extras. It does not take care if there is a window missing i.e. the sequence is shorter \n",
    "                df_mi_imsi_log = df_mi_imsi_log.loc[pd.IndexSlice[:, sim_start_time : sim_end_time], :]\n",
    "                df_mi_imsi_log.index = df_mi_imsi_log.index.set_names(['IMSI', 'wind_tstamp'])\n",
    "                #df_imsi_log = df_mi_imsi_log.droplevel('IMSI')\n",
    "\n",
    "                full_df = pd.DataFrame()\n",
    "                # the imsis from whose application's perspective we are constructing the dataset \n",
    "                for i_imsi in macro_imsis:\n",
    "                    # The metrics from this imsi \n",
    "                    my_metrics = df_mi_imsi_log.loc[i_imsi]\n",
    "                    # the time over which the simulation ran \n",
    "                    num_rows = df_mi_imsi_log.loc[i_imsi].shape[0]\n",
    "                    num_cols = df_mi_imsi_log.loc[i_imsi].shape[1]\n",
    "                    others_metrics = pd.DataFrame(np.zeros((num_rows, num_cols)))\n",
    "                    others_metrics.index = df_mi_imsi_log.loc[i_imsi].index\n",
    "                    others_metrics.columns = ['others_'+col for col in df_mi_imsi_log.loc[i_imsi].columns]\n",
    "                    \n",
    "                    for wind in df_mi_imsi_log.loc[i_imsi].index:\n",
    "                        # the cell that the UE was connected to\n",
    "                        # This can be Nan, which becomes a problem later on\n",
    "                        # Why can this be nan ? because of the windowing ? I guess if there are no samples in this window, then it shall be nan  \n",
    "                        con_cell = np.round(df_mi_imsi_log.loc[i_imsi, wind]['cellId'])\n",
    "                        if np.isnan(con_cell):\n",
    "                            others_metrics.loc[wind] = np.nan \n",
    "                            continue\n",
    "                        #con_cell = min(max_cell_id, max(1, np.round(df_mi_imsi_log[i_imsi, wind] ['cellId'])))                        \n",
    "                        # This can be nan and needs to be handled \n",
    "                        # nan was triggered by the file UlTxPhyStats.txt\n",
    "                        other_imsis = np.unique(df_mi_cellId.loc[con_cell, wind][0])\n",
    "                        other_imsis = other_imsis[other_imsis != i_imsi] \n",
    "                        for o_imsi in other_imsis:\n",
    "                            # The metrics related to this other imsi for this window  \n",
    "                            # I need to drop repeated cols and non metric type cols before I do this\n",
    "                            # lets join them and then see whats we end up with and then decide what to drop \n",
    "                            others_metrics.loc[wind] = others_metrics.loc[wind] + df_mi_imsi_log.loc[o_imsi, wind].values # add row by row  \n",
    "                        others_metrics.loc[wind] = others_metrics.loc[wind]/len(other_imsis)\n",
    "                    print(others_metrics)\n",
    "                    this_imsi_df = pd.concat([my_metrics, others_metrics], axis=1)\n",
    "                    this_imsi_df = this_imsi_df.drop(drop_cols_after_sep, axis=1, errors='ignore')\n",
    "                    #print(this_imsi_df)\n",
    "                    full_df = pd.concat([full_df, this_imsi_df], axis=0)\n",
    "                print(full_df.shape)\n",
    "                print(full_df.columns)\n",
    "                print('time to parse file: ', (time.time() - start_time))\n",
    "                ggg\n",
    "    # end of for over files \n",
    "        \n",
    "    #num_runs=num_runs+1\n",
    "# end of for over runs\n",
    "print('============================================================')   \n",
    "print('======================= DONE =====================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "16461a54-5e2d-4cfa-ad8e-d08661ed962f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system('cp parse_visualize_data.ipynb '+'./saved_notebooks/'+data_dir.split('/')[-2]+'.ipynb')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
