{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2e3e665-f8c9-4dc0-8ad9-94ec774bbc43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reload\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from helper_func_create_dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dff9e1d9-f635-4ec2-abe5-f4e8c87e9d27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Increase plot font size from default for all plots instead of setting it in each plot \n",
    "plt.rcParams.update({'font.size': 22})\n",
    "plt.rcParams[\"figure.autolayout\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec37f892-4511-4f01-b102-a9babff74722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data folder: \n",
      "../../../dataset_ver1/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "create_dataset = True\n",
    "print_logfile_info = True\n",
    "# When using time aggregation to smooth out plots and make them more visually interpretable what window size to use \n",
    "time_wind_str = '500ms'\n",
    "time_wind_val = 500\n",
    "\n",
    "n_step_history = True\n",
    "num_steps = 5\n",
    "small_wind_str = '100ms'\n",
    "small_wind_val = 100\n",
    "\n",
    "data_slice = 'all' #'macro' 'micro' 'fast' 'slow' 'all'\n",
    "\n",
    "#===================================\n",
    "# Data source\n",
    "#=================================== \n",
    "data_dir = '../../../dataset_ver1/'\n",
    "save_dir=data_dir+'parsed_data_5steps_debug/'\n",
    "\n",
    "# NOTE: 'mobility_trace.txt' needs to be first in the list since it is the only file that has consisently periodic values for all UEs\n",
    "# And this info is used to create the basic data structure \n",
    "files= ['mobility_trace.txt']+['dashClient_trace.txt', 'delay_trace.txt', 'vrFragment_trace.txt', 'httpClientRtt_trace.txt']+ran_files\n",
    "\n",
    "print('Raw data folder: \\n'+data_dir+'\\n')\n",
    "\n",
    "#=======================================\n",
    "# Initilizing empty lists \n",
    "#=======================================\n",
    "# Use 15 runs for pretraining and fine tuning and 5 runs for testing \n",
    "#runs = range(1, 20 + 1)\n",
    "runs = range(1, 1 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2399008f-1ebd-407b-a5f0-3fa7d641a914",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def setup_run(run, create_dataset, print_logfile_info, time_wind_str, time_wind_val, data_slice, save_dir, files):\n",
    "    # constant multipliers\n",
    "    M = (10**6)\n",
    "    K = (10**3)\n",
    "\n",
    "    print('============================================================')\n",
    "    print (run)\n",
    "    print('============================================================')\n",
    "    only_run = run.split('/')[-1]\n",
    "    print(only_run)\n",
    "\n",
    "    sim_info = pd.read_csv(run+'/sim_info.txt', sep=',').set_index('parameter').to_dict('index')\n",
    "    # Specify here topology details from the dataset we are using \n",
    "    # This can be infered from the files, but this is easier\n",
    "    print(sim_info)\n",
    "    separate_macro_micro = bool(int(sim_info['create_micro_layer']['value'])) \n",
    "    print('separate_macro_micro ', separate_macro_micro)\n",
    "    total_num_cells=(int(sim_info['macro_num_bs']['value'])+int(sim_info['micro_num_bs']['value'])) if separate_macro_micro else int(sim_info['macro_num_bs']['value']) \n",
    "    print('total_num_cells', total_num_cells)\n",
    "    total_num_ues=int(sim_info['macro_layer_ues']['value'])+int(sim_info['micro_layer_ues']['value']) if separate_macro_micro else int(sim_info['macro_layer_ues']['value'])\n",
    "    print('total_num_ues', total_num_ues)\n",
    "    sim_time = int(sim_info['simulation_time_seconds']['value']) # seconds\n",
    "    print('sim_time', sim_time)\n",
    "\n",
    "    dict_gnb_pos = pd.read_csv(run+'/gnb_locations.txt', sep=',').set_index('cellId').to_dict('index')\n",
    "    dict_gnb_pos[0] = {'gnbpos_x': np.nan, 'gnbpos_y': np.nan, 'gnbpos_z': np.nan}\n",
    "    #print(dict_gnb_pos)\n",
    "    # read UE group list\n",
    "    ue_groups = open(run+'/ue_gnb_groups.txt').readlines()\n",
    "    macro_imsis = [int(i) for i in ue_groups[0].split(',')[1:-1]]\n",
    "    print(\"Macro UE IMSIs: \", macro_imsis)\n",
    "    micro_imsis = list(set(range(1, total_num_ues+1)) - set(macro_imsis))\n",
    "    print(\"Micro UE IMSIs: \", micro_imsis)\n",
    "    fast_imsis = [int(i) for i in ue_groups[1].split(',')[1:-1]]\n",
    "    print(\"Macro fast UE IMSIs: \", fast_imsis)\n",
    "    slow_imsis = list(set(range(1, total_num_ues+1)) - set(fast_imsis) - set(micro_imsis))\n",
    "    print(\"Macro Slow UE IMSIs: \", slow_imsis)\n",
    "    #only_delay_imsis=[3,5,7,9,13,15,17,19,23,55,57]\n",
    "    only_delay_imsis=[int(i) for i in ue_groups[2].split(',')[1:-1]]\n",
    "    print(\"Only delay IMSIs: \", only_delay_imsis)\n",
    "    \n",
    "    # Read these from the ue_groups file later and also rename ue_groups to something else \n",
    "    macro_cells = [1,2,3]\n",
    "    print('Macro CellIds: ', macro_cells)\n",
    "    micro_cells = [4,5,6]\n",
    "    print('Micro CellIds: ', micro_cells)\n",
    "\n",
    "    sim_start_time = pd.to_datetime(1.0, unit='s', origin='unix')\n",
    "    sim_end_time = pd.to_datetime(sim_time, unit='s', origin='unix')\n",
    "\n",
    "    vr_imsi=np.empty(0)\n",
    "    dash_imsi=np.empty(0)\n",
    "    http_imsi=np.empty(0)\n",
    "    \n",
    "    df_mi_cellId = None\n",
    "    df_cellId = None\n",
    "    \n",
    "    # This dataframe shall aggregate the parsed features from all the different files \n",
    "    # before vertically concatenating to master_df \n",
    "    per_run_df = pd.DataFrame()\n",
    "    # stored from the first RAN file processed \n",
    "    basic_info = pd.DataFrame(columns=['IMSI', 'cellId', 'conn_imsi_count'])\n",
    "    first_ran_file = True \n",
    "    \n",
    "    #chosen_imsis = None\n",
    "#=================================================================\n",
    "    for file in files:\n",
    "#=================================================================\n",
    "        print('--------------------------------------------')\n",
    "        print(file)\n",
    "        start_time = time.time()\n",
    "        \n",
    "        #=======================================\n",
    "        # Preprocess logs \n",
    "        #=======================================\n",
    "        \n",
    "        ## Read file while fixing the tab issue\n",
    "        if file in files_with_trailing_tab:\n",
    "            df = pd.read_csv(run+'/'+file, sep='\\t', usecols=range(0,18))\n",
    "            print(run+'/'+file)\n",
    "        else:\n",
    "            df = pd.read_csv(run+'/'+file, sep='\\t')\n",
    "            print(run+'/'+file)\n",
    "        \n",
    "        print('time to read file: ', (time.time() - start_time))\n",
    "        \n",
    "        df = clean_up_logs(df, file, sim_start_time, sim_end_time)\n",
    "        \n",
    "        #=======================================\n",
    "        # Print log file info  \n",
    "        #=======================================\n",
    "        if print_logfile_info:\n",
    "            ## Display info about the UEs who have made entries in this file to make sure that all the UEs who should be here are here. \n",
    "            print('ueIds: min:', min(df['IMSI'].value_counts().index), 'max:', max(df['IMSI'].value_counts().index), \n",
    "                  'count:', len(df['IMSI'].value_counts().index))\n",
    "            if (file in ran_files) and (len(df['IMSI'].value_counts().index) < total_num_ues):\n",
    "                print('WARNING: Fewer UEs in this file than the total number in the simulation')\n",
    "                print(df['IMSI'].value_counts())\n",
    "\n",
    "            ## Total runtime of log\n",
    "            print('log time (start, end): (', np.nanmin(df['tstamp_us']), ', ' ,np.nanmax(df['tstamp_us']), ')')\n",
    "            print('log runtime:', (np.nanmax(df['tstamp_us']) - np.nanmin(df['tstamp_us'])), ' seconds')\n",
    "\n",
    "        # set the name of the index before doing all the index based processing that lies ahead \n",
    "        df.index.set_names('wind_tstamp', inplace=True)\n",
    "        #==========================================================================\n",
    "        # File specific extraction and aggregation of metrics for dataset creation\n",
    "        #==========================================================================\n",
    "        \n",
    "        if create_dataset:\n",
    "            \n",
    "            # this must come first, so make sure it is the first file being read \n",
    "            # by putting it first in the files list\n",
    "            \n",
    "            if file == 'mobility_trace.txt':\n",
    "                \n",
    "                # Need to take unique for each IMSI list\n",
    "                df_mi_cellId_imsi_conn = df[['IMSI', 'cellId']].sort_values(by=['cellId']).groupby(by=['cellId'])\n",
    "                df_mi_cellId_imsi_conn = df_mi_cellId_imsi_conn.resample(time_wind_str).agg(list)\n",
    "                # This slices away any extras. It does not take care if there is a window missing i.e. the sequence is shorter \n",
    "                # lets just slice off a second since I goofed up in the beginning and dont want to include extra samples for each cell \n",
    "                df_mi_cellId_imsi_conn = df_mi_cellId_imsi_conn.drop(['cellId'], axis=1).loc[pd.IndexSlice[:, sim_start_time : sim_end_time], :]  \n",
    "                df_mi_cellId_imsi_conn.index = df_mi_cellId_imsi_conn.index.set_names(['cellId', 'wind_tstamp'])\n",
    "\n",
    "\n",
    "            elif file == 'httpClientRtt_trace.txt':\n",
    "\n",
    "                chosen_imsis = filtered_imsis(data_slice, sorted(df['IMSI'].unique()))                    \n",
    "                print('Web browsing IMSIs: ', chosen_imsis)\n",
    "\n",
    "                this_log_df = pd.DataFrame()\n",
    "                tmp_this_log_df = pd.DataFrame()\n",
    "\n",
    "                # IMSI perspective \n",
    "                for imsi, group1 in df.sort_values(by=['IMSI']).groupby(by=['IMSI']):\n",
    "                    imsi = imsi[0]\n",
    "                    \n",
    "                    if imsi in chosen_imsis:\n",
    "                            \n",
    "                        page_load_time_df = list()\n",
    "                        webpage_size_df = list()\n",
    "                        http_index = list()\n",
    "                        \n",
    "                        for pageId, group2 in group1.groupby(by=['webpageId']):\n",
    "\n",
    "                            # this takes the receive timestamp\n",
    "                            http_index = http_index + [group2.index[-1]] # list append \n",
    "                            page_load_time_df = page_load_time_df + [group2['delay'].sum()] # list append \n",
    "                            webpage_size_df = webpage_size_df + [group2['objectSize'].sum()/K] # list append                            \n",
    "                            \n",
    "                        page_load_time_df = pd.DataFrame(index=pd.to_datetime(http_index), columns=['page_load_time'], data=page_load_time_df)\n",
    "                        page_load_time_df = page_load_time_df.resample(time_wind_str).mean().loc[sim_start_time : sim_end_time]\n",
    "                        \n",
    "                        webpage_size_df = pd.DataFrame(index=http_index, columns=['webpage_size'], data=webpage_size_df)\n",
    "                        webpage_size_df = webpage_size_df.resample(time_wind_str).mean().loc[sim_start_time : sim_end_time] # Mbps\n",
    "                        \n",
    "                        # concatenate\n",
    "                        this_log_df_per_imsi = pd.concat([page_load_time_df, webpage_size_df], axis=1)\n",
    "                        this_log_df_per_imsi.columns = ['page_load_time', 'webpage_size']\n",
    "                        # Add IMSI as a column to use later for indexing\n",
    "                        this_log_df_per_imsi['IMSI'] = [imsi] * this_log_df_per_imsi.shape[0]                        \n",
    "                        tmp_this_log_df = pd.concat([tmp_this_log_df, this_log_df_per_imsi], axis=0)  \n",
    "\n",
    "                        \n",
    "                # Make sure that the order of IMSIs is the same as when we do the RAN logs \n",
    "                # set index name for the resampled time stamp\n",
    "                print('---------------------------------------------------------')\n",
    "                tmp_this_log_df.index = tmp_this_log_df.index.set_names('wind_tstamp')\n",
    "                \n",
    "                # Convert the IMSI column to an index\n",
    "                tmp_this_log_df = tmp_this_log_df.set_index('IMSI', append=True)\n",
    "                this_log_df = tmp_this_log_df.reorder_levels(['IMSI', 'wind_tstamp'])\n",
    "                \n",
    "                # add a prefix to the column names\n",
    "                this_log_df = this_log_df.add_prefix(file+'_')\n",
    "                \n",
    "                # Concatenating along axis=1 and setting 'IMSI' and 'wind_tstamp' as the index again\n",
    "                per_run_df = pd.concat([per_run_df, this_log_df], axis=1, sort=True)\n",
    "                print('time to parse file: ', (time.time() - start_time))\n",
    "\n",
    "            elif file == 'dashClient_trace.txt':\n",
    "                \n",
    "                chosen_imsis = filtered_imsis(data_slice, sorted(df['IMSI'].unique()))                    \n",
    "                print('video streaming IMSIs: ', chosen_imsis)\n",
    "                \n",
    "                # IMSI perspective \n",
    "                \n",
    "                # Create the UE and cell perspective for the dataset \n",
    "                # This has 2 levels of indexes, one is IMSI and the other is windowed timestep  \n",
    "                df_mi_imsi_log = df.sort_values(by=['IMSI']).groupby(by=['IMSI'])\n",
    "                df_mi_imsi_log = df_mi_imsi_log.resample(time_wind_str).mean()\n",
    "        \n",
    "                # This slices away any extras. It does not take care if there is a window missing i.e. the sequence is shorter \n",
    "                df_mi_imsi_log = df_mi_imsi_log.loc[pd.IndexSlice[:, sim_start_time : sim_end_time], :]\n",
    "                # give a name to the resampled time index \n",
    "                df_mi_imsi_log.index = df_mi_imsi_log.index.set_names(['IMSI', 'wind_tstamp'])\n",
    "                \n",
    "                this_log_df = pd.DataFrame(columns=['newBitRate_bps', 'oldBitRate_bps'])\n",
    "                # I want to drop all the rows that correspond to IMSIs that are not in chosen_imsis\n",
    "                this_log_df['newBitRate_bps'] = df_mi_imsi_log[df_mi_imsi_log.index.get_level_values('IMSI').isin(chosen_imsis)]['newBitRate_bps']\n",
    "                this_log_df['oldBitRate_bps'] = df_mi_imsi_log[df_mi_imsi_log.index.get_level_values('IMSI').isin(chosen_imsis)]['oldBitRate_bps']\n",
    "                \n",
    "                # add a prefix to the column names\n",
    "                this_log_df = this_log_df.add_prefix(file+'_')\n",
    "                \n",
    "                # Concatenating along axis=1 and setting 'IMSI' and 'wind_tstamp' as the index again\n",
    "                per_run_df = pd.concat([per_run_df, this_log_df], axis=1, sort=True)\n",
    "                \n",
    "                print('time to parse file: ', (time.time() - start_time)) \n",
    "                \n",
    "            elif file == 'vrFragment_trace.txt':\n",
    "\n",
    "                chosen_imsis = filtered_imsis(data_slice, sorted(df['IMSI'].unique()))\n",
    "                print('VR IMSIs: ', chosen_imsis)\n",
    "                \n",
    "                this_log_df = pd.DataFrame()\n",
    "                tmp_this_log_df = pd.DataFrame()\n",
    "\n",
    "                for imsi, group1 in df.sort_values(by=['IMSI']).groupby(by=['IMSI']):\n",
    "                    imsi = imsi[0]\n",
    "                    \n",
    "                    if imsi in chosen_imsis:\n",
    "                        \n",
    "                        vr_frag_time_df = (group1['delay']).resample(time_wind_str).mean().loc[sim_start_time : sim_end_time]\n",
    "                        vr_frag_thput_df = (group1['burstSize']*8/K/group1['numFragsInBurst']/group1['delay']).resample(time_wind_str).mean().loc[sim_start_time : sim_end_time] #Mbps\n",
    "                        \n",
    "                        vr_burst_time_df = list()\n",
    "                        vr_burst_thput_df = list()\n",
    "                        vr_index = list()\n",
    "                        \n",
    "                        for burstSeqNum, group2 in group1.groupby(by=['burstSeqNum']):\n",
    "                            # this takes the receive timestamp\n",
    "                            vr_index = vr_index + [group2.index[-1]]\n",
    "                            vr_burst_time_df = vr_burst_time_df + [group2['delay'].iloc[-1]]\n",
    "                            vr_burst_thput_df = vr_burst_thput_df + [(group2['burstSize'].iloc[-1]*8/K)/group2['delay'].iloc[-1] ]# Mbps\n",
    "                        \n",
    "                        vr_burst_time_df = pd.DataFrame(index=pd.to_datetime(vr_index), columns=['vr_burst_time'], data=vr_burst_time_df)\n",
    "                        vr_burst_time_df = vr_burst_time_df.resample(time_wind_str).mean().loc[sim_start_time : sim_end_time]\n",
    "                        \n",
    "                        vr_burst_thput_df = pd.DataFrame(index=vr_index, columns=['vr_burst_thput_mbps'], data=vr_burst_thput_df)\n",
    "                        vr_burst_thput_df = vr_burst_thput_df.resample(time_wind_str).mean().loc[sim_start_time : sim_end_time] # Mbps\n",
    "                        \n",
    "                        # concatenate\n",
    "                        this_log_df_per_imsi = pd.concat([vr_frag_time_df, vr_frag_thput_df, vr_burst_time_df, vr_burst_thput_df], axis=1)\n",
    "                        this_log_df_per_imsi.columns = ['vr_frag_time', 'vr_frag_thput_mbps', 'vr_burst_time', 'vr_burst_thput_mbps']\n",
    "                        # Add IMSI as a column to use later for indexing\n",
    "                        this_log_df_per_imsi['IMSI'] = [imsi] * this_log_df_per_imsi.shape[0]                        \n",
    "                        tmp_this_log_df = pd.concat([tmp_this_log_df, this_log_df_per_imsi], axis=0)  \n",
    "\n",
    "                        \n",
    "                # Make sure that the order of IMSIs is the same as when we do the RAN logs \n",
    "                # set index name for the resampled time stamp\n",
    "                print('---------------------------------------------------------')\n",
    "                tmp_this_log_df.index = tmp_this_log_df.index.set_names('wind_tstamp')\n",
    "                \n",
    "                # Convert the IMSI column to an index\n",
    "                tmp_this_log_df = tmp_this_log_df.set_index('IMSI', append=True)\n",
    "                this_log_df = tmp_this_log_df.reorder_levels(['IMSI', 'wind_tstamp'])\n",
    "                \n",
    "                # add a prefix to the column names\n",
    "                this_log_df = this_log_df.add_prefix(file+'_')\n",
    "                \n",
    "                # Concatenating along axis=1 and setting 'IMSI' and 'wind_tstamp' as the index again\n",
    "                per_run_df = pd.concat([per_run_df, this_log_df], axis=1, sort=True)\n",
    "                \n",
    "                print('time to parse file: ', (time.time() - start_time))\n",
    "                \n",
    "            elif file == 'delay_trace.txt': \n",
    "                # process the my_metrics part and skip the cell_metrics part \n",
    "                # just take the ul and dl delay cols and discard the rest \n",
    "                \n",
    "                # IMSI perspective \n",
    "                \n",
    "                # Create the UE and cell perspective for the dataset \n",
    "                # This has 2 levels of indexes, one is IMSI and the other is windowed timestep  \n",
    "                # Also drop the UL and DL markers so that only numerica features are left\n",
    "                df_ul = df[df['dir'] == 'UL'].drop(['dir'], axis=1)\n",
    "                df_dl = df[df['dir'] == 'DL'].drop(['dir'], axis=1)\n",
    "\n",
    "                df_mi_imsi_log_ul = df_ul.sort_values(by=['IMSI']).groupby(by=['IMSI'])\n",
    "                \n",
    "                df_mi_imsi_log_ul = df_mi_imsi_log_ul.resample(time_wind_str).mean()\n",
    "                \n",
    "                df_mi_imsi_log_dl = df_dl.sort_values(by=['IMSI']).groupby(by=['IMSI'])\n",
    "                df_mi_imsi_log_dl = df_mi_imsi_log_dl.resample(time_wind_str).mean()\n",
    "                \n",
    "                # This slices away any extras. It does not take care if there is a window missing i.e. the sequence is shorter \n",
    "                df_mi_imsi_log_ul = df_mi_imsi_log_ul.loc[pd.IndexSlice[:, sim_start_time : sim_end_time], :]\n",
    "                df_mi_imsi_log_dl = df_mi_imsi_log_dl.loc[pd.IndexSlice[:, sim_start_time : sim_end_time], :]\n",
    "                # give a name to the resampled time index \n",
    "                df_mi_imsi_log_ul.index = df_mi_imsi_log_ul.index.set_names(['IMSI', 'wind_tstamp'])\n",
    "                df_mi_imsi_log_dl.index = df_mi_imsi_log_dl.index.set_names(['IMSI', 'wind_tstamp'])\n",
    "                \n",
    "                this_log_df = pd.DataFrame(columns=['ul_delay', 'dl_delay'])\n",
    "                \n",
    "                chosen_imsis = filtered_imsis(data_slice, sorted(df['IMSI'].unique()))\n",
    "\n",
    "                this_log_df['ul_delay'] = df_mi_imsi_log_ul[df_mi_imsi_log_ul.index.get_level_values('IMSI').isin(chosen_imsis)]['delay']\n",
    "                this_log_df['dl_delay'] = df_mi_imsi_log_dl[df_mi_imsi_log_dl.index.get_level_values('IMSI').isin(chosen_imsis)]['delay']\n",
    "            \n",
    "                # Make sure that the order of IMSIs is the same as when we do the RAN logs\n",
    "                # add a prefix to the column names\n",
    "                this_log_df = this_log_df.add_prefix(file+'_')\n",
    "                per_run_df = pd.concat([per_run_df, this_log_df], axis=1, sort=True)\n",
    "                \n",
    "                print('time to parse file: ', (time.time() - start_time)) \n",
    "            \n",
    "            # ends up being all the RAN files \n",
    "            else:\n",
    "                # identify the sum and mean columns from the set of columns in this log file\n",
    "                sum_cols_feats = list(set(sum_feats) & set(df.columns))\n",
    "                mean_cols_feats = list(set(df.columns) - set(sum_cols_feats))\n",
    "                mean_cols_feats.remove('cellId')\n",
    "\n",
    "                # IMSI perspective\n",
    "                \n",
    "                # Create the UE and cell perspective for the dataset \n",
    "                # This has 2 levels of indexes, one is IMSI and the other is windowed timestep\n",
    "                df_mi_imsi_log = df.sort_values(by=['IMSI']).groupby(by=['IMSI']) # groupBy object \n",
    "                #resample into windows and either mean or sum based on which group the column belongs to\n",
    "                if n_step_history:\n",
    "                    sum_df_mi_imsi_tmp = df_mi_imsi_log[sum_cols_feats].resample(small_wind_str).sum()\n",
    "                    sum_df_mi_imsi_log = sum_df_mi_imsi_tmp.groupby([pd.Grouper(level='IMSI'), pd.Grouper(level='wind_tstamp', freq=time_wind_str)]).agg(list)\n",
    "                    # don't expand the cellId column                    \n",
    "                    mean_df_mi_imsi_tmp = df_mi_imsi_log[mean_cols_feats].resample(small_wind_str).mean()\n",
    "                    mean_df_mi_imsi_log = mean_df_mi_imsi_tmp.groupby([pd.Grouper(level='IMSI'), pd.Grouper(level='wind_tstamp', freq=time_wind_str)]).agg(list)\n",
    "                    mean_df_mi_imsi_log['cellId'] = df_mi_imsi_log['cellId'].resample(time_wind_str).mean()\n",
    "                else: # single step history \n",
    "                    sum_df_mi_imsi_log = df_mi_imsi_log[sum_cols_feats].resample(time_wind_str).sum()\n",
    "                    mean_df_mi_imsi_log = df_mi_imsi_log[mean_cols_feats].resample(time_wind_str).mean()\n",
    "\n",
    "                if not sum_cols_feats: # if sum cols is null\n",
    "                    df_mi_imsi_log = mean_df_mi_imsi_log\n",
    "                elif not mean_cols_feats:# if mean cols is null\n",
    "                    df_mi_imsi_log = sum_df_mi_imsi_log\n",
    "                else:\n",
    "                    df_mi_imsi_log = pd.concat([mean_df_mi_imsi_log, sum_df_mi_imsi_log], axis=1)\n",
    "                # This slices away any extras. It does not take care if there is a window missing i.e. the sequence is shorter \n",
    "                df_mi_imsi_log = df_mi_imsi_log.loc[pd.IndexSlice[:, sim_start_time : sim_end_time], :]\n",
    "                # give a name to the resampled time index \n",
    "                df_mi_imsi_log.index = df_mi_imsi_log.index.set_names(['IMSI', 'wind_tstamp'])\n",
    "                df_mi_imsi_log = df_mi_imsi_log.drop(drop_cols_before_sep, axis=1, errors='ignore')\n",
    "\n",
    "                # Cell perspective\n",
    "\n",
    "                # Create the UE abnd cell perspective for the dataset \n",
    "                # This has 2 levels of indexes, one is IMSI and the other is windowed timestep  \n",
    "                df_mi_cell_log = df.sort_values(by=['cellId']).groupby(by=['cellId'])\n",
    "                #resample into windows and either mean or sum based on which group the column belongs to \n",
    "                if n_step_history:\n",
    "                    sum_df_mi_cell_tmp = df_mi_cell_log[sum_cols_feats].resample(small_wind_str).sum()\n",
    "                    sum_df_mi_cell_log = sum_df_mi_cell_tmp.groupby([pd.Grouper(level='cellId'), pd.Grouper(level='wind_tstamp', freq=time_wind_str)]).agg(list)\n",
    "                    # don't expand the cellId column\n",
    "                    mean_df_mi_cell_tmp = df_mi_cell_log[mean_cols_feats].resample(small_wind_str).mean()\n",
    "                    mean_df_mi_cell_log = mean_df_mi_cell_tmp.groupby([pd.Grouper(level='cellId'), pd.Grouper(level='wind_tstamp', freq=time_wind_str)]).agg(list)\n",
    "                    mean_df_mi_cell_log['cellId'] = df_mi_cell_log['cellId'].resample(time_wind_str).mean()\n",
    "                else:\n",
    "                    sum_df_mi_cell_log = df_mi_cell_log[sum_cols_feats].resample(time_wind_str).sum()\n",
    "                    mean_df_mi_cell_log = df_mi_cell_log[mean_cols_feats].resample(time_wind_str).mean()\n",
    "\n",
    "                if not sum_cols_feats: # if sum cols is null\n",
    "                    df_mi_cell_log = mean_df_mi_cell_log\n",
    "                elif not mean_cols_feats:# if mean cols is null\n",
    "                    df_mi_cell_log = sum_df_mi_cell_log\n",
    "                else:\n",
    "                    df_mi_cell_log = pd.concat([mean_df_mi_cell_log, sum_df_mi_cell_log], axis=1)\n",
    "\n",
    "                # This slices away any extras. It does not take care if there is a window missing i.e. the sequence is shorter \n",
    "                df_mi_cell_log = df_mi_cell_log.loc[pd.IndexSlice[:, sim_start_time : sim_end_time], :]\n",
    "                # give a name to the resampled time index \n",
    "                df_mi_cell_log.index = df_mi_cell_log.index.set_names(['cellId', 'wind_tstamp'])\n",
    "                df_mi_cell_log = df_mi_cell_log.drop(drop_cols_before_sep, axis=1, errors='ignore')\n",
    "\n",
    "                if n_step_history:\n",
    "                    # Expecting that the columns in m_ulSched are the same as ld_ulSched \n",
    "                    # before they are concatenated with the prefix loadUe\n",
    "                    for col in df_mi_imsi_log.columns:\n",
    "                        if col == 'cellId': # because we did not expand this\n",
    "                            continue\n",
    "                        # add the list as multiple colums \n",
    "                        step_cols = [col+'_'+str(i) for i in range(num_steps)]\n",
    "                        df_mi_imsi_log[step_cols] = pd.DataFrame(df_mi_imsi_log[col].to_list(), index=df_mi_imsi_log.index) \n",
    "                        df_mi_cell_log[step_cols] = pd.DataFrame(df_mi_cell_log[col].to_list(), index=df_mi_cell_log.index) \n",
    "                        # then delete the col with the list\n",
    "                        df_mi_imsi_log = df_mi_imsi_log.drop([col], axis=1)\n",
    "                        df_mi_cell_log = df_mi_cell_log.drop([col], axis=1)\n",
    "                                        \n",
    "                # Align the cell perspective to the IMSI perspective\n",
    "                \n",
    "                this_log_df = pd.DataFrame()\n",
    "                \n",
    "                # the imsis from whose application's perspective we are constructing the dataset \n",
    "                # make sure we are reading from a fixed list and not something like for imsi in all_imsis_in_this_file\n",
    "                # we need the same order of IMSIs to be iterated thru so that the data frame for each file \n",
    "                # will have the same imsi order \n",
    "                chosen_imsis = filtered_imsis(data_slice, sorted(df['IMSI'].unique()))\n",
    "                \n",
    "                for i_imsi in chosen_imsis:\n",
    "                    \n",
    "                    num_rows = df_mi_imsi_log.loc[i_imsi].shape[0]\n",
    "                    num_cols = df_mi_imsi_log.loc[i_imsi].shape[1]\n",
    "                    basic_info_per_imsi = pd.DataFrame(index=df_mi_imsi_log.loc[i_imsi].index,\n",
    "                                                   columns=['IMSI', 'cellId', 'conn_imsi_count'])\n",
    "                    if first_ran_file:\n",
    "                        # set the index for this as well \n",
    "                        basic_info_per_imsi['IMSI'] = [i_imsi] * num_rows\n",
    "                        basic_info_per_imsi['cellId'] = [0] * num_rows #an array of the corresponding cellId\n",
    "                        basic_info_per_imsi['conn_imsi_count'] = [0] * num_rows #as given by the imsis connected cell \n",
    "                        \n",
    "                    # Get the metrics for this IMSI\n",
    "                    my_metrics = df_mi_imsi_log.loc[i_imsi]\n",
    "\n",
    "                    # Initialize a DataFrame to store metrics for other IMSIs\n",
    "                    cell_metrics = pd.DataFrame(index=df_mi_imsi_log.loc[i_imsi].index,\n",
    "                                                  columns=['cell_' + col for col in df_mi_imsi_log.loc[i_imsi].columns],\n",
    "                                                  data=np.zeros((num_rows, num_cols)))\n",
    "                    \n",
    "                    # Iterate over each window for this IMSI\n",
    "                    for wind in df_mi_imsi_log.loc[i_imsi].index:\n",
    "                        # the cell that the UE was connected to\n",
    "                        con_cell = np.round(df_mi_imsi_log.loc[i_imsi, wind]['cellId'])\n",
    "                        if first_ran_file:\n",
    "                            basic_info_per_imsi.loc[wind, 'cellId'] = con_cell\n",
    "                        # Handle NaN case\n",
    "                        if np.isnan(con_cell):\n",
    "                            cell_metrics.loc[wind] = np.nan\n",
    "                            continue\n",
    "\n",
    "                        if first_ran_file:\n",
    "                            basic_info_per_imsi.loc[wind, 'conn_imsi_count'] = len(np.unique(df_mi_cellId_imsi_conn.loc[con_cell, wind][0]))\n",
    "                        cell_metrics.loc[wind] = df_mi_cell_log.loc[con_cell, wind].values\n",
    "                        \n",
    "                    # Combine metrics for the current IMSI and other IMSIs\n",
    "                    this_imsi_df = pd.concat([my_metrics, cell_metrics], axis=1)\n",
    "\n",
    "                    # Drop the columns that need to be removed\n",
    "                    this_imsi_df = this_imsi_df.drop(drop_cols_after_sep, axis=1, errors='ignore')\n",
    "\n",
    "                    # Append the metrics for this IMSI to the final DataFrame and include IMSI as a column to use as index later\n",
    "                    this_imsi_df['IMSI'] = [i_imsi] * this_imsi_df.shape[0]\n",
    "                    this_log_df = pd.concat([this_log_df, this_imsi_df], axis=0)\n",
    "                    \n",
    "                    if first_ran_file:\n",
    "                        # combine the basic info \n",
    "                        basic_info = pd.concat([basic_info, basic_info_per_imsi], axis=0)\n",
    "     \n",
    "                # end of for over imsis\n",
    "\n",
    "                # convert the IMSI column of this_log_df into an index\n",
    "                this_log_df = this_log_df.set_index('IMSI', append=True)\n",
    "                this_log_df = this_log_df.reorder_levels(['IMSI', 'wind_tstamp'])\n",
    "                \n",
    "                # add a prefix to the column names\n",
    "                this_log_df = this_log_df.add_prefix(file+'_')\n",
    "                \n",
    "                # horz concatenate it to the per_run_df\n",
    "                if first_ran_file:\n",
    "                    basic_info.index = basic_info.index.set_names('wind_tstamp')                    \n",
    "                    # Convert the IMSI column to an index\n",
    "                    basic_info = basic_info.set_index('IMSI', append=True)\n",
    "                    basic_info = basic_info.reorder_levels(['IMSI', 'wind_tstamp'])                 \n",
    "                    per_run_df = pd.concat([basic_info, per_run_df, this_log_df], axis=1)\n",
    "                \n",
    "                per_run_df = pd.concat([per_run_df, this_log_df], axis=1)\n",
    "                first_ran_file = False\n",
    "                print(this_log_df.columns.to_list())\n",
    "                print('Number of columns added from this log file', this_log_df.shape[1])\n",
    "                print('time to parse file: ', (time.time() - start_time))   \n",
    "            # end of elif file in ran_files:\n",
    "        # end of if create_dataset:\n",
    "    # end of for over files\n",
    "    # each run is saved separately in a different file \n",
    "    # Save the dataset\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    per_run_df = per_run_df.reset_index(level=('IMSI',))    \n",
    "    per_run_df.to_csv(save_dir+only_run+'_dataslice_'+data_slice+'_webpage_video_delay_vr_'+time_wind_str+'.csv', index=True)\n",
    "    print('============================================================')   \n",
    "    print('======================= RUN DONE =====================================')\n",
    "    return per_run_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facef1d6-8ba6-41aa-8d77-64d28bcca82c",
   "metadata": {},
   "source": [
    "# Process each run in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51375bf3-3dd1-4f7d-a2b8-a64d1fd27a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "../../../dataset_ver1/run1\n",
      "============================================================\n",
      "run1\n",
      "{'macro_rings': {'value': '0'}, 'macro_num_bs': {'value': '3'}, 'macro_layer_ues': {'value': '30'}, 'simulation_time_seconds': {'value': '1000'}, 'rand_seed': {'value': '0'}, 'create_micro_layer': {'value': '1'}, 'scheduler': {'value': 'PF'}, 'handover_algo': {'value': 'A2A4Rsrq'}, 'micro_num_bs': {'value': '3'}, 'micro_layer_ues': {'value': '60'}, 'delay_app_installed': {'value': '1'}, 'delay_pkt_interval_seconds': {'value': '+0.1s'}, 'rtt_app_installed': {'value': '0'}, 'http_app_installed': {'value': '1'}, 'dash_app_installed': {'value': '1'}, 'vr_app_installed': {'value': '1'}}\n",
      "separate_macro_micro  True\n",
      "total_num_cells 6\n",
      "total_num_ues 90\n",
      "sim_time 1000\n",
      "Macro UE IMSIs:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
      "Micro UE IMSIs:  [31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90]\n",
      "Macro fast UE IMSIs:  [1, 2, 3, 4, 5, 6]\n",
      "Macro Slow UE IMSIs:  [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
      "Only delay IMSIs:  [3, 5, 7, 9, 13, 15, 17, 19, 23, 25, 27, 29, 33, 35, 37, 39, 43, 45, 47, 49, 53, 55, 57, 59, 63, 65, 67, 69, 73, 75, 77, 79, 83, 85, 87]\n",
      "Macro CellIds:  [1, 2, 3]\n",
      "Micro CellIds:  [4, 5, 6]\n",
      "--------------------------------------------\n",
      "mobility_trace.txt\n",
      "../../../dataset_ver1/run1/mobility_trace.txt\n",
      "time to read file:  0.1658637523651123\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.5 ,  1001.0 )\n",
      "log runtime: 1000.5  seconds\n",
      "--------------------------------------------\n",
      "dashClient_trace.txt\n",
      "../../../dataset_ver1/run1/dashClient_trace.txt\n",
      "time to read file:  0.011927366256713867\n",
      "ueIds: min: 1 max: 84 count: 28\n",
      "log time (start, end): ( 0.570999 ,  1000.943999 )\n",
      "log runtime: 1000.3729999999999  seconds\n",
      "video streaming IMSIs:  [1, 6, 11, 16, 21, 26, 32, 34, 38, 40, 42, 44, 48, 50, 52, 54, 58, 60, 62, 64, 68, 70, 72, 74, 78, 80, 82, 84]\n",
      "time to parse file:  0.23297739028930664\n",
      "--------------------------------------------\n",
      "delay_trace.txt\n",
      "../../../dataset_ver1/run1/delay_trace.txt\n",
      "time to read file:  1.1776981353759766\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.503999 ,  1001.032999 )\n",
      "log runtime: 1000.529  seconds\n",
      "time to parse file:  12.4602792263031\n",
      "--------------------------------------------\n",
      "vrFragment_trace.txt\n",
      "../../../dataset_ver1/run1/vrFragment_trace.txt\n",
      "time to read file:  0.9959776401519775\n",
      "ueIds: min: 30 max: 90 count: 4\n",
      "log time (start, end): ( 0.503999 ,  562.895999 )\n",
      "log runtime: 562.3919999999999  seconds\n",
      "VR IMSIs:  [30, 88, 89, 90]\n",
      "---------------------------------------------------------\n",
      "time to parse file:  15.767237663269043\n",
      "--------------------------------------------\n",
      "httpClientRtt_trace.txt\n",
      "../../../dataset_ver1/run1/httpClientRtt_trace.txt\n",
      "time to read file:  0.014046907424926758\n",
      "ueIds: min: 2 max: 86 count: 23\n",
      "log time (start, end): ( 0.816999 ,  1000.641999 )\n",
      "log runtime: 999.825  seconds\n",
      "Web browsing IMSIs:  [2, 4, 8, 10, 12, 14, 18, 20, 22, 24, 28, 31, 36, 41, 46, 51, 56, 61, 66, 71, 76, 81, 86]\n",
      "---------------------------------------------------------\n",
      "time to parse file:  0.3083035945892334\n",
      "--------------------------------------------\n",
      "UlSinrStats.txt\n",
      "../../../dataset_ver1/run1/UlSinrStats.txt\n",
      "time to read file:  0.36257219314575195\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.321 ,  1001.05 )\n",
      "log runtime: 1000.7289999999999  seconds\n",
      "['UlSinrStats.txt_sinrLinear_0', 'UlSinrStats.txt_sinrLinear_1', 'UlSinrStats.txt_sinrLinear_2', 'UlSinrStats.txt_sinrLinear_3', 'UlSinrStats.txt_sinrLinear_4', 'UlSinrStats.txt_cell_sinrLinear_0', 'UlSinrStats.txt_cell_sinrLinear_1', 'UlSinrStats.txt_cell_sinrLinear_2', 'UlSinrStats.txt_cell_sinrLinear_3', 'UlSinrStats.txt_cell_sinrLinear_4']\n",
      "time to parse file:  215.12316966056824\n",
      "--------------------------------------------\n",
      "DlRsrpSinrStats.txt\n",
      "../../../dataset_ver1/run1/DlRsrpSinrStats.txt\n",
      "time to read file:  1.0144634246826172\n",
      "ueIds: min: 1 max: 90 count: 90\n",
      "log time (start, end): ( 0.239214 ,  1001.04 )\n",
      "log runtime: 1000.800786  seconds\n",
      "['DlRsrpSinrStats.txt_sinr_0', 'DlRsrpSinrStats.txt_sinr_1', 'DlRsrpSinrStats.txt_sinr_2', 'DlRsrpSinrStats.txt_sinr_3', 'DlRsrpSinrStats.txt_sinr_4', 'DlRsrpSinrStats.txt_rsrp_0', 'DlRsrpSinrStats.txt_rsrp_1', 'DlRsrpSinrStats.txt_rsrp_2', 'DlRsrpSinrStats.txt_rsrp_3', 'DlRsrpSinrStats.txt_rsrp_4', 'DlRsrpSinrStats.txt_cell_sinr_0', 'DlRsrpSinrStats.txt_cell_sinr_1', 'DlRsrpSinrStats.txt_cell_sinr_2', 'DlRsrpSinrStats.txt_cell_sinr_3', 'DlRsrpSinrStats.txt_cell_sinr_4', 'DlRsrpSinrStats.txt_cell_rsrp_0', 'DlRsrpSinrStats.txt_cell_rsrp_1', 'DlRsrpSinrStats.txt_cell_rsrp_2', 'DlRsrpSinrStats.txt_cell_rsrp_3', 'DlRsrpSinrStats.txt_cell_rsrp_4']\n",
      "time to parse file:  157.70880651474\n",
      "--------------------------------------------\n",
      "UlTxPhyStats.txt\n",
      "../../../dataset_ver1/run1/UlTxPhyStats.txt\n",
      "time to read file:  4.436372995376587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-15:\n",
      "Process ForkPoolWorker-7:\n",
      "Process ForkPoolWorker-11:\n",
      "Process ForkPoolWorker-16:\n",
      "Process ForkPoolWorker-20:\n",
      "Process ForkPoolWorker-4:\n",
      "Process ForkPoolWorker-12:\n",
      "Process ForkPoolWorker-19:\n",
      "Process ForkPoolWorker-13:\n",
      "Process ForkPoolWorker-17:\n",
      "Process ForkPoolWorker-14:\n",
      "Process ForkPoolWorker-2:\n",
      "Process ForkPoolWorker-5:\n",
      "Process ForkPoolWorker-18:\n",
      "Process ForkPoolWorker-9:\n",
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-6:\n",
      "Process ForkPoolWorker-10:\n",
      "Process ForkPoolWorker-8:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkPoolWorker-1:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 356, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 51, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/tmp/ipykernel_490349/3149612784.py\", line 87, in setup_run\n",
      "    df = clean_up_logs(df, file, sim_start_time, sim_end_time)\n",
      "  File \"/home/ubuntu/mobile-network-datasets-ns3/data_processing_and_ML_code/data_processing_code/helper_func_create_dataset.py\", line 171, in clean_up_logs\n",
      "    df = pd.concat([start_sample, df, end_sample])\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/reshape/concat.py\", line 385, in concat\n",
      "    return op.get_result()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/reshape/concat.py\", line 616, in get_result\n",
      "    new_data = concatenate_managers(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/internals/concat.py\", line 242, in concatenate_managers\n",
      "    values = _concatenate_join_units(join_units, copy=copy)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/internals/concat.py\", line 613, in _concatenate_join_units\n",
      "    concat_values = concat_compat(to_concat, axis=1)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/dtypes/concat.py\", line 117, in concat_compat\n",
      "    result = np.concatenate(to_concat, axis=axis)\n",
      "  File \"<__array_function__ internals>\", line 200, in concatenate\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m pool \u001b[38;5;241m=\u001b[39m Pool(processes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m      2\u001b[0m inputs \u001b[38;5;241m=\u001b[39m [(run, create_dataset, print_logfile_info, time_wind_str, time_wind_val, data_slice, save_dir, files) \u001b[38;5;28;01mfor\u001b[39;00m run \u001b[38;5;129;01min\u001b[39;00m [data_dir\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m runs]]\n\u001b[0;32m----> 3\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstarmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43msetup_run\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#outputs = pool.map(setup_run, range(0,runs))\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/pool.py:372\u001b[0m, in \u001b[0;36mPool.starmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstarmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    367\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;124;03m    Like `map()` method but the elements of the `iterable` are expected to\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;124;03m    be iterables as well and will be unpacked as arguments. Hence\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;124;03m    `func` and (a, b) becomes func(a, b).\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 372\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstarmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    767\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/pool.py:762\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 762\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.8/threading.py:558\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    556\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 558\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/usr/lib/python3.8/threading.py:302\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-31:\n",
      "Process ForkPoolWorker-22:\n",
      "Process ForkPoolWorker-21:\n",
      "Process ForkPoolWorker-34:\n",
      "Process ForkPoolWorker-36:\n",
      "Process ForkPoolWorker-39:\n",
      "Process ForkPoolWorker-37:\n",
      "Process ForkPoolWorker-40:\n",
      "Process ForkPoolWorker-28:\n",
      "Process ForkPoolWorker-24:\n",
      "Process ForkPoolWorker-32:\n",
      "Process ForkPoolWorker-25:\n",
      "Process ForkPoolWorker-26:\n",
      "Process ForkPoolWorker-33:\n",
      "Process ForkPoolWorker-35:\n",
      "Process ForkPoolWorker-30:\n",
      "Process ForkPoolWorker-29:\n",
      "Process ForkPoolWorker-38:\n",
      "Process ForkPoolWorker-23:\n",
      "Process ForkPoolWorker-27:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "pool = Pool(processes=20)\n",
    "inputs = [(run, create_dataset, print_logfile_info, time_wind_str, time_wind_val, data_slice, save_dir, files) for run in [data_dir+'run'+str(r) for r in runs]]\n",
    "outputs = pool.starmap(setup_run, inputs)  \n",
    "#outputs = pool.map(setup_run, range(0,runs))\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6448e5a-98a5-43ac-bd9d-9a60b471af39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05b5d30-7145-4e79-bb41-4ef95aac3711",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
